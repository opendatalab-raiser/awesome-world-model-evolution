# Awesome World Model Evolution - é“¸å‘ä¸–ç•Œæ¨¡å‹å®‡å®™ï¼šåŸºäºç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹çš„èåˆä¹‹è·¯ [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

<div align="center">
  <img src="https://img.shields.io/github/stars/opendatalab-raiser/awesome-world-model-evolution" alt="Stars">
  <img src="https://img.shields.io/github/forks/opendatalab-raiser/awesome-world-model-evolution" alt="Forks">
  <img src="https://img.shields.io/github/license/opendatalab-raiser/awesome-world-model-evolution" alt="License">
  <img src="https://img.shields.io/github/last-commit/opendatalab-raiser/awesome-world-model-evolution" alt="Last Commit">
  <img src="https://img.shields.io/badge/PRs-welcome-brightgreen.svg" alt="PRs Welcome">
</div>

<p align="center">
  <a href="README.md">English</a> | <a href="README_zh.md">ä¸­æ–‡</a>
</p>

<p align="center">
  <strong>ç²¾å¿ƒæ”¶é›†çš„ç ”ç©¶è®ºæ–‡ã€æ¨¡å‹å’Œèµ„æºåˆé›†ï¼Œè¿½æº¯ä»ä¸“ç”¨æ¨¡å‹åˆ°ç»Ÿä¸€ä¸–ç•Œæ¨¡å‹çš„æ¼”åŒ–å†ç¨‹ã€‚</strong>
</p>

---

## ğŸ“‹ ç›®å½•

- [Awesome World Model Evolution - é“¸å‘ä¸–ç•Œæ¨¡å‹å®‡å®™ï¼šåŸºäºç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹çš„èåˆä¹‹è·¯ ](#awesome-world-model-evolution---é“¸å‘ä¸–ç•Œæ¨¡å‹å®‡å®™åŸºäºç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹çš„èåˆä¹‹è·¯-)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [ğŸ¯ å¼•è¨€](#-å¼•è¨€)
    - [ä»€ä¹ˆæ˜¯ä¸–ç•Œæ¨¡å‹ï¼Ÿ](#ä»€ä¹ˆæ˜¯ä¸–ç•Œæ¨¡å‹)
    - [ä¸‰å¤§åŸºç¡€ä¸€è‡´æ€§](#ä¸‰å¤§åŸºç¡€ä¸€è‡´æ€§)
      - [1ï¸âƒ£ **æ¨¡æ€ä¸€è‡´æ€§ï¼ˆModality Consistencyï¼‰**](#1ï¸âƒ£-æ¨¡æ€ä¸€è‡´æ€§modality-consistency)
      - [2ï¸âƒ£ **ç©ºé—´ä¸€è‡´æ€§ï¼ˆSpatial Consistencyï¼‰**](#2ï¸âƒ£-ç©ºé—´ä¸€è‡´æ€§spatial-consistency)
      - [3ï¸âƒ£ **æ—¶é—´ä¸€è‡´æ€§ï¼ˆTemporal Consistencyï¼‰**](#3ï¸âƒ£-æ—¶é—´ä¸€è‡´æ€§temporal-consistency)
    - [ä¸ºä½•é€‰æ‹©ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹ï¼Ÿ](#ä¸ºä½•é€‰æ‹©ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹)
  - [ğŸ”¬ ç‹¬ç«‹æ¢ç´¢ï¼šä¸“ç”¨æ¨¡å‹](#-ç‹¬ç«‹æ¢ç´¢ä¸“ç”¨æ¨¡å‹)
    - [æ¨¡æ€ä¸€è‡´æ€§](#æ¨¡æ€ä¸€è‡´æ€§)
      - [ä»£è¡¨æ€§å·¥ä½œ](#ä»£è¡¨æ€§å·¥ä½œ)
    - [ç©ºé—´ä¸€è‡´æ€§](#ç©ºé—´ä¸€è‡´æ€§)
      - [ä»£è¡¨æ€§å·¥ä½œ](#ä»£è¡¨æ€§å·¥ä½œ-1)
    - [æ—¶é—´ä¸€è‡´æ€§](#æ—¶é—´ä¸€è‡´æ€§)
      - [ä»£è¡¨æ€§å·¥ä½œ](#ä»£è¡¨æ€§å·¥ä½œ-2)
  - [ğŸ”— åˆæ­¥èåˆï¼šç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹](#-åˆæ­¥èåˆç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹)
    - [æ¨¡æ€ + ç©ºé—´ä¸€è‡´æ€§](#æ¨¡æ€--ç©ºé—´ä¸€è‡´æ€§)
      - [ä»£è¡¨æ€§å·¥ä½œ](#ä»£è¡¨æ€§å·¥ä½œ-3)
    - [æ¨¡æ€ + æ—¶é—´ä¸€è‡´æ€§](#æ¨¡æ€--æ—¶é—´ä¸€è‡´æ€§)
      - [ä»£è¡¨æ€§å·¥ä½œ](#ä»£è¡¨æ€§å·¥ä½œ-4)
    - [ç©ºé—´ä¸€è‡´æ€§ + æ—¶é—´ä¸€è‡´æ€§](#ç©ºé—´ä¸€è‡´æ€§--æ—¶é—´ä¸€è‡´æ€§)
      - [ä»£è¡¨æ€§å·¥ä½œ](#ä»£è¡¨æ€§å·¥ä½œ-5)
  - [ğŸŒŸ "ä¸‰ä½ä¸€ä½“"åŸå‹ï¼šæ¶Œç°çš„ä¸–ç•Œæ¨¡å‹](#-ä¸‰ä½ä¸€ä½“åŸå‹æ¶Œç°çš„ä¸–ç•Œæ¨¡å‹)
    - [æ–‡æœ¬åˆ°ä¸–ç•Œç”Ÿæˆå™¨](#æ–‡æœ¬åˆ°ä¸–ç•Œç”Ÿæˆå™¨)
      - [ä»£è¡¨æ€§å·¥ä½œ](#ä»£è¡¨æ€§å·¥ä½œ-6)
    - [å…·èº«æ™ºèƒ½ç³»ç»Ÿ](#å…·èº«æ™ºèƒ½ç³»ç»Ÿ)
      - [ä»£è¡¨æ€§å·¥ä½œ](#ä»£è¡¨æ€§å·¥ä½œ-7)
  - [ğŸ“Š åŸºå‡†æµ‹è¯•ä¸è¯„ä¼°](#-åŸºå‡†æµ‹è¯•ä¸è¯„ä¼°)
      - [ä»£è¡¨æ€§å·¥ä½œ](#ä»£è¡¨æ€§å·¥ä½œ-8)
  - [ğŸ“ è´¡çŒ®æŒ‡å—](#-è´¡çŒ®æŒ‡å—)
    - [å¦‚ä½•è´¡çŒ®](#å¦‚ä½•è´¡çŒ®)
    - [æ¡ç›®æ ¼å¼æ¨¡æ¿](#æ¡ç›®æ ¼å¼æ¨¡æ¿)
  - [â­ Star History](#-star-history)
  - [ğŸ“„ License](#-license)

---

## ğŸ¯ å¼•è¨€

### ä»€ä¹ˆæ˜¯ä¸–ç•Œæ¨¡å‹ï¼Ÿ

**ä¸–ç•Œæ¨¡å‹ï¼ˆWorld Modelï¼‰** æ˜¯ä¸€ç§è®¡ç®—ç³»ç»Ÿï¼Œèƒ½å¤Ÿå­¦ä¹ æ”¯é…æˆ‘ä»¬ç‰©ç†ä¸–ç•Œçš„å†…åœ¨è§„å¾‹ï¼Œå¹¶åœ¨å†…éƒ¨æ„å»ºä¸€ä¸ªå¯æ‰§è¡Œçš„ã€åŠ¨æ€çš„æ¨¡æ‹Ÿç¯å¢ƒã€‚ä¸ä»…æ‰§è¡Œæ¨¡å¼åŒ¹é…æˆ–åˆ†ç±»çš„ä¼ ç»Ÿ AI æ¨¡å‹ä¸åŒï¼Œä¸–ç•Œæ¨¡å‹ä½œä¸ºä¸€ä¸ª**å†…éƒ¨æ¨¡æ‹Ÿå™¨**ï¼Œå…·å¤‡ä»¥ä¸‹èƒ½åŠ›ï¼š

- ğŸ”„ **å¤ç°**å·²è§‚å¯Ÿåˆ°çš„äº‹ä»¶å’Œç°è±¡
- ğŸ² **é¢„æµ‹**æœªæ¥çŠ¶æ€å’Œç»“æœ
- ğŸ¤” **æ¨ç†**åäº‹å®åœºæ™¯ï¼ˆå‡è®¾åˆ†æï¼‰
- ğŸ¯ **è§„åˆ’**åŸºäºæ¨¡æ‹Ÿåæœçš„é•¿æœŸç­–ç•¥

**ä¸–ç•Œæ¨¡å‹çš„é‡è¦æ€§ï¼š**

ä¸–ç•Œæ¨¡å‹è¢«è§†ä¸ºå®ç°**é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰** çš„åŸºçŸ³ï¼š

- ğŸ¤– **å…·èº«æ™ºèƒ½**ï¼šæœºå™¨äººå¯ä»¥åœ¨æ‰§è¡Œå‰"é¢„æ¼”"åŠ¨ä½œ
- ğŸš— **è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿ**ï¼šæ¨¡æ‹Ÿå±é™©è¾¹ç¼˜åœºæ™¯ä»¥å®ç°æ›´å®‰å…¨çš„éƒ¨ç½²
- ğŸ”¬ **ç§‘å­¦å‘ç°**ï¼šé€šè¿‡æ•°å­—å®éªŒåŠ é€Ÿå¯¹å¤æ‚ç³»ç»Ÿçš„ç†è§£
- ğŸ® **äº¤äº’å¼ç¯å¢ƒ**ï¼šåˆ›å»ºå¯æ§çš„ã€ç¬¦åˆç‰©ç†è§„å¾‹çš„è™šæ‹Ÿä¸–ç•Œ

### ä¸‰å¤§åŸºç¡€ä¸€è‡´æ€§

ä¸€ä¸ªåŠŸèƒ½å®Œå¤‡çš„ä¸–ç•Œæ¨¡å‹å¿…é¡»æŒæ¡ä¸‰é¡¹æ ¸å¿ƒèƒ½åŠ›ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸º**ä¸‰å¤§åŸºç¡€ä¸€è‡´æ€§**ï¼š

#### 1ï¸âƒ£ **æ¨¡æ€ä¸€è‡´æ€§ï¼ˆModality Consistencyï¼‰**
æ¨¡å‹ä¸ç°å®ä¸–ç•Œä¹‹é—´çš„"è¯­è¨€æ¥å£"ã€‚

- **èƒ½åŠ›**ï¼šåœ¨é«˜ç»´æ„Ÿå®˜è¾“å…¥ï¼ˆå›¾åƒã€è§†é¢‘ã€éŸ³é¢‘ï¼‰ä¸æŠ½è±¡ç¬¦å·è¡¨ç¤ºï¼ˆè¯­è¨€ã€ç»“æ„åŒ–æ•°æ®ï¼‰ä¹‹é—´è¿›è¡ŒåŒå‘è½¬æ¢
- **ç¤ºä¾‹ä»»åŠ¡**ï¼šå›¾åƒæè¿°ç”Ÿæˆã€æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€è§†è§‰é—®ç­”
- **æ„ä¹‰**ï¼šä½¿ä¸–ç•Œæ¨¡å‹èƒ½å¤Ÿæ¥æ”¶æŒ‡ä»¤å¹¶ä»¥äººç±»å¯ç†è§£çš„æ ¼å¼äº¤æµè§‚å¯Ÿç»“æœ

#### 2ï¸âƒ£ **ç©ºé—´ä¸€è‡´æ€§ï¼ˆSpatial Consistencyï¼‰**
å¯¹ç‰©ç†ä¸–ç•Œçš„"é™æ€ä¸‰ç»´ç†è§£"ã€‚

- **èƒ½åŠ›**ï¼šç†è§£ç‰©ä½“å…·æœ‰å›ºå®šçš„å‡ ä½•å½¢æ€ã€å æ®ç©ºé—´ã€å­˜åœ¨é®æŒ¡å…³ç³»ï¼Œå¹¶åœ¨ä¸åŒè§†è§’ä¸‹ä¿æŒèº«ä»½
- **ç¤ºä¾‹ä»»åŠ¡**ï¼šæ–°è§†è§’åˆæˆã€ä¸‰ç»´é‡å»ºã€å¤šè§†è§’ä¸€è‡´æ€§
- **æ„ä¹‰**ï¼šæ„å»ºæ”¯æŒç²¾ç¡®ç©ºé—´æ¨ç†å’Œå¯¼èˆªçš„åŸºç¡€"åœºæ™¯å›¾"

#### 3ï¸âƒ£ **æ—¶é—´ä¸€è‡´æ€§ï¼ˆTemporal Consistencyï¼‰**
ç”¨äºåŠ¨æ€æ¨¡æ‹Ÿçš„"ç‰©ç†å¼•æ“"ã€‚

- **èƒ½åŠ›**ï¼šå»ºæ¨¡ä¸–ç•Œå¦‚ä½•éšæ—¶é—´æ¼”åŒ–ï¼ŒåŒ…æ‹¬ç‰©ä½“è¿åŠ¨ã€ç‰©ç†äº¤äº’å’Œå› æœäº‹ä»¶é“¾
- **ç¤ºä¾‹ä»»åŠ¡**ï¼šè§†é¢‘é¢„æµ‹ã€åŠ¨åŠ›å­¦å»ºæ¨¡ã€æœªæ¥çŠ¶æ€é¢„æµ‹
- **æ„ä¹‰**ï¼šå®ç°åæœé¢„åˆ¤å’Œé•¿æœŸè§„åˆ’èƒ½åŠ›

### ä¸ºä½•é€‰æ‹©ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹ï¼Ÿ

æœ¬ä»“åº“é€šè¿‡**ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹**ï¼ˆç‰¹åˆ«æ˜¯å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ - LMMsï¼‰è¿½æº¯ä¸–ç•Œæ¨¡å‹çš„æ¼”åŒ–è·¯å¾„ï¼Œæˆ‘ä»¬è®¤ä¸ºè¿™ä»£è¡¨äº†æœ€æœ‰å‰æ™¯çš„å‘å±•æ–¹å‘ï¼š

**æ ¸å¿ƒä¼˜åŠ¿ï¼š**

- ğŸ—ï¸ **æ¶æ„ç»Ÿä¸€æ€§**ï¼šä¸­å¤®å¤„ç†æ ¸å¿ƒï¼ˆLLM ä¸»å¹²ï¼‰é…åˆæ¨¡å—åŒ–æ„ŸçŸ¥æ¥å£ï¼Œè‡ªç„¶ä¿ƒè¿›è·¨æ¨¡æ€æ•´åˆ
- âœ¨ **æ¶Œç°å¼ç†è§£**ï¼šåœ¨æµ·é‡ã€å¤šæ ·åŒ–çš„å¤šæ¨¡æ€æ•°æ®é›†ä¸Šé¢„è®­ç»ƒï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿéšå¼å­¦ä¹ ä¸–ç•Œè§„å¾‹ï¼Œè€Œéä¾èµ–äººå·¥è®¾è®¡çš„è§„åˆ™
- ğŸ“ˆ **å¯æ‰©å±•æ€§**ï¼šå·²éªŒè¯çš„ç¼©æ”¾å®šå¾‹è¡¨æ˜ï¼Œèƒ½åŠ›éšå‚æ•°ã€æ•°æ®å’Œè®¡ç®—é‡çš„å¢åŠ è€Œå¯é¢„è§åœ°æå‡
- ğŸ”„ **ååŒå­¦ä¹ **ï¼šä¸åŒçš„ä¸€è‡´æ€§èƒ½åŠ›å¯ä»¥é€šè¿‡è”åˆè®­ç»ƒç›¸äº’æ¶Œç°å’Œå¼ºåŒ–

**æˆ‘ä»¬çš„è®ºç‚¹ï¼š** ç«¯åˆ°ç«¯è®­ç»ƒçš„ç»Ÿä¸€æ¨¡å‹ä¼˜äºç‰¹å®šä»»åŠ¡çš„ä¸“ç”¨æ¨¡å‹æ¥æ„å»ºä¸–ç•Œæ¨¡å‹ï¼Œå› ä¸ºå®ƒä»¬èƒ½å¤Ÿå­¦ä¹ æ¨¡æ€ã€ç©ºé—´å’Œæ—¶é—´ä¹‹é—´çš„æ·±å±‚å…³è”ã€‚

---

## ğŸ”¬ ç‹¬ç«‹æ¢ç´¢ï¼šä¸“ç”¨æ¨¡å‹

åœ¨ç»Ÿä¸€æ¨¡å‹æ—¶ä»£ä¹‹å‰ï¼Œç ”ç©¶äººå‘˜é‡‡ç”¨"åˆ†è€Œæ²»ä¹‹"çš„æ–¹æ³•ï¼Œé’ˆå¯¹æ¯ä¸ªä¸€è‡´æ€§æŒ‘æˆ˜å¼€å‘ä¸“é—¨çš„æ¶æ„ã€‚è¿™äº›å¥ åŸºæ€§å·¥ä½œå»ºç«‹äº†å…³é”®æŠ€æœ¯å’Œæ´è§ï¼Œä¸ºå½“å‰çš„ç»Ÿä¸€æ–¹æ³•æä¾›äº†åŸºç¡€ã€‚

### <a href="./consisency-paper/modality-consistency/README_zh.md">æ¨¡æ€ä¸€è‡´æ€§</a>

![modality](./assets/m.jpg)

**ç›®æ ‡**ï¼šåœ¨ç¬¦å·ï¼ˆè¯­è¨€ï¼‰å’Œæ„ŸçŸ¥ï¼ˆè§†è§‰ï¼‰è¡¨ç¤ºä¹‹é—´å»ºç«‹åŒå‘æ˜ å°„ã€‚

**å†å²æ„ä¹‰**ï¼šè¿™äº›æ¨¡å‹åˆ›å»ºäº†é¦–æ‰¹"ç¬¦å·-æ„ŸçŸ¥æ¡¥æ¢"ï¼Œè§£å†³äº†ä¸–ç•Œæ¨¡å‹çš„åŸºæœ¬è¾“å…¥/è¾“å‡ºé—®é¢˜ã€‚

#### ä»£è¡¨æ€§å·¥ä½œ

<details>
<summary><b>è®ºæ–‡ç›®å½•</b></summary>

* CLIP: Learning Transferable Visual Models From Natural Language Supervision, ICML 2021.
* DALL-E: Zero-Shot Text-to-Image Generation, ICML 2021.
* Show, Attend and Tell: Neural Image Caption Generation with Visual Attention, ICML 2015.
* AttnGAN: Fine-Grained Text-to-Image Generation with Attentional GANs, CVPR 2018.
* NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis, ECCV 2020.
*  Zero-Shot Text-to-Image Generation, arxiv (24 Feb 2021).
*  High-Resolution Image Synthesis with Latent Diffusion Models, arxiv (20 Dec 2021).
*  Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding, arxiv (23 May 2022).
*  Scaling Autoregressive Models for Content-Rich Text-to-Image Generation, arxiv (22 Jun 2022).
*  Language Quantized AutoEncoders: Towards Unsupervised Text-Image Alignment, arxiv (2 Feb 2023).
*  IconShop: Text-Guided Vector Icon Synthesis with Autoregressive Transformers, arxiv (27 Apr 2023).
*  SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis, arxiv (4 Jul 2023).
*  Emu: Generative Pretraining in Multimodality, arxiv (11 Jul 2023).
*  SDXL-Lightning: Progressive Adversarial Diffusion Distillation, arxiv (21 Feb 2024).
*  Beyond Text: Frozen Large Language Models in Visual Signal Comprehension, arxiv (12 Mar 2024).
*  Kandinsky 3.0 Technical Report, arxiv (6 Dec 2023).
*  PixArt-$ä¼ª$: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis, arxiv (30 Sep 2023).
*  EmoGen: Emotional Image Content Generation with Text-to-Image Diffusion Models, arxiv (9 Jan 2024).
*  Parrot: Pareto-optimal Multi-Reward Reinforcement Learning Framework for Text-to-Image Generation, arxiv (11 Jan 2024).
*  DiffusionGPT: LLM-Driven Text-to-Image Generation System, arxiv (18 Jan 2024).
*  MM-Interleaved: Interleaved Image-Text Generative Modeling via Multi-modal Feature Synchronizer, arxiv (18 Jan 2024).
*  Taiyi-Diffusion-XL: Advancing Bilingual Text-to-Image Generation with Large Vision-Language Model Support, arxiv (26 Jan 2024).
*  IntentTuner: An Interactive Framework for Integrating Human Intents in Fine-tuning Text-to-Image Generative Models, arxiv (28 Jan 2024).
*  Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation, arxiv (15 Feb 2024).
*  Universal Prompt Optimizer for Safe Text-to-Image Generation, arxiv (16 Feb 2024).
*  Visual Concept-driven Image Generation with Text-to-Image Diffusion Model, arxiv (18 Feb 2024).
*  A User-Friendly Framework for Generating Model-Preferred Prompts in Text-to-Image Synthesis, arxiv (20 Feb 2024).
*  Scaling Rectified Flow Transformers for High-Resolution Image Synthesis, arxiv (5 Mar 2024).
*  PromptCharm: Text-to-Image Generation through Multi-modal Prompting and Refinement, arxiv (6 Mar 2024).
*  Discriminative Probing and Tuning for Text-to-Image Generation, arxiv (7 Mar 2024).
*  PixArt-å±: Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation, arxiv (7 Mar 2024).
*  A 28.6 mJ/iter Stable Diffusion Processor for Text-to-Image Generation with Patch Similarity-based Sparsity Augmentation and Text-based Mixed-Precision, arxiv (8 Mar 2024).
*  CogView3: Finer and Faster Text-to-Image Generation via Relay Diffusion, arxiv (8 Mar 2024).
*  DivCon: Divide and Conquer for Complex Numerical and Spatial Reasoning in Text-to-Image Generation, arxiv (11 Mar 2024).
*  Block-wise LoRA: Revisiting Fine-grained LoRA for Effective Personalization and Stylization in Text-to-Image Generation, arxiv (12 Mar 2024).
*  Optimizing Negative Prompts for Enhanced Aesthetics and Fidelity in Text-To-Image Generation, arxiv (12 Mar 2024).
*  Bridging Different Language Models and Generative Vision Models for Text-to-Image Generation, arxiv (12 Mar 2024).
*  DialogGen: Multi-modal Interactive Dialogue System for Multi-turn Text-to-Image Generation, arxiv (13 Mar 2024).
*  CLIP-VQDiffusion : Langauge Free Training of Text To Image generation using CLIP and vector quantized diffusion model, arxiv (22 Mar 2024).
*  FlashEval: Towards Fast and Accurate Evaluation of Text-to-image Diffusion Generative Models, arxiv (25 Mar 2024).
*  Skews in the Phenomenon Space Hinder Generalization in Text-to-Image Generation, arxiv (25 Mar 2024).
*  Refining Text-to-Image Generation: Towards Accurate Training-Free Glyph-Enhanced Image Generation, arxiv (25 Mar 2024).
*  Isolated Diffusion: Optimizing Multi-Concept Text-to-Image Generation Training-Freely with Isolated Diffusion Guidance, arxiv (25 Mar 2024).
*  Be Yourself: Bounded Attention for Multi-Subject Text-to-Image Generation, arxiv (25 Mar 2024).
*  Capability-aware Prompt Reformulation Learning for Text-to-Image Generation, arxiv (27 Mar 2024).
*  Evaluating Text-to-Visual Generation with Image-to-Text Generation, arxiv (1 Apr 2024).
*  InstantStyle: Free Lunch towards Style-Preserving in Text-to-Image Generation, arxiv (3 Apr 2024).
*  MULAN: A Multi Layer Annotated Dataset for Controllable Text-to-Image Generation, arxiv (3 Apr 2024).
*  On the Scalability of Diffusion-based Text-to-Image Generation, arxiv (3 Apr 2024).
*  RL for Consistency Models: Faster Reward Guided Text-to-Image Generation, arxiv (25 Mar 2024).
*  Dynamic Prompt Optimizing for Text-to-Image Generation, arxiv (5 Apr 2024).
*  SafeGen: Mitigating Sexually Explicit Content Generation in Text-to-Image Models, arxiv (10 Apr 2024).
*  TextCenGen: Attention-Guided Text-Centric Background Adaptation for Text-to-Image Generation, arxiv (18 Apr 2024).
*  EdgeFusion: On-Device Text-to-Image Generation, arxiv (18 Apr 2024).
*  Object-Attribute Binding in Text-to-Image Generation: Evaluation and Control, arxiv (21 Apr 2024).
*  Towards Better Text-to-Image Generation Alignment via Attention Modulation, arxiv (22 Apr 2024).
*  Multimodal Large Language Model is a Human-Aligned Annotator for Text-to-Image Generation, arxiv (23 Apr 2024).
*  ID-Aligner: Enhancing Identity-Preserving Text-to-Image Generation with Reward Feedback Learning, arxiv (23 Apr 2024).
*  G-Refine: A General Quality Refiner for Text-to-Image Generation, arxiv (29 Apr 2024).
*  On Mechanistic Knowledge Localization in Text-to-Image Generative Models, arxiv (2 May 2024).
*  FlexEControl: Flexible and Efficient Multimodal Control for Text-to-Image Generation, arxiv (8 May 2024).
*  TriLoRA: Integrating SVD for Advanced Style Personalization in Text-to-Image Generation, arxiv (18 May 2024).
*  An Empirical Study and Analysis of Text-to-Image Generation Using Large Language Model-Powered Textual Representation, arxiv (21 May 2024).
*  Personalized Residuals for Concept-Driven Text-to-Image Generation, arxiv (21 May 2024).
*  Kaleido Diffusion: Improving Conditional Diffusion Models with Autoregressive Latent Modeling, arxiv (31 May 2024).
*  AttnDreamBooth: Towards Text-Aligned Personalized Text-to-Image Generation, arxiv (7 Jun 2024).
*  Ctrl-X: Controlling Structure and Appearance for Text-To-Image Generation Without Guidance, arxiv (11 Jun 2024).
*  Commonsense-T2I Challenge: Can Text-to-Image Generation Models Understand Commonsense?, arxiv (11 Jun 2024).
*  Improving Compositional Attribute Binding in Text-to-Image Generative Models via Enhanced Text Embeddings, arxiv (12 Jun 2024).
*  FairCoT: Enhancing Fairness in Text-to-Image Generation via Chain of Thought Reasoning with Multimodal Large Language Models, arxiv (13 Jun 2024).
*  STAR: Scale-wise Text-conditioned AutoRegressive image generation, arxiv (16 Jun 2024).
*  AITTI: Learning Adaptive Inclusive Token for Text-to-Image Generation, arxiv (18 Jun 2024).
*  Beyond Thumbs Up/Down: Untangling Challenges of Fine-Grained Feedback for Text-to-Image Generation, arxiv (24 Jun 2024).
*  MUMU: Bootstrapping Multimodal Image Generation from Text-to-Image Data, arxiv (26 Jun 2024).
*  AnyControl: Create Your Artwork with Versatile Control on Text-to-Image Generation, arxiv (27 Jun 2024).
*  PopAlign: Population-Level Alignment for Fair Text-to-Image Generation, arxiv (28 Jun 2024).
*  Prompt Refinement with Image Pivot for Text-to-Image Generation, arxiv (28 Jun 2024).
*  Efficient Personalized Text-to-image Generation by Leveraging Textual Subspace, arxiv (30 Jun 2024).
*  LLM4GEN: Leveraging Semantic Representation of LLMs for Text-to-Image Generation, arxiv (30 Jun 2024).
*  InstantStyle-Plus: Style Transfer with Content-Preserving in Text-to-Image Generation, arxiv (30 Jun 2024).
*  JeDi: Joint-Image Diffusion Models for Finetuning-Free Personalized Text-to-Image Generation, arxiv (8 Jul 2024).
*  Powerful and Flexible: Personalized Text-to-Image Generation via Reinforcement Learning, arxiv (9 Jul 2024).
*  MARS: Mixture of Auto-Regressive Models for Fine-grained Text-to-image Synthesis, arxiv (10 Jul 2024).
*  Addressing Image Hallucination in Text-to-Image Generation through Factual Image Retrieval, arxiv (15 Jul 2024).
*  Subject-driven Text-to-Image Generation via Preference-based Reinforcement Learning, arxiv (16 Jul 2024).
*  GreenStableYolo: Optimizing Inference Time and Image Quality of Text-to-Image Generation, arxiv (20 Jul 2024).
*  VersusDebias: Universal Zero-Shot Debiasing for Text-to-Image Models via SLM-Based Prompt Engineering and Generative Adversary, arxiv (28 Jul 2024).
*  Reproducibility Study of &#34;ITI-GEN: Inclusive Text-to-Image Generation&#34;, arxiv (29 Jul 2024).
*  VAR-CLIP: Text-to-Image Generator with Visual Auto-Regressive Modeling, arxiv (2 Aug 2024).
*  Lumina-mGPT: Illuminate Flexible Photorealistic Text-to-Image Generation with Multimodal Generative Pretraining, arxiv (5 Aug 2024).
*  FRAP: Faithful and Realistic Text-to-Image Generation with Adaptive Prompt Weighting, arxiv (21 Aug 2024).
*  Rethinking Training for De-biasing Text-to-Image Generation: Unlocking the Potential of Stable Diffusion, arxiv (22 Aug 2024).
*  Taming Text-to-Image Synthesis for Novices: User-centric Prompt Generation via Multi-turn Guidance, arxiv (23 Aug 2024).
*  Focus on Neighbors and Know the Whole: Towards Consistent Dense Multiview Text-to-Image Generator for 3D Creation, arxiv (23 Aug 2024).
*  CSGO: Content-Style Composition in Text-to-Image Generation, arxiv (29 Aug 2024).
*  Text-to-Image Generation Via Energy-Based CLIP, arxiv (30 Aug 2024).
*  SPDiffusion: Semantic Protection Diffusion Models for Multi-concept Text-to-image Generation, arxiv (2 Sep 2024).
*  Qihoo-T2X: An Efficient Proxy-Tokenized Diffusion Transformer for Text-to-Any-Task, arxiv (6 Sep 2024).
*  IFAdapter: Instance Feature Control for Grounded Text-to-Image Generation, arxiv (12 Sep 2024).
*  Generalizing Alignment Paradigm of Text-to-Image Generation with Preferences through $f$-divergence Minimization, arxiv (15 Sep 2024).
*  Evaluating Image Hallucination in Text-to-Image Generation with Question-Answering, arxiv (19 Sep 2024).
*  Text Image Generation for Low-Resource Languages with Dual Translation Learning, arxiv (26 Sep 2024).
*  MCGM: Mask Conditional Text-to-Image Generative Model, arxiv (1 Oct 2024).
*  Accelerating Auto-regressive Text-to-Image Generation with Training-free Speculative Jacobi Decoding, arxiv (2 Oct 2024).
*  ComfyGen: Prompt-Adaptive Workflows for Text-to-Image Generation, arxiv (2 Oct 2024).
*  A Spark of Vision-Language Intelligence: 2-Dimensional Autoregressive Transformer for Efficient Finegrained Image Generation, arxiv (2 Oct 2024).
*  EvolveDirector: Approaching Advanced Text-to-Image Generation with Large Vision-Language Models, arxiv (9 Oct 2024).
*  IterComp: Iterative Composition-Aware Feedback Learning from Model Gallery for Text-to-Image Generation, arxiv (9 Oct 2024).
*  Minority-Focused Text-to-Image Generation via Prompt Optimization, arxiv (10 Oct 2024).
*  DART: Denoising Autoregressive Transformer for Scalable Text-to-Image Generation, arxiv (10 Oct 2024).
*  Meissonic: Revitalizing Masked Generative Transformers for Efficient High-Resolution Text-to-Image Synthesis, arxiv (10 Oct 2024).
*  Text-To-Image with Generative Adversarial Networks, arxiv (11 Oct 2024).
*  Generating Intermediate Representations for Compositional Text-To-Image Generation, arxiv (13 Oct 2024).
*  FlexGen: Flexible Multi-View Generation from Text and Image Inputs, arxiv (14 Oct 2024).
*  HART: Efficient Visual Generation with Hybrid Autoregressive Transformer, arxiv (14 Oct 2024).
*  SAFREE: Training-Free and Adaptive Guard for Safe Text-to-Image And Video Generation, arxiv (16 Oct 2024).
*  Fluid: Scaling Autoregressive Text-to-image Generative Models with Continuous Tokens, arxiv (17 Oct 2024).
*  Synergistic Dual Spatial-aware Generation of Image-to-Text and Text-to-Image, arxiv (20 Oct 2024).
*  Progressive Compositionality in Text-to-Image Generative Models, arxiv (22 Oct 2024).
*  FairQueue: Rethinking Prompt Learning for Fair Text-to-Image Generation, arxiv (24 Oct 2024).
*  Diff-Instruct++: Training One-step Text-to-image Generator Model to Align with Human Preferences, arxiv (24 Oct 2024).
*  Kandinsky 3: Text-to-Image Synthesis for Multifunctional Generative Framework, arxiv (28 Oct 2024).
*  Diffusion Beats Autoregressive: An Evaluation of Compositional Generation in Text-to-Image Models, arxiv (30 Oct 2024).
*  Image2Text2Image: A Novel Framework for Label-Free Evaluation of Image-to-Text Generation with Text-to-Image Diffusion Models, arxiv (8 Nov 2024).
*  Region-Aware Text-to-Image Generation via Hard Binding and Soft Refinement, arxiv (10 Nov 2024).
*  JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation, arxiv (12 Nov 2024).
*  Visual question answering based evaluation metrics for text-to-image generation, arxiv (15 Nov 2024).
*  Safe Text-to-Image Generation: Simply Sanitize the Prompt Embedding, arxiv (15 Nov 2024).
*  High-Resolution Image Synthesis via Next-Token Prediction, arxiv (22 Nov 2024).
*  Automatic Evaluation for Text-to-image Generation: Task-decomposed Framework, Distilled Training, and Meta-evaluation Benchmark, arxiv (23 Nov 2024).
*  Interactive Visual Assessment for Text-to-Image Generation Models, arxiv (23 Nov 2024).
*  ChatGen: Automatic Text-to-Image Generation From FreeStyle Chatting, arxiv (26 Nov 2024).
*  Interleaved Scene Graphs for Interleaved Text-and-Image Generation Assessment, arxiv (26 Nov 2024).
*  Reward Incremental Learning in Text-to-Image Generation, arxiv (26 Nov 2024).
*  Type-R: Automatically Retouching Typos for Text-to-Image Generation, arxiv (27 Nov 2024).
*  Enhancing MMDiT-Based Text-to-Image Models for Similar Subject Generation, arxiv (27 Nov 2024).
*  All Seeds Are Not Equal: Enhancing Compositional Text-to-Image Generation with Reliable Random Seeds, arxiv (27 Nov 2024).
*  Switti: Designing Scale-Wise Transformers for Text-to-Image Synthesis, arxiv (2 Dec 2024).
*  Cross-Attention Head Position Patterns Can Align with Human Visual Concepts in Text-to-Image Generative Models, arxiv (3 Dec 2024).
*  DynamicControl: Adaptive Condition Selection for Improved Text-to-Image Generation, arxiv (4 Dec 2024).
*  Safeguarding Text-to-Image Generation via Inference-Time Prompt-Noise Optimization, arxiv (5 Dec 2024).
*  Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis, arxiv (5 Dec 2024).
*  LayerFusion: Harmonized Multi-Layer Text-to-Image Generation with Generative Priors, arxiv (5 Dec 2024).
*  Proactive Agents for Multi-Turn Text-to-Image Generation Under Uncertainty, arxiv (9 Dec 2024).
*  Boosting Alignment for Post-Unlearning Text-to-Image Generative Models, arxiv (9 Dec 2024).
*  Fast Prompt Alignment for Text-to-Image Generation, arxiv (11 Dec 2024).
*  Preference Adaptive and Sequential Text-to-Image Generation, arxiv (10 Dec 2024).
*  AlignGuard: Scalable Safety Alignment for Text-to-Image Generation, arxiv (13 Dec 2024).
*  Efficient Scaling of Diffusion Transformers for Text-to-Image Generation, arxiv (16 Dec 2024).
*  ArtAug: Enhancing Text-to-Image Generation through Synthesis-Understanding Interaction, arxiv (17 Dec 2024).
*  GALOT: Generative Active Learning via Optimizable Zero-shot Text-to-image Generation, arxiv (18 Dec 2024).
*  Self-Corrected Flow Distillation for Consistent One-Step and Few-Step Text-to-Image Generation, arxiv (22 Dec 2024).
*  Hierarchical Vision-Language Alignment for Text-to-Image Generation via Diffusion Models, arxiv (1 Jan 2025).
*  Evaluating Image Caption via Cycle-consistent Text-to-Image Generation, arxiv (7 Jan 2025).
*  Boosting Text-To-Image Generation via Multilingual Prompting in Large Multimodal Models, arxiv (13 Jan 2025).
*  Democratizing Text-to-Image Masked Generative Models with Compact Text-Aware One-Dimensional Tokens, arxiv (13 Jan 2025).
*  SHYI: Action Support for Contrastive Learning in High-Fidelity Text-to-Image Generation, arxiv (15 Jan 2025).
*  IMAGINE-E: Image Generation Intelligence Evaluation of State-of-the-art Text-to-Image Models, arxiv (23 Jan 2025).
*  Text-to-Image Generation for Vocabulary Learning Using the Keyword Method, arxiv (28 Jan 2025).
*  TextAtlas5M: A Large-scale Dataset for Dense Text Image Generation, arxiv (11 Feb 2025).
*  Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation, arxiv (12 Feb 2025).
*  FlexControl: Computation-Aware ControlNet with Differentiable Router for Text-to-Image Generation, arxiv (11 Feb 2025).
*  REAL: Realism Evaluation of Text-to-Image Generation Models for Effective Data Augmentation, arxiv (15 Feb 2025).
*  Learning to Sample Effective and Diverse Prompts for Text-to-Image Generation, arxiv (17 Feb 2025).
*  CHATS: Combining Human-Aligned Optimization and Test-Time Sampling for Text-to-Image Generation, arxiv (18 Feb 2025).
*  FlipConcept: Tuning-Free Multi-Concept Personalization for Text-to-Image Generation, arxiv (21 Feb 2025).
*  Multi-Agent Multimodal Models for Multicultural Text to Image Generation, arxiv (21 Feb 2025).
*  Multimodal Representation Alignment for Image Generation: Text-Image Interleaved Control Is Easier Than You Think, arxiv (27 Feb 2025).
*  Fine-Grained Alignment and Noise Refinement for Compositional Text-to-Image Generation, arxiv (9 Mar 2025).
*  Unleashing the Potential of Large Language Models for Text-to-Image Generation through Autoregressive Representation Alignment, arxiv (10 Mar 2025).
*  SANA-Sprint: One-Step Diffusion with Continuous-Time Consistency Distillation, arxiv (12 Mar 2025).
*  ConceptGuard: Continual Personalized Text-to-Image Generation with Forgetting and Confusion Mitigation, arxiv (13 Mar 2025).
*  DiT-Air: Revisiting the Efficiency of Diffusion Model Architecture Design in Text to Image Generation, arxiv (13 Mar 2025).
*  TF-TI2I: Training-Free Text-and-Image-to-Image Generation via Multi-Modal Implicit-Context Learning in Text-to-Image Models, arxiv (19 Mar 2025).
*  Zero-Shot Styled Text Image Generation, but Make It Autoregressive, arxiv (21 Mar 2025).
*  Progressive Prompt Detailing for Improved Alignment in Text-to-Image Generative Models, arxiv (22 Mar 2025).
*  Plug-and-Play Interpretable Responsible Text-to-Image Generation via Dual-Space Multi-facet Concept Control, arxiv (24 Mar 2025).
*  Beyond Words: Advancing Long-Text Image Generation via Multimodal Autoregressive Models, arxiv (26 Mar 2025).
*  Lumina-Image 2.0: A Unified and Efficient Image Generative Framework, arxiv (27 Mar 2025).
*  Geometrical Properties of Text Token Embeddings for Strong Semantic Binding in Text-to-Image Generation, arxiv (29 Mar 2025).
*  LayerCraft: Enhancing Text-to-Image Generation with CoT Reasoning and Layered Object Integration, arxiv (25 Mar 2025).
*  Compass Control: Multi Object Orientation Control for Text-to-Image Generation, arxiv (9 Apr 2025).
*  Towards NSFW-Free Text-to-Image Generation via Safety-Constraint Direct Preference Optimization, arxiv (19 Apr 2025).
*  LLM-Enabled Style and Content Regularization for Personalized Text-to-Image Generation, arxiv (19 Apr 2025).
*  RefVNLI: Towards Scalable Evaluation of Subject-driven Text-to-image Generation, arxiv (24 Apr 2025).
*  TextTIGER: Text-based Intelligent Generation with Entity Prompt Refinement for Text-to-Image Generation, arxiv (25 Apr 2025).
*  T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level and Token-level CoT, arxiv (1 May 2025).
*  Deconstructing Bias: A Multifaceted Framework for Diagnosing Cultural and Compositional Inequities in Text-to-Image Generative Models, arxiv (5 Apr 2025).
*  Improving Physical Object State Representation in Text-to-Image Generative Systems, arxiv (4 May 2025).
*  MCCD: Multi-Agent Collaboration-based Compositional Diffusion for Complex Text-to-Image Generation, arxiv (5 May 2025).
*  HCMA: Hierarchical Cross-model Alignment for Grounded Text-to-Image Generation, arxiv (10 May 2025).
*  IMAGE-ALCHEMY: Advancing subject fidelity in personalised text-to-image generation, arxiv (15 May 2025).
*  CompAlign: Improving Compositional Text-to-Image Generation with a Complex Benchmark and Fine-Grained Feedback, arxiv (16 May 2025).
*  Diff-MM: Exploring Pre-trained Text-to-Image Generation Model for Unified Multi-modal Object Tracking, arxiv (19 May 2025).
*  Emerging Properties in Unified Multimodal Pretraining, arxiv (20 May 2025).
*  Harnessing Caption Detailness for Data-Efficient Text-to-Image Generation, arxiv (21 May 2025).
*  IA-T2I: Internet-Augmented Text-to-Image Generation, arxiv (21 May 2025).
*  Self-Rewarding Large Vision-Language Models for Optimizing Prompts in Text-to-Image Generation, arxiv (22 May 2025).
*  RePrompt: Reasoning-Augmented Reprompting for Text-to-Image Generation via Reinforcement Learning, arxiv (23 May 2025).
*  Align Beyond Prompts: Evaluating World Knowledge Alignment in Text-to-Image Generation, arxiv (24 May 2025).
*  Training-free Stylized Text-to-Image Generation with Fast Inference, arxiv (25 May 2025).
*  Alchemist: Turning Public Text-to-Image Data into Generative Gold, arxiv (25 May 2025).
*  StyleAR: Customizing Multimodal Autoregressive Model for Style-Aligned Text-to-Image Generation, arxiv (26 May 2025).
*  Identity-Preserving Text-to-Image Generation via Dual-Level Feature Decoupling and Expert-Guided Fusion, arxiv (28 May 2025).
*  HiDream-I1: A High-Efficient Image Generative Foundation Model with Sparse Diffusion Transformer, arxiv (28 May 2025).
*  Rhetorical Text-to-Image Generation via Two-layer Diffusion Policy Optimization, arxiv (28 May 2025).
*  Muddit: Liberating Generation Beyond Text-to-Image with a Unified Discrete Diffusion Model, arxiv (29 May 2025).
*  OSPO: Object-centric Self-improving Preference Optimization for Text-to-Image Generation, arxiv (28 May 2025).
*  DIMCIM: A Quantitative Evaluation Framework for Default-mode Diversity and Generalization in Text-to-Image Generative Models, arxiv (5 Jun 2025).
*  FocusDiff: Advancing Fine-Grained Text-Image Alignment for Autoregressive Visual Generation through RL, arxiv (5 Jun 2025).
*  A Comprehensive Study of Decoder-Only LLMs for Text-to-Image Generation, arxiv (9 Jun 2025).
*  A High-Quality Dataset and Reliable Evaluation for Interleaved Image-Text Generation, arxiv (11 Jun 2025).
*  ELBO-T2IAlign: A Generic ELBO-Based Method for Calibrating Pixel-level Text-Image Alignment in Diffusion Models, arxiv (11 Jun 2025).
*  Fair Generation without Unfair Distortions: Debiasing Text-to-Image Generation with Entanglement-Free Attention, arxiv (16 Jun 2025).
*  Discrete JEPA: Learning Discrete Token Representations without Reconstruction, arxiv (17 Jun 2025).
*  Cost-Aware Routing for Efficient Text-To-Image Generation, arxiv (17 Jun 2025).
*  NSFW-Classifier Guided Prompt Sanitization for Safe Text-to-Image Generation, arxiv (23 Jun 2025).
*  Ovis-U1 Technical Report, arxiv (29 Jun 2025).
*  RichControl: Structure- and Appearance-Rich Training-Free Spatial Control for Text-to-Image Generation, arxiv (3 Jul 2025).
*  Subject-Consistent and Pose-Diverse Text-to-Image Generation, arxiv (11 Jul 2025).
*  Visual Semantic Description Generation with MLLMs for Image-Text Matching, arxiv (11 Jul 2025).
*  RaDL: Relation-aware Disentangled Learning for Multi-Instance Text-to-Image Generation, arxiv (16 Jul 2025).
*  ID-EA: Identity-driven Text Enhancement and Adaptation with Textual Inversion for Personalized Text-to-Image Generation, arxiv (16 Jul 2025).
*  Local Representative Token Guided Merging for Text-to-Image Generation, arxiv (17 Jul 2025).
*  LSSGen: Leveraging Latent Space Scaling in Flow and Diffusion for Efficient Text to Image Generation, arxiv (22 Jul 2025).
*  Lumina-mGPT 2.0: Stand-Alone AutoRegressive Image Modeling, arxiv (23 Jul 2025).
*  T2I-Copilot: A Training-Free Multi-Agent Text-to-Image System for Enhanced Prompt Interpretation and Interactive Generation, arxiv (28 Jul 2025).
*  Multimodal LLMs as Customized Reward Models for Text-to-Image Generation, arxiv (28 Jul 2025).
*  Qwen-Image Technical Report, arxiv (4 Aug 2025).
*  Documenting Patterns of Exoticism of Marginalized Populations within Text-to-Image Generators, arxiv (4 Aug 2025).
*  Draw Your Mind: Personalized Generation via Condition-Level Modeling in Text-to-Image Diffusion Models, arxiv (5 Aug 2025).
*  LumiGen: An LVLM-Enhanced Iterative Framework for Fine-Grained Text-to-Image Generation, arxiv (5 Aug 2025).
*  UNCAGE: Contrastive Attention Guidance for Masked Generative Transformers in Text-to-Image Generation, arxiv (7 Aug 2025).
*  CoAR: Concept Injection into Autoregressive Models for Personalized Text-to-Image Generation, arxiv (10 Aug 2025).
*  Dual Recursive Feedback on Generation and Appearance Latents for Pose-Robust Text-to-Image Diffusion, arxiv (13 Aug 2025).
*  High Fidelity Text to Image Generation with Contrastive Alignment and Structural Guidance, arxiv (14 Aug 2025).
*  CEIDM: A Controlled Entity and Interaction Diffusion Model for Enhanced Text-to-Image Generation, arxiv (25 Aug 2025).
*  HADIS: Hybrid Adaptive Diffusion Model Serving for Efficient Text-to-Image Generation, arxiv (31 Aug 2025).
*  Data-Driven Loss Functions for Inference-Time Optimization in Text-to-Image Generation, arxiv (2 Sep 2025).
*  PromptEnhancer: A Simple Approach to Enhance Text-to-Image Models via Chain-of-Thought Prompt Rewriting, arxiv (4 Sep 2025).
*  EditIDv2: Editable ID Customization with Data-Lubricated ID Feature Integration for Text-to-Image Generation, arxiv (6 Sep 2025).
*  BiasMap: Leveraging Cross-Attentions to Discover and Mitigate Hidden Social Biases in Text-to-Image Generation, arxiv (16 Sep 2025).
*  DEFT: Decompositional Efficient Fine-Tuning for Text-to-Image Models, arxiv (26 Sep 2025).
*  No Concept Left Behind: Test-Time Optimization for Compositional Text-to-Image Generation, arxiv (27 Sep 2025).
*  Free Lunch Alignment of Text-to-Image Diffusion Models without Preference Image Pairs, arxiv (30 Sep 2025).
*  VQGAN-CLIP: Open Domain Image Generation and Editing with Natural Language Guidance, arxiv (18 Apr 2022).
*  InstructPix2Pix: Learning to Follow Image Editing Instructions, arxiv (17 Nov 2022).
*  MagicBrush: A Manually Annotated Dataset for Instruction-Guided Image Editing, arxiv (16 Jun 2023).
*  An Item is Worth a Prompt: Versatile Image Editing with Disentangled Control, arxiv (7 Mar 2024).
*  InstructGIE: Towards Generalizable Image Editing, arxiv (8 Mar 2024).
*  Leveraging LLMs for On-the-Fly Instruction Guided Image Editing, arxiv (12 Mar 2024).
*  Enhancing Text-to-Image Editing via Hybrid Mask-Informed Fusion, arxiv (24 May 2024).
*  Text Guided Image Editing with Automatic Concept Locating and Forgetting, arxiv (30 May 2024).
*  Empowering Visual Creativity: A Vision-Language Assistant to Image Editing Recommendations, arxiv (31 May 2024).
*  The Curious Case of End Token: A Zero-Shot Disentangled Image Editing using CLIP, arxiv (1 Jun 2024).
*  UltraEdit: Instruction-based Fine-Grained Image Editing at Scale, arxiv (7 Jul 2024).
*  EditScribe: Non-Visual Image Editing with Natural Language Verification Loops, arxiv (13 Aug 2024).
*  FastEdit: Fast Text-Guided Single-Image Editing via Semantic-Aware Diffusion Fine-Tuning, arxiv (6 Aug 2024).
*  TurboEdit: Instant text-based image editing, arxiv (14 Aug 2024).
*  ReEdit: Multimodal Exemplar-Based Image Editing with Diffusion Models, arxiv (6 Nov 2024).
*  Multi-Reward as Condition for Instruction-based Image Editing, arxiv (6 Nov 2024).
*  OmniEdit: Building Image Editing Generalist Models Through Specialist Supervision, arxiv (11 Nov 2024).
*  AnyEdit: Mastering Unified High-Quality Image Editing for Any Idea, arxiv (24 Nov 2024).
*  TPIE: Topology-Preserved Image Editing With Text Instructions, arxiv (22 Nov 2024).
*  InsightEdit: Towards Better Instruction Following for Image Editing, arxiv (26 Nov 2024).
*  UIP2P: Unsupervised Instruction-based Image Editing via Edit Reversibility Constraint, arxiv (19 Dec 2024).
*  Textualize Visual Prompt for Image Editing via Diffusion Bridge, arxiv (7 Jan 2025).
*  Hands-off Image Editing: Language-guided Editing without any Task-specific Labeling, Masking or even Training, arxiv (14 Feb 2025).
*  PromptArtisan: Multi-instruction Image Editing in Single Pass with Complete Attention Control, arxiv (14 Feb 2025).
*  Instruct-CLIP: Improving Instruction-Guided Image Editing with Automated Data Refinement Using Contrastive Learning, arxiv (24 Mar 2025).
*  Tuning-Free Image Editing with Fidelity and Editability via Unified Latent Diffusion Model, arxiv (8 Apr 2025).
*  Omni$^2$: Unifying Omnidirectional Image Generation and Editing in an Omni Model, arxiv (15 Apr 2025).
*  X-Edit: Detecting and Localizing Edits in Images Altered by Text-Guided Diffusion Models, arxiv (16 May 2025).
*  Step1X-Edit: A Practical Framework for General Image Editing, arxiv (24 Apr 2025).
*  FLUX.1 Kontext: Flow Matching for In-Context Image Generation and Editing in Latent Space, arxiv (17 Jun 2025).
*  Towards Efficient Exemplar Based Image Editing with Multimodal VLMs, arxiv (25 Jun 2025).
*  Reasoning to Edit: Hypothetical Instruction-Based Image Editing with Visual Reasoning, arxiv (2 Jul 2025).
*  Beyond Simple Edits: X-Planner for Complex Instruction-Based Image Editing, arxiv (7 Jul 2025).
*  NoHumansRequired: Autonomous High-Quality Image Editing Triplet Mining, arxiv (18 Jul 2025).
*  Qwen-Image Technical Report, arxiv (4 Aug 2025).
*  Talk2Image: A Multi-Agent System for Multi-Turn Image Generation and Editing, arxiv (9 Aug 2025).
*  CannyEdit: Selective Canny Control and Dual-Prompt Guidance for Training-Free Image Editing, arxiv (9 Aug 2025).
*  Exploring Multimodal Diffusion Transformers for Enhanced Prompt-based Image Editing, arxiv (11 Aug 2025).
*  X2Edit: Revisiting Arbitrary-Instruction Image Editing through Self-Constructed Data and Task-Aware Representation Learning, arxiv (11 Aug 2025).
*  An LLM-LVLM Driven Agent for Iterative and Fine-Grained Image Editing, arxiv (24 Aug 2025).
*  Describe, Don&#39;t Dictate: Semantic Image Editing with Natural Language Intent, arxiv (28 Aug 2025).
*  Draw-In-Mind: Rebalancing Designer-Painter Roles in Unified Multimodal Models Benefits Image Editing, arxiv (2 Sep 2025).
*  MultiEdit: Advancing Instruction-based Image Editing on Diverse and Challenging Tasks, arxiv (18 Sep 2025).
*  AutoEdit: Automatic Hyperparameter Tuning for Image Editing, arxiv (18 Sep 2025).
*  CAMILA: Context-Aware Masking for Image Editing with Language Alignment, arxiv (24 Sep 2025).
*  EditVerse: Unifying Image and Video Editing and Generation with In-Context Learning, arxiv (24 Sep 2025).
*  EditScore: Unlocking Online RL for Image Editing via High-Fidelity Reward Modeling, arxiv (28 Sep 2025).
*  EditReward: A Human-Aligned Reward Model for Instruction-Guided Image Editing, arxiv (30 Sep 2025).
*  Query-Kontext: An Unified Multimodal Model for Image Generation and Editing, arxiv (30 Sep 2025).
*  GODIVA: Generating Open-DomaIn Videos from nAtural Descriptions, arxiv (30 Apr 2021).
*  Nè„ºWA: Visual Synthesis Pre-training for Neural visUal World creAtion, arxiv (24 Nov 2021).
*  VideoGen: A Reference-Guided Latent Diffusion Approach for High Definition Text-to-Video Generation, arxiv (1 Sep 2023).
*  Hierarchical Spatio-temporal Decoupling for Text-to-Video Generation, arxiv (7 Dec 2023).
*  UniCtrl: Improving the Spatiotemporal Consistency of Text-to-Video Diffusion Models via Training-Free Unified Attention Control, arxiv (4 Mar 2024).
*  ViD-GPT: Introducing GPT-style Autoregressive Generation in Video Diffusion Models, arxiv (16 Jun 2024).
*  TALC: Time-Aligned Captions for Multi-Scene Text-to-Video Generation, arxiv (7 May 2024).
*  DisenStudio: Customized Multi-subject Text-to-Video Generation with Disentangled Spatial Control, arxiv (21 May 2024).
*  MotionBooth: Motion-Aware Customized Text-to-Video Generation, arxiv (25 Jun 2024).
*  Text-Animator: Controllable Visual Text Video Generation, arxiv (25 Jun 2024).
*  CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer, arxiv (12 Aug 2024).
*  Qihoo-T2X: An Efficient Proxy-Tokenized Diffusion Transformer for Text-to-Any-Task, arxiv (6 Sep 2024).
*  Loong: Generating Minute-level Long Videos with Autoregressive Language Models, arxiv (3 Oct 2024).
*  DiCoDe: Diffusion-Compressed Deep Tokens for Autoregressive Video Generation with Language Models, arxiv (5 Dec 2024).
*  InstanceCap: Improving Text-to-Video Generation via Instance-aware Structured Caption, arxiv (12 Dec 2024).
*  TIV-Diffusion: Towards Object-Centric Movement for Text-driven Image to Video Generation, arxiv (13 Dec 2024).
*  LTX-Video: Realtime Video Latent Diffusion, arxiv (30 Dec 2024).
*  Step-Video-T2V Technical Report: The Practice, Challenges, and Future of Video Foundation Model, arxiv (14 Feb 2025).
*  Wan: Open and Advanced Large-Scale Video Generative Models, arxiv (26 Mar 2025).
*  VPO: Aligning Text-to-Video Generation Models with Prompt Optimization, arxiv (26 Mar 2025).
*  DyST-XL: Dynamic Layout Planning and Content Control for Compositional Text-to-Video Generation, arxiv (21 Apr 2025).
*  ShotAdapter: Text-to-Multi-Shot Video Generation with Diffusion Models, arxiv (12 May 2025).
*  MOVi: Training-free Text-conditioned Multi-Object Video Generation, arxiv (29 May 2025).
*  Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion, arxiv (9 Jun 2025).
*  Lumos-1: On Autoregressive Video Generation from a Unified Model Perspective, arxiv (11 Jul 2025).
*  TITAN-Guide: Taming Inference-Time AligNment for Guided Text-to-Video Diffusion Models, arxiv (1 Aug 2025).
*  VidCLearn: A Continual Learning Approach for Text-to-Video Generation, arxiv (21 Sep 2025).
*  Wan-Alpha: High-Quality Text-to-Video Generation with Alpha Channel, arxiv (29 Sep 2025).
*  TempoControl: Temporal Attention Guidance for Text-to-Video Models, arxiv (2 Oct 2025).
*  CamViG: Camera Aware Image-to-Video Generation with Multimodal Transformers, arxiv (21 May 2024).
*  CamCo: Camera-Controllable 3D-Consistent Image-to-Video Generation, arxiv (4 Jun 2024).
*  JVID: Joint Video-Image Diffusion for Visual-Quality and Temporal-Consistency in Video Generation, arxiv (21 Sep 2024).
*  FrameBridge: Improving Image-to-Video Generation with Bridge Models, arxiv (20 Oct 2024).
*  TIP-I2V: A Million-Scale Real Text and Image Prompt Dataset for Image-to-Video Generation, arxiv (5 Nov 2024).
*  SG-I2V: Self-Guided Trajectory Control in Image-to-Video Generation, arxiv (7 Nov 2024).
*  OmniDrag: Enabling Motion Control for Omnidirectional Image-to-Video Generation, arxiv (12 Dec 2024).
*  TIV-Diffusion: Towards Object-Centric Movement for Text-driven Image to Video Generation, arxiv (13 Dec 2024).
*  Through-The-Mask: Mask-based Motion Trajectories for Image-to-Video Generation, arxiv (6 Jan 2025).
*  VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video Generation, arxiv (11 Feb 2025).
*  RealCam-I2V: Real-World Image-to-Video Generation with Interactive Complex Camera Control, arxiv (14 Feb 2025).
*  TextOCVP: Object-Centric Video Prediction with Language Guidance, arxiv (17 Feb 2025).
*  Dynamic-I2V: Exploring Image-to-Video Generation Models via Multimodal LLM, arxiv (26 May 2025).
*  MotionPro: A Precise Motion Controller for Image-to-Video Generation, arxiv (26 May 2025).
*  Consistent Video Editing as Flow-Driven Image-to-Video Generation, arxiv (9 Jun 2025).
*  Versatile Transition Generation with Image-to-Video Diffusion, arxiv (3 Aug 2025).
*  Zero-shot 3D-Aware Trajectory-Guided image-to-video generation via Test-Time Training, arxiv (8 Sep 2025).
*  Vid-Freeze: Protecting Images from Malicious Image-to-Video Generation via Temporal Freezing, arxiv (27 Sep 2025).
*  MotionRAG: Motion Retrieval-Augmented Image-to-Video Generation, arxiv (30 Sep 2025).
*  Moonshot: Towards Controllable Video Generation and Editing with Multimodal Conditions, arxiv (3 Jan 2024).
*  LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video Editing, arxiv (15 Feb 2024).
*  ExpressEdit: Video Editing with Natural Language and Sketching, arxiv (26 Mar 2024).
*  GenVideo: One-shot Target-image and Shape Aware Video Editing using T2I Diffusion Models, arxiv (18 Apr 2024).
*  Edit-Your-Motion: Space-Time Diffusion Decoupling Learning for Video Motion Editing, arxiv (7 May 2024).
*  Learning Action and Reasoning-Centric Image Editing from Videos and Simulations, arxiv (3 Jul 2024).
*  A Reinforcement Learning-Based Automatic Video Editing Method Using Pre-trained Vision-Language Model, arxiv (7 Nov 2024).
*  VideoDirector: Precise Video Editing via Text-to-Video Models, arxiv (26 Nov 2024).
*  SPAgent: Adaptive Task Decomposition and Model Selection for General Video Generation and Editing, arxiv (28 Nov 2024).
*  DIVE: Taming DINO for Subject-Driven Video Editing, arxiv (4 Dec 2024).
*  Re-Attentional Controllable Video Diffusion Editing, arxiv (16 Dec 2024).
*  Edit as You See: Image-guided Video Editing via Masked Motion Modeling, arxiv (8 Jan 2025).
*  VideoPainter: Any-length Video Inpainting and Editing with Plug-and-Play Context Control, arxiv (7 Mar 2025).
*  VACE: All-in-One Video Creation and Editing, arxiv (10 Mar 2025).
*  VEGGIE: Instructional Editing and Reasoning Video Concepts with Grounded Generation, arxiv (18 Mar 2025).
*  InstructVEdit: A Holistic Approach for Instructional Video Editing, arxiv (22 Mar 2025).
*  OmniV2V: Versatile Video Generation and Editing via Dynamic Content Manipulation, arxiv (2 Jun 2025).
*  Yan: Foundational Interactive Video Generation, arxiv (12 Aug 2025).
*  EditDuet: A Multi-Agent System for Video Non-Linear Editing, arxiv (13 Sep 2025).
*  Prompt-Driven Agentic Video Editing System: Autonomous Comprehension of Long-Form, Story-Driven Media, arxiv (20 Sep 2025).
*  EditVerse: Unifying Image and Video Editing and Generation with In-Context Learning, arxiv (24 Sep 2025).
*  ERNIE-ViLG: Unified Generative Pre-training for Bidirectional Vision-Language Generation, arxiv (31 Dec 2021).
*  Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks, arxiv (17 Jun 2022).
*  Grounding Language Models to Images for Multimodal Inputs and Outputs, arxiv (31 Jan 2023).
*  Any-to-Any Generation via Composable Diffusion, arxiv (19 May 2023).
*  Planting a SEED of Vision in Large Language Model, arxiv (16 Jul 2023).
*  Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization, arxiv (9 Sep 2023).
*  VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation, arxiv (14 Dec 2023).
*  Unifying Generation and Compression: Ultra-low bitrate Image Coding Via Multi-stage Transformer, arxiv (6 Mar 2024).
*  In-Context Translation: Towards Unifying Image Recognition, Processing, and Generation, arxiv (15 Apr 2024).
*  SEED-X: Multimodal Models with Unified Multi-granularity Comprehension and Generation, arxiv (22 Apr 2024).
*  Chameleon: Mixed-Modal Early-Fusion Foundation Models, arxiv (16 May 2024).
*  Libra: Building Decoupled Vision System on Large Language Models, arxiv (16 May 2024).
*  GenArtist: Multimodal LLM as an Agent for Unified Image Generation and Editing, arxiv (8 Jul 2024).
*  ANOLE: An Open, Autoregressive, Native Large Multimodal Models for Interleaved Image-Text Generation, arxiv (8 Jul 2024).
*  Show-o: One Single Transformer to Unify Multimodal Understanding and Generation, arxiv (22 Aug 2024).
*  PUMA: Empowering Unified MLLM with Multi-granular Visual Generation, arxiv (17 Oct 2024).
*  VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation, arxiv (6 Sep 2024).
*  UniMuMo: Unified Text, Music and Motion Generation, arxiv (6 Oct 2024).
*  Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation, arxiv (17 Oct 2024).
*  Orthus: Autoregressive Interleaved Image-Text Generation with Modality-Specific Heads, arxiv (28 Nov 2024).
*  MUSE-VL: Modeling Unified VLM through Semantic Discrete Encoding, arxiv (26 Nov 2024).
*  Liquid: Language Models are Scalable and Unified Multi-modal Generators, arxiv (5 Dec 2024).
*  SILMM: Self-Improving Large Multimodal Models for Compositional Text-to-Image Generation, arxiv (8 Dec 2024).
*  ILLUME: Illuminating Your LLMs to See, Draw, and Self-Enhance, arxiv (9 Dec 2024).
*  Visual Lexicon: Rich Image Features in Language Space, arxiv (9 Dec 2024).
*  SynerGen-VL: Towards Synergistic Image Understanding and Generation with Vision Experts and Token Folding, arxiv (12 Dec 2024).
*  MetaMorph: Multimodal Understanding and Generation via Instruction Tuning, arxiv (18 Dec 2024).
*  LMFusion: Adapting Pretrained Language Models for Multimodal Generation, arxiv (19 Dec 2024).
*  Dual Diffusion for Unified Image Generation and Understanding, arxiv (31 Dec 2024).
*  Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling, arxiv (29 Jan 2025).
*  QLIP: Text-Aligned Visual Tokenization Unifies Auto-Regressive Multimodal Understanding and Generation, arxiv (7 Feb 2025).
*  UniCMs: A Unified Consistency Model For Efficient Multimodal Generation and Understanding, arxiv (8 Feb 2025).
*  UniTok: A Unified Tokenizer for Visual Generation and Understanding, arxiv (27 Feb 2025).
*  MMGen: Unified Multi-modal Image Generation and Understanding in One Go, arxiv (26 Mar 2025).
*  Towards Enhanced Image Generation Via Multi-modal Chain of Thought in Unified Generative Models, arxiv (3 Mar 2025).
*  SemHiTok: A Unified Image Tokenizer via Semantic-Guided Hierarchical Codebook for Multimodal Understanding and Generation, arxiv (9 Mar 2025).
*  WISE: A World Knowledge-Informed Semantic Evaluation for Text-to-Image Generation, arxiv (10 Mar 2025).
*  DualToken: Towards Unifying Visual Understanding and Generation with Dual Visual Vocabularies, arxiv (18 Mar 2025).
*  Unified Multimodal Discrete Diffusion, arxiv (26 Mar 2025).
*  Harmonizing Visual Representations for Unified Multimodal Understanding and Generation, arxiv (27 Mar 2025).
*  YoChameleon: Personalized Vision and Language Generation, arxiv (29 Apr 2025).
*  X-Fusion: Introducing New Modality to Frozen Large Language Models, arxiv (29 Apr 2025).
*  Nexus-Gen: A Unified Model for Image Understanding, Generation, and Editing, arxiv (30 Apr 2025).
*  T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level and Token-level CoT, arxiv (1 May 2025).
*  Ming-Lite-Uni: Advancements in Unified Architecture for Natural Multimodal Interaction, arxiv (5 May 2025).
*  TokLIP: Marry Visual Tokens to CLIP for Multimodal Comprehension and Generation, arxiv (8 May 2025).
*  Selftok: Discrete Visual Tokens of Autoregression, by Diffusion, and for Reasoning, arxiv (12 May 2025).
*  BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture, Training and Dataset, arxiv (14 May 2025).
*  Exploring the Deep Fusion of Large Language Models and Diffusion Transformers for Text-to-Image Synthesis, arxiv (15 May 2025).
*  End-to-End Vision Tokenizer Tuning, arxiv (15 May 2025).
*  UniCTokens: Boosting Personalized Understanding and Generation via Unified Concept Tokens, arxiv (20 May 2025).
*  UniGen: Enhanced Training &amp; Test-Time Strategies for Unified Multimodal Understanding and Generation, arxiv (20 May 2025).
*  OpenUni: A Simple Baseline for Unified Multimodal Understanding and Generation, arxiv (29 May 2025).
*  Muddit: Liberating Generation Beyond Text-to-Image with a Unified Discrete Diffusion Model, arxiv (29 May 2025).
*  UniRL: Self-Improving Unified Multimodal Models via Supervised and Reinforcement Learning, arxiv (29 May 2025).
*  Are Unified Vision-Language Models Necessary: Generalization Across Understanding and Generation, arxiv (29 May 2025).
*  UniWorld-V1: High-Resolution Semantic Encoders for Unified Visual Understanding and Generation, arxiv (3 Jun 2025).
*  LaTtE-Flow: Layerwise Timestep-Expert Flow-based Transformer, arxiv (8 Jun 2025).
*  Symmetrical Flow Matching: Unified Image Generation, Segmentation, and Classification with Score-Based Generative Models, arxiv (12 Jun 2025).
*  Scale Your Instructions: Enhance the Instruction-Following Fidelity of Unified Image Generation Model by Self-Adaptive Attention Scaling, arxiv (22 Jul 2025).
*  OneReward: Unified Mask-Guided Image Generation via Multi-Task Human Preference Learning, arxiv (28 Aug 2025).
*  Reconstruction Alignment Improves Unified Multimodal Models, arxiv (8 Sep 2025).
*  GenExam: A Multidisciplinary Text-to-Image Exam, arxiv (17 Sep 2025).
*  MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid Vision Tokenizer, arxiv (19 Sep 2025).
*  EditVerse: Unifying Image and Video Editing and Generation with In-Context Learning, arxiv (24 Sep 2025).
*  UniAlignment: Semantic Alignment for Unified Image Generation, Understanding, Manipulation and Perception, arxiv (28 Sep 2025).
*  Query-Kontext: An Unified Multimodal Model for Image Generation and Editing, arxiv (30 Sep 2025).
*  M6: A Chinese Multimodal Pretrainer, arxiv (1 Mar 2021).
*  Flamingo: a Visual Language Model for Few-Shot Learning, arxiv (29 Apr 2022).
*  SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs, arxiv (30 Jun 2023).
*  Emu: Generative Pretraining in Multimodality, arxiv (11 Jul 2023).
*  NExT-GPT: Any-to-Any Multimodal LLM, arxiv (11 Sep 2023).
*  Making LLaMA SEE and Draw with SEED Tokenizer, arxiv (2 Oct 2023).
*  Kosmos-G: Generating Images in Context with Multimodal Large Language Models, arxiv (4 Oct 2023).
*  Gemini: A Family of Highly Capable Multimodal Models, arxiv (19 Dec 2023).
*  Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action, arxiv (28 Dec 2023).
*  VARGPT: Unified Understanding and Generation in a Visual Autoregressive Multimodal Large Language Model, arxiv (21 Jan 2025).
*  Generative Multimodal Models are In-Context Learners, arxiv (20 Dec 2023).
*  MM-Interleaved: Interleaved Image-Text Generative Modeling via Multi-modal Feature Synchronizer, arxiv (18 Jan 2024).
*  AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling, arxiv (19 Feb 2024).
*  PVC: Progressive Visual Token Compression for Unified Image and Video Processing in Large Vision-Language Models, arxiv (12 Dec 2024).
*  Omni-RGPT: Unifying Image and Video Region-level Understanding via Token Marks, arxiv (14 Jan 2025).
*  UGen: Unified Autoregressive Multimodal Model with Progressive Vocabulary Learning, arxiv (27 Mar 2025).
*  OmniMamba: Efficient and Unified Multimodal Understanding and Generation via State Space Models, arxiv (11 Mar 2025).
*  VARGPT-v1.1: Improve Visual Autoregressive Large Unified Model via Iterative Instruction Tuning and Reinforcement Learning, arxiv (3 Apr 2025).
*  FUDOKI: Discrete Flow-based Unified Understanding and Generation via Kinetic-Optimal Velocities, arxiv (26 May 2025).
*  MMaDA: Multimodal Large Diffusion Language Models, arxiv (21 May 2025).
*  Many-for-Many: Unify the Training of Multiple Video and Image Generation and Manipulation Tasks, arxiv (2 Jun 2025).
*  Pisces: An Auto-regressive Foundation Model for Image Understanding and Generation, arxiv (12 Jun 2025).
*  Ming-Omni: A Unified Multimodal Model for Perception and Generation, arxiv (11 Jun 2025).
*  UniCode$^2$: Cascaded Large-scale Codebooks for Unified Multimodal Understanding and Generation, arxiv (25 Jun 2025).
*  Ovis-U1 Technical Report, arxiv (29 Jun 2025).
*  UniLiP: Adapting CLIP for Unified Multimodal Understanding, Generation and Editing, arxiv (31 Jul 2025).
*  Hyper-Bagel: A Unified Acceleration Framework for Multimodal Understanding and Generation, arxiv (23 Sep 2025).

</details>

### <a href="./consisency-paper/spatial-consistency/README_zh.md">ç©ºé—´ä¸€è‡´æ€§</a>

![spatial](./assets/s.jpg)

**ç›®æ ‡**ï¼šä½¿æ¨¡å‹èƒ½å¤Ÿä»äºŒç»´è§‚å¯Ÿä¸­ç†è§£å’Œç”Ÿæˆä¸‰ç»´ç©ºé—´ç»“æ„ã€‚

**å†å²æ„ä¹‰**ï¼šä¸ºæ„å»ºå†…éƒ¨"3D åœºæ™¯å›¾"å’Œç†è§£å‡ ä½•å…³ç³»æä¾›äº†æ–¹æ³•è®ºã€‚

#### ä»£è¡¨æ€§å·¥ä½œ

<details>
<summary><b>è®ºæ–‡ç›®å½•</b></summary>

* NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis, ECCV 2020.
* 3D Gaussian Splatting for Real-Time Radiance Field Rendering, SIGGRAPH 2023.
* EG3D: Efficient Geometry-aware 3D Generative Adversarial Networks, CVPR 2022.
* Instant Neural Graphics Primitives with a Multiresolution Hash Encoding, SIGGRAPH 2022.

</details>

### <a href="./consisency-paper/temporal-consistency/README_zh.md">æ—¶é—´ä¸€è‡´æ€§</a>

![temporal](./assets/t.jpg)

**ç›®æ ‡**ï¼šå»ºæ¨¡è§†é¢‘åºåˆ—ä¸­çš„æ—¶é—´åŠ¨æ€ã€ç‰©ä½“è¿åŠ¨å’Œå› æœå…³ç³»ã€‚

**å†å²æ„ä¹‰**ï¼šå¯¹ä¸–ç•Œ"ç‰©ç†å¼•æ“"çš„æ—©æœŸæ¢ç´¢ï¼Œæ•æ‰åœºæ™¯éšæ—¶é—´æ¼”åŒ–çš„è§„å¾‹ã€‚

#### ä»£è¡¨æ€§å·¥ä½œ

<details>
<summary><b>è®ºæ–‡ç›®å½•</b></summary>

* PredRNN: A Recurrent Neural Network for Spatiotemporal Predictive Learning, TPAMI 2023.
* SimVP: Simpler yet Better Video Prediction, CVPR 2022.
* Temporal Attention Unit: Towards Efficient Spatiotemporal Predictive Learning, CVPR 2023.
* VideoGPT: Video Generation using VQ-VAE and Transformers, arXiv 2021.
* Phenaki: Variable Length Video Generation from Open Domain Textual Descriptions, ICLR 2023.

</details>

## ğŸ”— åˆæ­¥èåˆï¼šç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹

å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹æ­£å¼€å§‹æ‰“ç ´å„ä¸ªä¸€è‡´æ€§ä¹‹é—´çš„å£å’ã€‚æœ¬èŠ‚å±•ç¤ºæˆåŠŸæ•´åˆ**ä¸‰å¤§ä¸€è‡´æ€§ä¸­çš„ä¸¤é¡¹**çš„æ¨¡å‹ï¼Œå®ƒä»¬ä»£è¡¨äº†é€šå‘å®Œæ•´ä¸–ç•Œæ¨¡å‹çš„å…³é”®ä¸­é—´æ­¥éª¤ã€‚

### <a href="./consisency-paper/modality+spatial-consistency/README_zh.md">æ¨¡æ€ + ç©ºé—´ä¸€è‡´æ€§</a>

![modality+spatial](./assets/ms.png)

**èƒ½åŠ›ç‰¹å¾**ï¼šèƒ½å¤Ÿå°†æ–‡æœ¬/å›¾åƒæè¿°è½¬æ¢ä¸ºç©ºé—´è¿è´¯çš„ 3D è¡¨ç¤ºæˆ–å¤šè§†è§’ä¸€è‡´è¾“å‡ºçš„æ¨¡å‹ã€‚

**æ„ä¹‰**ï¼šè¿™äº›æ¨¡å‹å±•ç¤ºäº†"3D æƒ³è±¡åŠ›"â€”â€”å®ƒä»¬ä¸å†æ˜¯ç®€å•çš„"2D ç”»å®¶"ï¼Œè€Œæ˜¯ç†è§£ç©ºé—´ç»“æ„çš„"æ•°å­—é›•å¡‘å®¶"ã€‚

#### ä»£è¡¨æ€§å·¥ä½œ

<details>
<summary><b>è®ºæ–‡ç›®å½•</b></summary>

* Zero-1-to-3: Zero-shot One Image to 3D Object, ICCV 2023.
* MVDream: Multi-view Diffusion for 3D Generation, ICLR 2024.
* Wonder3D: Single Image to 3D using Cross-Domain Diffusion, CVPR 2024.
* SyncDreamer: Generating Multiview-consistent Images from a Single-view Image, ICLR 2024.
* DreamFusion: Text-to-3D using 2D Diffusion, ICRL 2023.
* ULIP-2: Towards Scalable Multimodal Pre-training for 3D Understanding, CVPR 2024.
* OpenShape: Scaling Up 3D Shape Representation Towards Open-World Understanding, NeurIPS 2023.
* DreamLLM: Synergistic Multimodal Comprehension and Creation, ICLR 2024.
* EditWorld: Simulating World Dynamics for Instruction-Following Image Editing, ACM Multimedia 2025.
* MIO: A Foundation Model on Multimodal Tokens, EMNLP 2025.
* SGEdit: Bridging LLM with Text2Image Generative Model for Scene Graph-based Image Editing, SIGGRAPH Asia 2024.
* UniReal: Universal Image Generation and Editing via Learning Real-world Dynamics, CVPR 2025.
* ShapeLLM-Omni: A Native Multimodal LLM for 3D Generation and Understanding, NeurIPS 2025.
* Step1X-Edit: A Practical Framework for General Image Editing, arXiv 2025.
* LangScene-X: Reconstruct Generalizable 3D Language-Embedded Scenes with TriMap Video Diffusion, ICCV 2025.
* MENTOR: Efficient Multimodal-Conditioned Tuning for Autoregressive Vision Generation Models, arXiv 2025.
* GSFixer: Improving 3D Gaussian Splatting with Reference-Guided Video Diffusion Priors, arXiv 2025.
* CLIP-NeRF: Text-and-Image Driven Manipulation of Neural Radiance Fields, CVPR 2022.
* Genesis: Multimodal Driving Scene Generation with Spatio-Temporal and Cross-Modal Consistency, arXiv 2025.
* Hunyuan3D-Omni: A Unified Framework for Controllable Generation of 3D Assets, arXiv 2025.
* LERF: Language Embedded Radiance Fields, ICCV 2023.
* Viewset Diffusion: (0-)Image-Conditioned 3D Generative Models from 2D Data, ICCV 2023.
* Aligned Novel View Image and Geometry Synthesis via Cross-modal Attention Instillation, arXiv 2025.
* Scaling Transformer-Based Novel View Synthesis Models with Token Disentanglement and Synthetic Data, ICCV 2025.
* NeRF-HuGS: Improved Neural Radiance Fields in Non-static Scenes Using Heuristics-Guided Segmentation, CVPR 2024.
* RealFusion: 360Â° Reconstruction of Any Object from a Single Image, CVPR 2023.

</details>
  
### <a href="./consisency-paper/modality+temporal-consistency/README_zh.md">æ¨¡æ€ + æ—¶é—´ä¸€è‡´æ€§</a>

![modality+temporal](./assets/mt.png)

**èƒ½åŠ›ç‰¹å¾**ï¼šå°†æ–‡æœ¬æè¿°æˆ–é™æ€å›¾åƒè½¬æ¢ä¸ºæ—¶é—´è¿è´¯çš„åŠ¨æ€è§†é¢‘åºåˆ—çš„æ¨¡å‹ã€‚

**æ„ä¹‰**ï¼šç›®å‰æœ€çªå‡ºçš„èåˆæ–¹å‘ï¼Œå®ç°é«˜è´¨é‡çš„æ–‡æœ¬åˆ°è§†é¢‘å’Œå›¾åƒåˆ°è§†é¢‘ç”Ÿæˆã€‚

#### ä»£è¡¨æ€§å·¥ä½œ

<details>
<summary><b>è®ºæ–‡ç›®å½•</b></summary>

* Lumiere: A Space-Time Diffusion Model for Video Generation, SIGGRAPH-ASIA 2024.
* Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets, arXiv 2023.
* AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning, arXiv 2023.
* Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning, ECCV 2024.
* VideoPoet: A Large Language Model for Zero-Shot Video Generation, ICML 2024.

</details>

### <a href="./consisency-paper/spatial-temporal-consistency/README_zh.md">ç©ºé—´ä¸€è‡´æ€§ + æ—¶é—´ä¸€è‡´æ€§</a>

![spatial-temporal](./assets/st.png)

**èƒ½åŠ›ç‰¹å¾**ï¼šè¿™ç±»æ¨¡å‹èƒ½å¤Ÿåœ¨æ¨¡æ‹Ÿæ—¶é—´åŠ¨æ€æ¼”åŒ–çš„åŒæ—¶ä¿æŒä¸‰ç»´ç©ºé—´ç»“æ„çš„ä¸€è‡´æ€§ï¼Œä½†å¯èƒ½åœ¨è¯­è¨€ç†è§£æˆ–å¯æ§æ€§æ–¹é¢å­˜åœ¨ä¸€å®šå±€é™ã€‚

**æ„ä¹‰**ï¼šè¿™äº›æ¨¡å‹ä»£è¡¨äº†ç†è§£"ä¸‰ç»´ä¸–ç•Œå¦‚ä½•è¿åŠ¨"çš„å…³é”®æŠ€æœ¯æˆå°±ï¼Œæ„æˆäº†ä¸–ç•Œæ¨¡å‹çš„ç‰©ç†å¼•æ“ç»„æˆéƒ¨åˆ†ã€‚

#### ä»£è¡¨æ€§å·¥ä½œ

<details>
<summary><b>è®ºæ–‡ç›®å½•</b></summary>

* DUSt3R: Geometric 3D Vision Made Easy, CVPR 2024.
* 4D Gaussian Splatting for Real-Time Dynamic Scene Rendering, CVPR 2024.
* Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes, CVPR 2021.
* CoTracker: It is Better to Track Together, ECCV 2024.
* GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control, 2024 (open release).

</details>

## ğŸŒŸ "ä¸‰ä½ä¸€ä½“"åŸå‹ï¼šæ¶Œç°çš„ä¸–ç•Œæ¨¡å‹

æœ¬èŠ‚é‡ç‚¹ä»‹ç»å±•ç¤º**ä¸‰å¤§ä¸€è‡´æ€§åˆæ­¥æ•´åˆ**çš„æ¨¡å‹ï¼Œè¡¨ç°å‡ºæ¶Œç°çš„ä¸–ç•Œæ¨¡å‹èƒ½åŠ›ã€‚è¿™äº›ç³»ç»Ÿä»£è¡¨äº†å½“å‰å‰æ²¿ï¼Œå±•ç°äº†çœŸæ­£ä¸–ç•Œæ¨¡æ‹Ÿçš„é›å½¢ã€‚

### <a href="./consisency-paper/world-models/README_zh.md">æ–‡æœ¬åˆ°ä¸–ç•Œç”Ÿæˆå™¨</a>

ä»è¯­è¨€æè¿°ç”ŸæˆåŠ¨æ€ã€ç©ºé—´ä¸€è‡´çš„è™šæ‹Ÿç¯å¢ƒçš„æ¨¡å‹ã€‚

**å…³é”®ç‰¹å¾ï¼š**
- âœ… æ¨¡æ€ä¸€è‡´æ€§ï¼šè‡ªç„¶è¯­è¨€ç†è§£å’Œåƒç´ ç©ºé—´ç”Ÿæˆ
- âœ… ç©ºé—´ä¸€è‡´æ€§ï¼šå…·æœ‰ç‰©ä½“æ’å­˜æ€§çš„ 3D æ„ŸçŸ¥åœºæ™¯ç»„åˆ
- âœ… æ—¶é—´ä¸€è‡´æ€§ï¼šç‰©ç†ä¸Šå¯ä¿¡çš„åŠ¨åŠ›å­¦å’Œè¿åŠ¨

#### ä»£è¡¨æ€§å·¥ä½œ

<details>
<summary><b>è®ºæ–‡ç›®å½•</b></summary>

* Runway Gen-3 Alpha, 2024 (Alpha)
* Pika 1.0, 2023 (November)

</details>

### <a href="./consisency-paper/embodied-intelligence-systems/README_zh.md">å…·èº«æ™ºèƒ½ç³»ç»Ÿ</a>

ä¸ºæœºå™¨äººæ§åˆ¶å’Œè‡ªä¸»æ™ºèƒ½ä½“è®¾è®¡çš„æ¨¡å‹ï¼Œå¿…é¡»æ•´åˆæ„ŸçŸ¥ã€ç©ºé—´æ¨ç†å’Œæ—¶é—´é¢„æµ‹ä»¥æ‰§è¡ŒçœŸå®ä¸–ç•Œä»»åŠ¡ã€‚

**å…³é”®ç‰¹å¾ï¼š**
- å¤šæ¨¡æ€æŒ‡ä»¤éµå¾ª
- 3D ç©ºé—´å¯¼èˆªå’Œæ“ä½œè§„åˆ’
- åŠ¨ä½œåæœçš„é¢„æµ‹å»ºæ¨¡

#### ä»£è¡¨æ€§å·¥ä½œ

<details>
<summary><b>è®ºæ–‡ç›®å½•</b></summary>

* RT-2: Vision-Language-Action Models, CoRL 2023.
* GAIA-1: A Generative World Model for Autonomous Driving, arXiv 2023.
* PaLM-E: An Embodied Multimodal Language Model, ICLR 2024.

</details>

## ğŸ“Š <a href="./consisency-paper/benchmarks+evaluation/README_zh.md">åŸºå‡†æµ‹è¯•ä¸è¯„ä¼°</a>

**å½“å‰æŒ‘æˆ˜**ï¼šç°æœ‰æŒ‡æ ‡ï¼ˆFIDã€FVDã€CLIP Scoreï¼‰æ— æ³•å……åˆ†è¯„ä¼°ä¸–ç•Œæ¨¡å‹èƒ½åŠ›ï¼Œä¾§é‡äºæ„ŸçŸ¥è´¨é‡è€Œéç‰©ç†ç†è§£ã€‚

**ç»¼åˆåŸºå‡†æµ‹è¯•çš„éœ€æ±‚ï¼š**

çœŸæ­£çš„ä¸–ç•Œæ¨¡å‹åŸºå‡†åº”è¯„ä¼°ï¼š
- ğŸ§© **å¸¸è¯†ç‰©ç†ç†è§£**ï¼šæ¨¡å‹æ˜¯å¦éµå®ˆé‡åŠ›ã€åŠ¨é‡ã€å®ˆæ’å®šå¾‹ï¼Ÿ
- ğŸ”® **åäº‹å®æ¨ç†**ï¼šèƒ½å¦é¢„æµ‹å‡è®¾å¹²é¢„çš„ç»“æœï¼Ÿ
- â³ **é•¿æœŸä¸€è‡´æ€§**ï¼šåœ¨æ‰©å±•çš„æ¨¡æ‹Ÿæ—¶é—´èŒƒå›´å†…è¿è´¯æ€§æ˜¯å¦ä¼šå´©æºƒï¼Ÿ
- ğŸ¯ **ç›®æ ‡å¯¼å‘è§„åˆ’**ï¼šèƒ½å¦é“¾æ¥åŠ¨ä½œä»¥å®ç°å¤æ‚ç›®æ ‡ï¼Ÿ
- ğŸ›ï¸ **å¯æ§æ€§**ï¼šç”¨æˆ·èƒ½å¤šç²¾ç¡®åœ°æ“æ§æ¨¡æ‹Ÿå…ƒç´ ï¼Ÿ

#### ä»£è¡¨æ€§å·¥ä½œ

<details>
<summary><b>è®ºæ–‡ç›®å½•</b></summary>

* WISE: A World Knowledge-Informed Semantic Evaluation for Text-to-Image Generation, arXiv 2025.
* Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-COF Benchmark, arXiv 2025.

</details>

## ğŸ“ è´¡çŒ®æŒ‡å—

æˆ‘ä»¬æ¬¢è¿å„ç§å½¢å¼çš„è´¡çŒ®ï¼åŒ…æ‹¬ä½†ä¸é™äºï¼š

- ğŸ†• æ·»åŠ æ–°è®ºæ–‡ã€å·¥å…·æˆ–æ•°æ®é›†
- ğŸ“ æ”¹è¿›ç°æœ‰æ¡ç›®çš„æè¿°
- ğŸ› ä¿®å¤é”™è¯¯æˆ–è¿‡æ—¶ä¿¡æ¯
- ğŸ’¡ æå‡ºæ”¹è¿›å»ºè®®

### å¦‚ä½•è´¡çŒ®

1. **Fork** æœ¬ä»“åº“
2. **åˆ›å»º**ç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingPaper`)
3. **æäº¤**æ›´æ”¹ (`git commit -m 'Add Paper: Amazing World Model'`)
4. **æ¨é€**åˆ°åˆ†æ”¯ (`git push origin feature/AmazingPaper`)
5. **å¼€å¯** Pull Request

### æ¡ç›®æ ¼å¼æ¨¡æ¿

æ·»åŠ è®ºæ–‡æ—¶ï¼Œè¯·ä½¿ç”¨ä»¥ä¸‹æ ¼å¼ï¼š

**[è®ºæ–‡æ ‡é¢˜]**

* **ä½œè€…ï¼š** [ä½œè€…åˆ—è¡¨]
* **å‘è¡¨äºï¼š** [ä¼šè®®/æœŸåˆŠ] [å¹´ä»½]
* **arXiv ID:** [arXiv ç¼–å·ï¼ˆå¦‚é€‚ç”¨ï¼‰]
* **æ ¸å¿ƒåˆ›æ–°ï¼š** [1-2 å¥è¯æ¦‚è¿°]
* **é“¾æ¥ï¼š** [[Paper]](url) | [[Code]](url) | [[Project Page]](url)

<details>
<summary>Abstract</summary>
[å®˜æ–¹æ‘˜è¦]
</details>

<details>
<summary>Key points</summary>
[è¦ç‚¹åˆ—ä¸¾ä¸»è¦è´¡çŒ®]
</details>

---

## â­ Star History

å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹æ‚¨çš„ç ”ç©¶æˆ–ç†è§£æœ‰å¸®åŠ©ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ª Star â­ï¸ï¼

[![Star History Chart](https://api.star-history.com/svg?repos=opendatalab-raiser/awesome-world-model-evolution&type=Date)](https://star-history.com/#opendatalab-raiser/awesome-world-model-evolution&Date)

---

## ğŸ“„ License

This project is licensed under [MIT License](LICENSE).
