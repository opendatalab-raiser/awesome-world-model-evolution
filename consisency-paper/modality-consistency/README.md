<p align="center">
  <a href="README.md">English</a> | <a href="README_zh.md">中文</a>
</p>

### Modality Consistency

**Objective**: Establish bidirectional mappings between symbolic (language) and perceptual (vision) representations.

**Historical Significance**: These models created the first "symbol-perception bridges," solving the fundamental I/O problem for world models.

#### Representative Works


<details>
<summary><b>CLIP: Learning Transferable Visual Models From Natural Language Supervision</b></summary>

* **Authors:** Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever
* **arXiv ID:** 2103.00020
* **One-liner:** Large-scale contrastive learning on 400M image–text pairs enabling strong zero-shot transfer across vision tasks
* **Published in:** ICML 2021
* **Links:** [[Paper]](https://arxiv.org/abs/2103.00020) | [[PDF]](https://arxiv.org/pdf/2103.00020.pdf) | [[Code]](https://github.com/openai/CLIP)

> **Core Innovation**  
> Contrastively trained on 400 million image–text pairs to establish a unified vision–language representation space, enabling zero-shot cross-task transfer and pioneering the large-scale multimodal training paradigm of “using text as a supervisory signal.”

<details>
    <summary>Abstract</summary>
    State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.
</details>

<details>
    <summary>Key points</summary>
    * Contrastive learning between image and text features  
    * Weakly supervised web-scale dataset (400M pairs)  
    * Zero-shot classification via text prompts  
    * Sparked multimodal foundation model wave  
</details>
</details>

---

<details>
<summary><b>DALL-E: Zero-Shot Text-to-Image Generation</b></summary>

* **Authors:** Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever
* **arXiv ID:** 2102.12092
* **One-liner:** First large transformer trained to generate images from text, demonstrating compositional text-to-image creativity
* **Published in:** ICML 2021
* **Links:** [[Paper]](https://arxiv.org/abs/2102.12092) | [[PDF]](https://arxiv.org/pdf/2102.12092.pdf) | [[Project Page]](https://openai.com/research/dall-e)

> **Core Innovation**  
> First to scale a Transformer for text-to-image generation, paired with a discrete VAE codec, demonstrating strong compositional generation of semantics (shape + style + attributes).

<details>
    <summary>Abstract</summary>
    Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.
</details>

<details>
    <summary>Key points</summary>
    * Text-conditioned transformer for image synthesis  
    * Uses VQ-VAE tokenization for images  
    * Compositional reasoning (shape + style + attribute)  
    * Launch of prompt-driven generative vision  
</details>
</details>

---

<details>
<summary><b>Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</b></summary>

* **Authors:** Kelvin Xu, Jimmy Lei Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, Yoshua Bengio
* **arXiv ID:** 1502.03044
* **One-liner:** First visual attention mechanism for image captioning; pioneered attention in vision models
* **Published in:** ICML 2015
* **Links:** [[Paper]](https://arxiv.org/abs/1502.03044) | [[PDF]](https://arxiv.org/pdf/1502.03044.pdf)

> **Core Innovation**  
> First to introduce visual attention into image captioning, enabling the model to focus on key regions and produce interpretable attention maps—laying the groundwork for later Vision Transformers.

<details>
    <summary>Abstract</summary>
    Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.
</details>

<details>
    <summary>Key points</summary>
    * Early attention mechanism for vision  
    * CNN encoder + RNN decoder  
    * Visual heatmaps → interpretability  
    * Influenced attention-based transformers in vision  
</details>
</details>

---

<details>
<summary><b>AttnGAN: Fine-Grained Text-to-Image Generation with Attentional GANs</b></summary>

* **Authors:** Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, Xiaodong He
* **arXiv ID:** 1711.10485
* **One-liner:** Word-region attention and semantic alignment loss for high-fidelity text-to-image synthesis
* **Published in:** CVPR 2018
* **Links:** [[Paper]](https://arxiv.org/abs/1711.10485) | [[PDF]](https://arxiv.org/pdf/1711.10485.pdf) | [[Code]](https://github.com/taoxugit/AttnGAN)

> **Core Innovation**  
> Achieves fine-grained text–image correspondence via word-level attention and semantic-alignment losses, markedly improving both the fidelity and semantic consistency of text-to-image generation.

<details>
    <summary>Abstract</summary>
    In this paper, we propose an Attentional Generative Adversarial Network (AttnGAN) that allows attention-driven, multi-stage refinement for fine-grained text-to-image generation. With a novel attentional generative network, the AttnGAN can synthesize fine-grained details at different subregions of the image by paying attentions to the relevant words in the natural language description. In addition, a deep attentional multimodal similarity model is proposed to compute a fine-grained image-text matching loss for training the generator. The proposed AttnGAN significantly outperforms the previous state of the art, boosting the best reported inception score by 14.14% on the CUB dataset and 170.25% on the more challenging COCO dataset. A detailed analysis is also performed by visualizing the attention layers of the AttnGAN. It for the first time shows that the layered attentional GAN is able to automatically select the condition at the word level for generating different parts of the image.
</details>

<details>
    <summary>Key points</summary>
    * Fine-grained word-to-region attention  
    * Multi-stage image refinement  
    * DAMSM loss for text-image consistency  
    * Major milestone before diffusion models  
</details>
</details>

---


<details>
<summary><b> Zero-Shot Text-to-Image Generation</b></summary>

* **Authors:** Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever
* **arXiv ID:** 2102.12092
* **One-liner:** Developed a simple transformer-based autoregressive model for text-to-image generation that is competitive with domain-specific models in zero-shot evaluation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2102.12092) | [[PDF]](https://arxiv.org/pdf/2102.12092)

> **Core Innovation**
> Achieved competitive zero-shot text-to-image generation by modeling text and image tokens as a single data stream with a transformer architecture.

<details>
    <summary>Abstract</summary>
    Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.
</details>

<details>
    <summary>Key points</summary>
    * Autoregressive modeling of text and image tokens as a single stream
    * Use of transformer architecture for sequence prediction
    * Training with sufficient data and scale for zero-shot capabilities
</details>
</details>

---


<details>
<summary><b> High-Resolution Image Synthesis with Latent Diffusion Models</b></summary>

* **Authors:** Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer
* **arXiv ID:** 2112.10752
* **One-liner:** Introduced latent diffusion models (LDMs) that reduce computational costs while maintaining high image quality and flexibility for tasks like text-to-image generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2112.10752) | [[PDF]](https://arxiv.org/pdf/2112.10752)

> **Core Innovation**
> Applied diffusion models in the latent space of pretrained autoencoders to balance complexity reduction and detail preservation, enabling efficient high-fidelity image synthesis.

<details>
    <summary>Abstract</summary>
    By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at <a href="https://github.com/CompVis/latent-diffusion" rel="external noopener nofollow" class="link-external link-https">this https URL</a> .
</details>

<details>
    <summary>Key points</summary>
    * Training diffusion models in latent space using pretrained autoencoders
    * Introduction of cross-attention layers for conditioning on inputs like text
    * Achievement of state-of-the-art in image inpainting and competitive performance in various tasks
</details>
</details>

---


<details>
<summary><b> Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding</b></summary>

* **Authors:** Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, Mohammad Norouzi
* **arXiv ID:** 2205.11487
* **One-liner:** Created Imagen, a text-to-image diffusion model with exceptional photorealism and language understanding, leveraging large language models for text encoding.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2205.11487) | [[PDF]](https://arxiv.org/pdf/2205.11487)

> **Core Innovation**
> Demonstrated that scaling the language model in Imagen improves sample fidelity and image-text alignment more than scaling the image diffusion model, achieving state-of-the-art FID scores.

<details>
    <summary>Abstract</summary>
    We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment. See <a href="https://imagen.research.google/" rel="external noopener nofollow" class="link-external link-https">this https URL</a> for an overview of the results.
</details>

<details>
    <summary>Key points</summary>
    * Integration of large transformer language models (e.g., T5) for text encoding
    * Use of diffusion models for high-fidelity image generation
    * Introduction of DrawBench for comprehensive evaluation
</details>
</details>

---


<details>
<summary><b> Scaling Autoregressive Models for Content-Rich Text-to-Image Generation</b></summary>

* **Authors:** Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, Yonghui Wu
* **arXiv ID:** 2206.10789
* **One-liner:** Developed Parti, a sequence-to-sequence model for text-to-image generation that scales to 20B parameters, achieving high-fidelity and content-rich image synthesis.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2206.10789) | [[PDF]](https://arxiv.org/pdf/2206.10789)

> **Core Innovation**
> Treated text-to-image generation as a sequence-to-sequence problem, using a transformer-based image tokenizer and scaling model size for improved performance.

<details>
    <summary>Abstract</summary>
    We present the Pathways Autoregressive Text-to-Image (Parti) model, which generates high-fidelity photorealistic images and supports content-rich synthesis involving complex compositions and world knowledge. Parti treats text-to-image generation as a sequence-to-sequence modeling problem, akin to machine translation, with sequences of image tokens as the target outputs rather than text tokens in another language. This strategy can naturally tap into the rich body of prior work on large language models, which have seen continued advances in capabilities and performance through scaling data and model sizes. Our approach is simple: First, Parti uses a Transformer-based image tokenizer, ViT-VQGAN, to encode images as sequences of discrete tokens. Second, we achieve consistent quality improvements by scaling the encoder-decoder Transformer model up to 20B parameters, with a new state-of-the-art zero-shot FID score of 7.23 and finetuned FID score of 3.22 on MS-COCO. Our detailed analysis on Localized Narratives as well as PartiPrompts (P2), a new holistic benchmark of over 1600 English prompts, demonstrate the effectiveness of Parti across a wide variety of categories and difficulty aspects. We also explore and highlight limitations of our models in order to define and exemplify key areas of focus for further improvements. See <a href="https://parti.research.google/" rel="external noopener nofollow" class="link-external link-https">this https URL</a> for high-resolution images.
</details>

<details>
    <summary>Key points</summary>
    * Sequence-to-sequence modeling with image tokens as targets
    * Use of ViT-VQGAN for image tokenization
    * Scaling encoder-decoder transformer up to 20B parameters
</details>
</details>

---


<details>
<summary><b> Language Quantized AutoEncoders: Towards Unsupervised Text-Image Alignment</b></summary>

* **Authors:** Hao Liu, Wilson Yan, Pieter Abbeel
* **arXiv ID:** 2302.00902
* **One-liner:** Proposed LQAE, an unsupervised method to align text and image modalities using pretrained language models, enabling few-shot image classification.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2302.00902) | [[PDF]](https://arxiv.org/pdf/2302.00902)

> **Core Innovation**
> Learned to represent images as sequences of text tokens by quantizing image embeddings with a pretrained language codebook, facilitating multimodal tasks without aligned data.

<details>
    <summary>Abstract</summary>
    Recent progress in scaling up large language models has shown impressive capabilities in performing few-shot learning across a wide range of text-based tasks. However, a key limitation is that these language models fundamentally lack visual perception - a crucial attribute needed to extend these models to be able to interact with the real world and solve vision tasks, such as in visual-question answering and robotics. Prior works have largely connected image to text through pretraining and/or fine-tuning on curated image-text datasets, which can be a costly and expensive process. In order to resolve this limitation, we propose a simple yet effective approach called Language-Quantized AutoEncoder (LQAE), a modification of VQ-VAE that learns to align text-image data in an unsupervised manner by leveraging pretrained language models (e.g., BERT, RoBERTa). Our main idea is to encode image as sequences of text tokens by directly quantizing image embeddings using a pretrained language codebook. We then apply random masking followed by a BERT model, and have the decoder reconstruct the original image from BERT predicted text token embeddings. By doing so, LQAE learns to represent similar images with similar clusters of text tokens, thereby aligning these two modalities without the use of aligned text-image pairs. This enables few-shot image classification with large language models (e.g., GPT-3) as well as linear classification of images based on BERT text features. To the best of our knowledge, our work is the first work that uses unaligned images for multimodal tasks by leveraging the power of pretrained language models.
</details>

<details>
    <summary>Key points</summary>
    * Quantization of image embeddings using pretrained language codebooks
    * Application of random masking and BERT for reconstruction
    * Unsupervised alignment of text and image modalities
</details>
</details>

---


<details>
<summary><b> IconShop: Text-Guided Vector Icon Synthesis with Autoregressive Transformers</b></summary>

* **Authors:** Ronghuan Wu, Wanchao Su, Kede Ma, Jing Liao
* **arXiv ID:** 2304.14400
* **One-liner:** Introduced IconShop, a text-guided vector icon synthesis method using autoregressive transformers for high-quality and diverse SVG generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2304.14400) | [[PDF]](https://arxiv.org/pdf/2304.14400)

> **Core Innovation**
> Sequentialized and tokenized SVG paths and text descriptions to leverage autoregressive transformers for unconditional and text-conditioned icon synthesis.

<details>
    <summary>Abstract</summary>
    Scalable Vector Graphics (SVG) is a popular vector image format that offers good support for interactivity and animation. Despite its appealing characteristics, creating custom SVG content can be challenging for users due to the steep learning curve required to understand SVG grammars or get familiar with professional editing software. Recent advancements in text-to-image generation have inspired researchers to explore vector graphics synthesis using either image-based methods (i.e., text -&gt; raster image -&gt; vector graphics) combining text-to-image generation models with image vectorization, or language-based methods (i.e., text -&gt; vector graphics script) through pretrained large language models. However, these methods still suffer from limitations in terms of generation quality, diversity, and flexibility. In this paper, we introduce IconShop, a text-guided vector icon synthesis method using autoregressive transformers. The key to success of our approach is to sequentialize and tokenize SVG paths (and textual descriptions as guidance) into a uniquely decodable token sequence. With that, we are able to fully exploit the sequence learning power of autoregressive transformers, while enabling both unconditional and text-conditioned icon synthesis. Through standard training to predict the next token on a large-scale vector icon dataset accompanied by textural descriptions, the proposed IconShop consistently exhibits better icon synthesis capability than existing image-based and language-based methods both quantitatively and qualitatively. Meanwhile, we observe a dramatic improvement in generation diversity, which is validated by the objective Uniqueness and Novelty measures. More importantly, we demonstrate the flexibility of IconShop with multiple novel icon synthesis tasks, including icon editing, icon interpolation, icon semantic combination, and icon design auto-suggestion.
</details>

<details>
    <summary>Key points</summary>
    * Tokenization of SVG paths and text into decodable sequences
    * Use of autoregressive transformers for sequence learning
    * Demonstration of flexibility in tasks like icon editing and interpolation
</details>
</details>

---


<details>
<summary><b> SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis</b></summary>

* **Authors:** Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, Robin Rombach
* **arXiv ID:** 2307.01952
* **One-liner:** Presented SDXL, an enhanced latent diffusion model with a larger UNet backbone and novel conditioning schemes for improved text-to-image synthesis.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2307.01952) | [[PDF]](https://arxiv.org/pdf/2307.01952)

> **Core Innovation**
> Increased model parameters and introduced multiple conditioning schemes and aspect ratio training, with a refinement model for better visual fidelity.

<details>
    <summary>Abstract</summary>
    We present SDXL, a latent diffusion model for text-to-image synthesis. Compared to previous versions of Stable Diffusion, SDXL leverages a three times larger UNet backbone: The increase of model parameters is mainly due to more attention blocks and a larger cross-attention context as SDXL uses a second text encoder. We design multiple novel conditioning schemes and train SDXL on multiple aspect ratios. We also introduce a refinement model which is used to improve the visual fidelity of samples generated by SDXL using a post-hoc image-to-image technique. We demonstrate that SDXL shows drastically improved performance compared the previous versions of Stable Diffusion and achieves results competitive with those of black-box state-of-the-art image generators. In the spirit of promoting open research and fostering transparency in large model training and evaluation, we provide access to code and model weights at <a href="https://github.com/Stability-AI/generative-models" rel="external noopener nofollow" class="link-external link-https">this https URL</a>
</details>

<details>
    <summary>Key points</summary>
    * Larger UNet backbone with more attention blocks and cross-attention context
    * Use of a second text encoder and multiple conditioning schemes
    * Introduction of a refinement model for post-hoc improvement
</details>
</details>

---


<details>
<summary><b> Emu: Generative Pretraining in Multimodality</b></summary>

* **Authors:** Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, Xinlong Wang
* **arXiv ID:** 2307.05222
* **One-liner:** Developed Emu, a multimodal foundation model that generates images and texts from any single or multimodal input using a unified autoregressive training process.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2307.05222) | [[PDF]](https://arxiv.org/pdf/2307.05222)

> **Core Innovation**
> Trained end-to-end with a unified objective for next token prediction in multimodal sequences, enabling versatile tasks like image captioning and text-to-image generation.

<details>
    <summary>Abstract</summary>
    We present Emu, a Transformer-based multimodal foundation model, which can seamlessly generate images and texts in multimodal context. This omnivore model can take in any single-modality or multimodal data input indiscriminately (e.g., interleaved image, text and video) through a one-model-for-all autoregressive training process. First, visual signals are encoded into embeddings, and together with text tokens form an interleaved input sequence. Emu is then end-to-end trained with a unified objective of classifying the next text token or regressing the next visual embedding in the multimodal sequence. This versatile multimodality empowers the exploration of diverse pretraining data sources at scale, such as videos with interleaved frames and text, webpages with interleaved images and text, as well as web-scale image-text pairs and video-text pairs. Emu can serve as a generalist multimodal interface for both image-to-text and text-to-image tasks, and supports in-context image and text generation. Across a broad range of zero-shot/few-shot tasks including image captioning, visual question answering, video question answering and text-to-image generation, Emu demonstrates superb performance compared to state-of-the-art large multimodal models. Extended capabilities such as multimodal assistants via instruction tuning are also demonstrated with impressive performance.
</details>

<details>
    <summary>Key points</summary>
    * Unified autoregressive training for multimodal sequences
    * Encoding of visual signals into embeddings combined with text tokens
    * Support for in-context image and text generation
</details>
</details>

---


<details>
<summary><b> SDXL-Lightning: Progressive Adversarial Diffusion Distillation</b></summary>

* **Authors:** Shanchuan Lin, Anran Wang, Xiao Yang
* **arXiv ID:** 2402.13929
* **One-liner:** Proposed a diffusion distillation method for efficient one-step/few-step text-to-image generation based on SDXL, achieving state-of-the-art results.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2402.13929) | [[PDF]](https://arxiv.org/pdf/2402.13929)

> **Core Innovation**
> Combined progressive and adversarial distillation to balance quality and mode coverage, enabling fast inference with high fidelity.

<details>
    <summary>Abstract</summary>
    We propose a diffusion distillation method that achieves new state-of-the-art in one-step/few-step 1024px text-to-image generation based on SDXL. Our method combines progressive and adversarial distillation to achieve a balance between quality and mode coverage. In this paper, we discuss the theoretical analysis, discriminator design, model formulation, and training techniques. We open-source our distilled SDXL-Lightning models both as LoRA and full UNet weights.
</details>

<details>
    <summary>Key points</summary>
    * Progressive and adversarial distillation techniques
    * Distillation applied to SDXL for one-step/few-step generation
    * Open-sourcing of distilled models as LoRA and full UNet weights
</details>
</details>

---


<details>
<summary><b> Beyond Text: Frozen Large Language Models in Visual Signal Comprehension</b></summary>

* **Authors:** Lei Zhu, Fangyun Wei, Yanye Lu
* **arXiv ID:** 2403.07874
* **One-liner:** Introduced V2T Tokenizer to enable large language models to comprehend and process images as linguistic entities without fine-tuning on multimodal data.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2403.07874) | [[PDF]](https://arxiv.org/pdf/2403.07874)

> **Core Innovation**
> Transformed images into discrete words using an encoder-decoder and CLIP model, allowing LLMs to perform visual tasks autoregressively.

<details>
    <summary>Abstract</summary>
    In this work, we investigate the potential of a large language model (LLM) to directly comprehend visual signals without the necessity of fine-tuning on multi-modal datasets. The foundational concept of our method views an image as a linguistic entity, and translates it to a set of discrete words derived from the LLM&#39;s vocabulary. To achieve this, we present the Vision-to-Language Tokenizer, abbreviated as V2T Tokenizer, which transforms an image into a ``foreign language&#39;&#39; with the combined aid of an encoder-decoder, the LLM vocabulary, and a CLIP model. With this innovative image encoding, the LLM gains the ability not only for visual comprehension but also for image denoising and restoration in an auto-regressive fashion-crucially, without any fine-tuning. We undertake rigorous experiments to validate our method, encompassing understanding tasks like image recognition, image captioning, and visual question answering, as well as image denoising tasks like inpainting, outpainting, deblurring, and shift restoration. Code and models are available at <a href="https://github.com/zh460045050/V2L-Tokenizer" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Translation of images to discrete words from LLM vocabulary
    * Use of encoder-decoder and CLIP for image encoding
    * Autoregressive processing for visual comprehension and denoising tasks
</details>
</details>

---


<details>
<summary><b> Kandinsky 3.0 Technical Report</b></summary>

* **Authors:** Vladimir Arkhipkin, Andrei Filatov, Viacheslav Vasilev, Anastasia Maltseva, Said Azizov, Igor Pavlov, Julia Agafonova, Andrey Kuznetsov, Denis Dimitrov
* **arXiv ID:** 2312.03511
* **One-liner:** Introduced Kandinsky 3.0, a high-quality text-to-image model based on latent diffusion.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2312.03511) | [[PDF]](https://arxiv.org/pdf/2312.03511)

> **Core Innovation**
> Achieved higher realism and quality in image generation through architectural improvements and training techniques.

<details>
    <summary>Abstract</summary>
    We present Kandinsky 3.0, a large-scale text-to-image generation model based on latent diffusion, continuing the series of text-to-image Kandinsky models and reflecting our progress to achieve higher quality and realism of image generation. In this report we describe the architecture of the model, the data collection procedure, the training technique, and the production system for user interaction. We focus on the key components that, as we have identified as a result of a large number of experiments, had the most significant impact on improving the quality of our model compared to the others. We also describe extensions and applications of our model, including super resolution, inpainting, image editing, image-to-video generation, and a distilled version of Kandinsky 3.0 - Kandinsky 3.1, which does inference in 4 steps of the reverse process and 20 times faster without visual quality decrease. By side-by-side human preferences comparison, Kandinsky becomes better in text understanding and works better on specific domains. The code is available at <a href="https://github.com/ai-forever/Kandinsky-3" rel="external noopener nofollow" class="link-external link-https">this https URL</a>
</details>

<details>
    <summary>Key points</summary>
    * Latent diffusion architecture
    * Training technique optimization
    * Extensions like super-resolution and inpainting
    * Distilled version for faster inference
</details>
</details>

---


<details>
<summary><b> PixArt-$α$: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis</b></summary>

* **Authors:** Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, Zhenguo Li
* **arXiv ID:** 2310.00426
* **One-liner:** Developed PIXART-α, a low-cost Transformer-based text-to-image model with competitive quality.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2310.00426) | [[PDF]](https://arxiv.org/pdf/2310.00426)

> **Core Innovation**
> Reduced training costs and CO2 emissions while maintaining high-resolution image synthesis.

<details>
    <summary>Abstract</summary>
    The most advanced text-to-image (T2I) models require significant training costs (e.g., millions of GPU hours), seriously hindering the fundamental innovation for the AIGC community while increasing CO2 emissions. This paper introduces PIXART-$\alpha$, a Transformer-based T2I diffusion model whose image generation quality is competitive with state-of-the-art image generators (e.g., Imagen, SDXL, and even Midjourney), reaching near-commercial application standards. Additionally, it supports high-resolution image synthesis up to 1024px resolution with low training cost, as shown in Figure 1 and 2. To achieve this goal, three core designs are proposed: (1) Training strategy decomposition: We devise three distinct training steps that separately optimize pixel dependency, text-image alignment, and image aesthetic quality; (2) Efficient T2I Transformer: We incorporate cross-attention modules into Diffusion Transformer (DiT) to inject text conditions and streamline the computation-intensive class-condition branch; (3) High-informative data: We emphasize the significance of concept density in text-image pairs and leverage a large Vision-Language model to auto-label dense pseudo-captions to assist text-image alignment learning. As a result, PIXART-$\alpha$&#39;s training speed markedly surpasses existing large-scale T2I models, e.g., PIXART-$\alpha$ only takes 10.8% of Stable Diffusion v1.5&#39;s training time (675 vs. 6,250 A100 GPU days), saving nearly \$300,000 (\$26,000 vs. \$320,000) and reducing 90% CO2 emissions. Moreover, compared with a larger SOTA model, RAPHAEL, our training cost is merely 1%. Extensive experiments demonstrate that PIXART-$\alpha$ excels in image quality, artistry, and semantic control. We hope PIXART-$\alpha$ will provide new insights to the AIGC community and startups to accelerate building their own high-quality yet low-cost generative models from scratch.
</details>

<details>
    <summary>Key points</summary>
    * Training strategy decomposition into three steps
    * Efficient T2I Transformer with cross-attention
    * Use of high-informative data with pseudo-captions
</details>
</details>

---


<details>
<summary><b> EmoGen: Emotional Image Content Generation with Text-to-Image Diffusion Models</b></summary>

* **Authors:** Jingyuan Yang, Jiawei Feng, Hui Huang
* **arXiv ID:** 2401.04608
* **One-liner:** Proposed Emotional Image Content Generation (EICG) for generating emotion-faithful images.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2401.04608) | [[PDF]](https://arxiv.org/pdf/2401.04608)

> **Core Innovation**
> Enabled generation of semantic-clear images based on abstract emotions using CLIP alignment.

<details>
    <summary>Abstract</summary>
    Recent years have witnessed remarkable progress in image generation task, where users can create visually astonishing images with high-quality. However, existing text-to-image diffusion models are proficient in generating concrete concepts (dogs) but encounter challenges with more abstract ones (emotions). Several efforts have been made to modify image emotions with color and style adjustments, facing limitations in effectively conveying emotions with fixed image contents. In this work, we introduce Emotional Image Content Generation (EICG), a new task to generate semantic-clear and emotion-faithful images given emotion categories. Specifically, we propose an emotion space and construct a mapping network to align it with the powerful Contrastive Language-Image Pre-training (CLIP) space, providing a concrete interpretation of abstract emotions. Attribute loss and emotion confidence are further proposed to ensure the semantic diversity and emotion fidelity of the generated images. Our method outperforms the state-of-the-art text-to-image approaches both quantitatively and qualitatively, where we derive three custom metrics, i.e., emotion accuracy, semantic clarity and semantic diversity. In addition to generation, our method can help emotion understanding and inspire emotional art design.
</details>

<details>
    <summary>Key points</summary>
    * Emotion space mapping to CLIP space
    * Attribute loss for semantic diversity
    * Emotion confidence for fidelity
</details>
</details>

---


<details>
<summary><b> Parrot: Pareto-optimal Multi-Reward Reinforcement Learning Framework for Text-to-Image Generation</b></summary>

* **Authors:** Seung Hyun Lee, Yinxiao Li, Junjie Ke, Innfarn Yoo, Han Zhang, Jiahui Yu, Qifei Wang, Fei Deng, Glenn Entis, Junfeng He, Gang Li, Sangpil Kim, Irfan Essa, Feng Yang
* **arXiv ID:** 2401.05675
* **One-liner:** Introduced Parrot for multi-objective optimization in text-to-image generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2401.05675) | [[PDF]](https://arxiv.org/pdf/2401.05675)

> **Core Innovation**
> Automated reward trade-off to improve image quality without manual weight adjustment.

<details>
    <summary>Abstract</summary>
    Recent works have demonstrated that using reinforcement learning (RL) with multiple quality rewards can improve the quality of generated images in text-to-image (T2I) generation. However, manually adjusting reward weights poses challenges and may cause over-optimization in certain metrics. To solve this, we propose Parrot, which addresses the issue through multi-objective optimization and introduces an effective multi-reward optimization strategy to approximate Pareto optimal. Utilizing batch-wise Pareto optimal selection, Parrot automatically identifies the optimal trade-off among different rewards. We use the novel multi-reward optimization algorithm to jointly optimize the T2I model and a prompt expansion network, resulting in significant improvement of image quality and also allow to control the trade-off of different rewards using a reward related prompt during inference. Furthermore, we introduce original prompt-centered guidance at inference time, ensuring fidelity to user input after prompt expansion. Extensive experiments and a user study validate the superiority of Parrot over several baselines across various quality criteria, including aesthetics, human preference, text-image alignment, and image sentiment.
</details>

<details>
    <summary>Key points</summary>
    * Multi-objective optimization with Pareto optimal selection
    * Joint optimization of T2I model and prompt expansion
    * Original prompt-centered guidance
</details>
</details>

---


<details>
<summary><b> DiffusionGPT: LLM-Driven Text-to-Image Generation System</b></summary>

* **Authors:** Jie Qin, Jie Wu, Weifeng Chen, Yuxi Ren, Huixia Li, Hefeng Wu, Xuefeng Xiao, Rui Wang, Shilei Wen
* **arXiv ID:** 2401.10061
* **One-liner:** Proposed DiffusionGPT, a unified system for diverse prompts and model integration using LLMs.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2401.10061) | [[PDF]](https://arxiv.org/pdf/2401.10061)

> **Core Innovation**
> Leveraged LLMs to select appropriate generative models based on input prompts.

<details>
    <summary>Abstract</summary>
    Diffusion models have opened up new avenues for the field of image generation, resulting in the proliferation of high-quality models shared on open-source platforms. However, a major challenge persists in current text-to-image systems are often unable to handle diverse inputs, or are limited to single model results. Current unified attempts often fall into two orthogonal aspects: i) parse Diverse Prompts in input stage; ii) activate expert model to output. To combine the best of both worlds, we propose DiffusionGPT, which leverages Large Language Models (LLM) to offer a unified generation system capable of seamlessly accommodating various types of prompts and integrating domain-expert models. DiffusionGPT constructs domain-specific Trees for various generative models based on prior knowledge. When provided with an input, the LLM parses the prompt and employs the Trees-of-Thought to guide the selection of an appropriate model, thereby relaxing input constraints and ensuring exceptional performance across diverse domains. Moreover, we introduce Advantage Databases, where the Tree-of-Thought is enriched with human feedback, aligning the model selection process with human preferences. Through extensive experiments and comparisons, we demonstrate the effectiveness of DiffusionGPT, showcasing its potential for pushing the boundaries of image synthesis in diverse domains.
</details>

<details>
    <summary>Key points</summary>
    * Domain-specific Trees for model selection
    * Use of Large Language Models for parsing
    * Advantage Databases with human feedback
</details>
</details>

---


<details>
<summary><b> MM-Interleaved: Interleaved Image-Text Generative Modeling via Multi-modal Feature Synchronizer</b></summary>

* **Authors:** Changyao Tian, Xizhou Zhu, Yuwen Xiong, Weiyun Wang, Zhe Chen, Wenhai Wang, Yuntao Chen, Lewei Lu, Tong Lu, Jie Zhou, Hongsheng Li, Yu Qiao, Jifeng Dai
* **arXiv ID:** 2401.10208
* **One-liner:** Presented MM-Interleaved, an end-to-end model for interleaved image-text data generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2401.10208) | [[PDF]](https://arxiv.org/pdf/2401.10208)

> **Core Innovation**
> Improved handling of multi-image scenarios with fine-grained feature access.

<details>
    <summary>Abstract</summary>
    Developing generative models for interleaved image-text data has both research and practical value. It requires models to understand the interleaved sequences and subsequently generate images and text. However, existing attempts are limited by the issue that the fixed number of visual tokens cannot efficiently capture image details, which is particularly problematic in the multi-image scenarios. To address this, this paper presents MM-Interleaved, an end-to-end generative model for interleaved image-text data. It introduces a multi-scale and multi-image feature synchronizer module, allowing direct access to fine-grained image features in the previous context during the generation process. MM-Interleaved is end-to-end pre-trained on both paired and interleaved image-text corpora. It is further enhanced through a supervised fine-tuning phase, wherein the model improves its ability to follow complex multi-modal instructions. Experiments demonstrate the versatility of MM-Interleaved in recognizing visual details following multi-modal instructions and generating consistent images following both textual and visual conditions. Code and models are available at \url{<a href="https://github.com/OpenGVLab/MM-Interleaved" rel="external noopener nofollow" class="link-external link-https">this https URL</a>}.
</details>

<details>
    <summary>Key points</summary>
    * Multi-scale and multi-image feature synchronizer
    * End-to-end pre-training on interleaved data
    * Supervised fine-tuning for multi-modal instructions
</details>
</details>

---


<details>
<summary><b> Taiyi-Diffusion-XL: Advancing Bilingual Text-to-Image Generation with Large Vision-Language Model Support</b></summary>

* **Authors:** Xiaojun Wu, Dixiang Zhang, Ruyi Gan, Junyu Lu, Ziwei Wu, Renliang Sun, Jiaxing Zhang, Pingjian Zhang, Yan Song
* **arXiv ID:** 2401.14688
* **One-liner:** Developed Taiyi-Diffusion-XL, a bilingual text-to-image model for Chinese and English.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2401.14688) | [[PDF]](https://arxiv.org/pdf/2401.14688)

> **Core Innovation**
> Extended CLIP and Stable-Diffusion-XL with bilingual support and enhanced prompts.

<details>
    <summary>Abstract</summary>
    Recent advancements in text-to-image models have significantly enhanced image generation capabilities, yet a notable gap of open-source models persists in bilingual or Chinese language support. To address this need, we present Taiyi-Diffusion-XL, a new Chinese and English bilingual text-to-image model which is developed by extending the capabilities of CLIP and Stable-Diffusion-XL through a process of bilingual continuous pre-training. This approach includes the efficient expansion of vocabulary by integrating the most frequently used Chinese characters into CLIP&#39;s tokenizer and embedding layers, coupled with an absolute position encoding expansion. Additionally, we enrich text prompts by large vision-language model, leading to better images captions and possess higher visual quality. These enhancements are subsequently applied to downstream text-to-image models. Our empirical results indicate that the developed CLIP model excels in bilingual image-text <a href="http://retrieval.Furthermore" rel="external noopener nofollow" class="link-external link-http">this http URL</a>, the bilingual image generation capabilities of Taiyi-Diffusion-XL surpass previous models. This research leads to the development and open-sourcing of the Taiyi-Diffusion-XL model, representing a notable advancement in the field of image generation, particularly for Chinese language applications. This contribution is a step forward in addressing the need for more diverse language support in multimodal research. The model and demonstration are made publicly available at \href{<a href="https://huggingface.co/IDEA-CCNL/Taiyi-Stable-Diffusion-XL-3.5B/" rel="external noopener nofollow" class="link-external link-https">this https URL</a>}, fostering further research and collaboration in this domain.
</details>

<details>
    <summary>Key points</summary>
    * Vocabulary expansion for Chinese characters
    * Bilingual continuous pre-training
    * Enrichment of text prompts with vision-language models
</details>
</details>

---


<details>
<summary><b> IntentTuner: An Interactive Framework for Integrating Human Intents in Fine-tuning Text-to-Image Generative Models</b></summary>

* **Authors:** Xingchen Zeng, Ziyao Gao, Yilin Ye, Wei Zeng
* **arXiv ID:** 2401.15559
* **One-liner:** Proposed IntentTuner, an interactive framework for intent-aligned fine-tuning of T2I models.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2401.15559) | [[PDF]](https://arxiv.org/pdf/2401.15559)

> **Core Innovation**
> Streamlined fine-tuning by incorporating user intentions and automated data augmentation.

<details>
    <summary>Abstract</summary>
    Fine-tuning facilitates the adaptation of text-to-image generative models to novel concepts (e.g., styles and portraits), empowering users to forge creatively customized content. Recent efforts on fine-tuning focus on reducing training data and lightening computation overload but neglect alignment with user intentions, particularly in manual curation of multi-modal training data and intent-oriented evaluation. Informed by a formative study with fine-tuning practitioners for comprehending user intentions, we propose IntentTuner, an interactive framework that intelligently incorporates human intentions throughout each phase of the fine-tuning workflow. IntentTuner enables users to articulate training intentions with imagery exemplars and textual descriptions, automatically converting them into effective data augmentation strategies. Furthermore, IntentTuner introduces novel metrics to measure user intent alignment, allowing intent-aware monitoring and evaluation of model training. Application exemplars and user studies demonstrate that IntentTuner streamlines fine-tuning, reducing cognitive effort and yielding superior models compared to the common baseline tool.
</details>

<details>
    <summary>Key points</summary>
    * Intent articulation with exemplars and descriptions
    * Automatic data augmentation strategies
    * Novel metrics for intent alignment evaluation
</details>
</details>

---


<details>
<summary><b> Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation</b></summary>

* **Authors:** Huizhuo Yuan, Zixiang Chen, Kaixuan Ji, Quanquan Gu
* **arXiv ID:** 2402.10210
* **One-liner:** Introduced SPIN-Diffusion, a self-play fine-tuning method for diffusion models.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2402.10210) | [[PDF]](https://arxiv.org/pdf/2402.10210)

> **Core Innovation**
> Enabled iterative self-improvement without additional human preference data.

<details>
    <summary>Abstract</summary>
    Fine-tuning Diffusion Models remains an underexplored frontier in generative artificial intelligence (GenAI), especially when compared with the remarkable progress made in fine-tuning Large Language Models (LLMs). While cutting-edge diffusion models such as Stable Diffusion (SD) and SDXL rely on supervised fine-tuning, their performance inevitably plateaus after seeing a certain volume of data. Recently, reinforcement learning (RL) has been employed to fine-tune diffusion models with human preference data, but it requires at least two images (&#34;winner&#34; and &#34;loser&#34; images) for each text prompt. In this paper, we introduce an innovative technique called self-play fine-tuning for diffusion models (SPIN-Diffusion), where the diffusion model engages in competition with its earlier versions, facilitating an iterative self-improvement process. Our approach offers an alternative to conventional supervised fine-tuning and RL strategies, significantly improving both model performance and alignment. Our experiments on the Pick-a-Pic dataset reveal that SPIN-Diffusion outperforms the existing supervised fine-tuning method in aspects of human preference alignment and visual appeal right from its first iteration. By the second iteration, it exceeds the performance of RLHF-based methods across all metrics, achieving these results with less data.
</details>

<details>
    <summary>Key points</summary>
    * Self-play competition with earlier model versions
    * Alternative to supervised fine-tuning and RL
    * Improved alignment and visual appeal
</details>
</details>

---


<details>
<summary><b> Universal Prompt Optimizer for Safe Text-to-Image Generation</b></summary>

* **Authors:** Zongyu Wu, Hongcheng Gao, Yueze Wang, Xiang Zhang, Suhang Wang
* **arXiv ID:** 2402.10882
* **One-liner:** Proposed POSI, a universal prompt optimizer for safe text-to-image generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2402.10882) | [[PDF]](https://arxiv.org/pdf/2402.10882)

> **Core Innovation**
> Reduced generation of unsafe content by converting toxic prompts to clean ones.

<details>
    <summary>Abstract</summary>
    Text-to-Image (T2I) models have shown great performance in generating images based on textual prompts. However, these models are vulnerable to unsafe input to generate unsafe content like sexual, harassment and illegal-activity images. Existing studies based on image checker, model fine-tuning and embedding blocking are impractical in real-world applications. Hence, we propose the first universal prompt optimizer for safe T2I (POSI) generation in black-box scenario. We first construct a dataset consisting of toxic-clean prompt pairs by GPT-3.5 Turbo. To guide the optimizer to have the ability of converting toxic prompt to clean prompt while preserving semantic information, we design a novel reward function measuring toxicity and text alignment of generated images and train the optimizer through Proximal Policy Optimization. Experiments show that our approach can effectively reduce the likelihood of various T2I models in generating inappropriate images, with no significant impact on text alignment. It is also flexible to be combined with methods to achieve better performance. Our code is available at <a href="https://github.com/wu-zongyu/POSI" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Dataset construction with toxic-clean pairs
    * Reward function for toxicity and text alignment
    * Training via Proximal Policy Optimization
</details>
</details>

---


<details>
<summary><b> Visual Concept-driven Image Generation with Text-to-Image Diffusion Model</b></summary>

* **Authors:** Tanzila Rahman, Shweta Mahajan, Hsin-Ying Lee, Jian Ren, Sergey Tulyakov, Leonid Sigal
* **arXiv ID:** 2402.11487
* **One-liner:** Proposed a concept-driven personalization framework for TTI models that enables generation with multiple interacting and entangled concepts.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2402.11487) | [[PDF]](https://arxiv.org/pdf/2402.11487)

> **Core Innovation**
> Joint learning of custom tokens and latent segmentation masks using an EM-like optimization to disentangle concepts in user-provided images.

<details>
    <summary>Abstract</summary>
    Text-to-image (TTI) diffusion models have demonstrated impressive results in generating high-resolution images of complex and imaginative scenes. Recent approaches have further extended these methods with personalization techniques that allow them to integrate user-illustrated concepts (e.g., the user him/herself) using a few sample image illustrations. However, the ability to generate images with multiple interacting concepts, such as human subjects, as well as concepts that may be entangled in one, or across multiple, image illustrations remains illusive. In this work, we propose a concept-driven TTI personalization framework that addresses these core challenges. We build on existing works that learn custom tokens for user-illustrated concepts, allowing those to interact with existing text tokens in the TTI model. However, importantly, to disentangle and better learn the concepts in question, we jointly learn (latent) segmentation masks that disentangle these concepts in user-provided image illustrations. We do so by introducing an Expectation Maximization (EM)-like optimization procedure where we alternate between learning the custom tokens and estimating (latent) masks encompassing corresponding concepts in user-supplied images. We obtain these masks based on cross-attention, from within the U-Net parameterized latent diffusion model and subsequent DenseCRF optimization. We illustrate that such joint alternating refinement leads to the learning of better tokens for concepts and, as a by-product, latent masks. We illustrate the benefits of the proposed approach qualitatively and quantitatively with several examples and use cases that can combine three or more entangled concepts.
</details>

<details>
    <summary>Key points</summary>
    * Learn custom tokens for user-illustrated concepts
    * Jointly learn latent segmentation masks via cross-attention and DenseCRF optimization
    * Alternate optimization between tokens and masks for better concept learning
</details>
</details>

---


<details>
<summary><b> A User-Friendly Framework for Generating Model-Preferred Prompts in Text-to-Image Synthesis</b></summary>

* **Authors:** Nailei Hei, Qianyu Guo, Zihao Wang, Yan Wang, Haofen Wang, Wenqiang Zhang
* **arXiv ID:** 2402.12760
* **One-liner:** Developed an automated prompt optimization framework to bridge the gap between novice user inputs and model-preferred prompts.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2402.12760) | [[PDF]](https://arxiv.org/pdf/2402.12760)

> **Core Innovation**
> Introduced a User-Friendly Fine-Grained Text Generation framework and a Coarse-Fine Granularity Prompts dataset for automated prompt refinement.

<details>
    <summary>Abstract</summary>
    Well-designed prompts have demonstrated the potential to guide text-to-image models in generating amazing images. Although existing prompt engineering methods can provide high-level guidance, it is challenging for novice users to achieve the desired results by manually entering prompts due to a discrepancy between novice-user-input prompts and the model-preferred prompts. To bridge the distribution gap between user input behavior and model training datasets, we first construct a novel Coarse-Fine Granularity Prompts dataset (CFP) and propose a novel User-Friendly Fine-Grained Text Generation framework (UF-FGTG) for automated prompt optimization. For CFP, we construct a novel dataset for text-to-image tasks that combines coarse and fine-grained prompts to facilitate the development of automated prompt generation methods. For UF-FGTG, we propose a novel framework that automatically translates user-input prompts into model-preferred prompts. Specifically, we propose a prompt refiner that continually rewrites prompts to empower users to select results that align with their unique needs. Meanwhile, we integrate image-related loss functions from the text-to-image model into the training process of text generation to generate model-preferred prompts. Additionally, we propose an adaptive feature extraction module to ensure diversity in the generated results. Experiments demonstrate that our approach is capable of generating more visually appealing and diverse images than previous state-of-the-art methods, achieving an average improvement of 5% across six quality and aesthetic metrics.
</details>

<details>
    <summary>Key points</summary>
    * Constructed CFP dataset with coarse and fine-grained prompts
    * Implemented prompt refiner for continuous rewriting
    * Integrated image-related loss functions and adaptive feature extraction for diversity
</details>
</details>

---


<details>
<summary><b> Scaling Rectified Flow Transformers for High-Resolution Image Synthesis</b></summary>

* **Authors:** Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, Robin Rombach
* **arXiv ID:** 2403.03206
* **One-liner:** Improved rectified flow models with biased noise sampling and introduced a novel transformer architecture for superior text-to-image synthesis.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2403.03206) | [[PDF]](https://arxiv.org/pdf/2403.03206)

> **Core Innovation**
> Enhanced noise sampling for perceptual relevance and designed a bidirectional transformer architecture for better text comprehension and typography.

<details>
    <summary>Abstract</summary>
    Diffusion models create data from noise by inverting the forward paths of data towards noise and have emerged as a powerful generative modeling technique for high-dimensional, perceptual data such as images and videos. Rectified flow is a recent generative model formulation that connects data and noise in a straight line. Despite its better theoretical properties and conceptual simplicity, it is not yet decisively established as standard practice. In this work, we improve existing noise sampling techniques for training rectified flow models by biasing them towards perceptually relevant scales. Through a large-scale study, we demonstrate the superior performance of this approach compared to established diffusion formulations for high-resolution text-to-image synthesis. Additionally, we present a novel transformer-based architecture for text-to-image generation that uses separate weights for the two modalities and enables a bidirectional flow of information between image and text tokens, improving text comprehension, typography, and human preference ratings. We demonstrate that this architecture follows predictable scaling trends and correlates lower validation loss to improved text-to-image synthesis as measured by various metrics and human evaluations. Our largest models outperform state-of-the-art models, and we will make our experimental data, code, and model weights publicly available.
</details>

<details>
    <summary>Key points</summary>
    * Biased noise sampling towards perceptually relevant scales
    * Novel transformer with separate weights for modalities and bidirectional flow
    * Demonstrated predictable scaling and correlation with improved synthesis metrics
</details>
</details>

---


<details>
<summary><b> PromptCharm: Text-to-Image Generation through Multi-modal Prompting and Refinement</b></summary>

* **Authors:** Zhijie Wang, Yuheng Huang, Da Song, Lei Ma, Tianyi Zhang
* **arXiv ID:** 2403.04014
* **One-liner:** Created PromptCharm, a mixed-initiative system to assist novice users in text-to-image creation through prompt engineering and refinement.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2403.04014) | [[PDF]](https://arxiv.org/pdf/2403.04014)

> **Core Innovation**
> Combined automatic prompt optimization, style exploration, model explanation visualization, and interactive refinement tools.

<details>
    <summary>Abstract</summary>
    The recent advancements in Generative AI have significantly advanced the field of text-to-image generation. The state-of-the-art text-to-image model, Stable Diffusion, is now capable of synthesizing high-quality images with a strong sense of aesthetics. Crafting text prompts that align with the model&#39;s interpretation and the user&#39;s intent thus becomes crucial. However, prompting remains challenging for novice users due to the complexity of the stable diffusion model and the non-trivial efforts required for iteratively editing and refining the text prompts. To address these challenges, we propose PromptCharm, a mixed-initiative system that facilitates text-to-image creation through multi-modal prompt engineering and refinement. To assist novice users in prompting, PromptCharm first automatically refines and optimizes the user&#39;s initial prompt. Furthermore, PromptCharm supports the user in exploring and selecting different image styles within a large database. To assist users in effectively refining their prompts and images, PromptCharm renders model explanations by visualizing the model&#39;s attention values. If the user notices any unsatisfactory areas in the generated images, they can further refine the images through model attention adjustment or image inpainting within the rich feedback loop of PromptCharm. To evaluate the effectiveness and usability of PromptCharm, we conducted a controlled user study with 12 participants and an exploratory user study with another 12 participants. These two studies show that participants using PromptCharm were able to create images with higher quality and better aligned with the user&#39;s expectations compared with using two variants of PromptCharm that lacked interaction or visualization support.
</details>

<details>
    <summary>Key points</summary>
    * Automatic prompt refinement and optimization
    * Style selection from a large database
    * Model attention visualization and adjustment, image inpainting for feedback
</details>
</details>

---


<details>
<summary><b> Discriminative Probing and Tuning for Text-to-Image Generation</b></summary>

* **Authors:** Leigang Qu, Wenjie Wang, Yongqi Li, Hanwang Zhang, Liqiang Nie, Tat-Seng Chua
* **arXiv ID:** 2403.04321
* **One-liner:** Enhanced text-to-image alignment by bolstering discriminative abilities through a discriminative adapter and fine-tuning.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2403.04321) | [[PDF]](https://arxiv.org/pdf/2403.04321)

> **Core Innovation**
> Proposed using discriminative modeling to improve generative alignment, with a self-correction mechanism during inference.

<details>
    <summary>Abstract</summary>
    Despite advancements in text-to-image generation (T2I), prior methods often face text-image misalignment problems such as relation confusion in generated images. Existing solutions involve cross-attention manipulation for better compositional understanding or integrating large language models for improved layout planning. However, the inherent alignment capabilities of T2I models are still inadequate. By reviewing the link between generative and discriminative modeling, we posit that T2I models&#39; discriminative abilities may reflect their text-image alignment proficiency during generation. In this light, we advocate bolstering the discriminative abilities of T2I models to achieve more precise text-to-image alignment for generation. We present a discriminative adapter built on T2I models to probe their discriminative abilities on two representative tasks and leverage discriminative fine-tuning to improve their text-image alignment. As a bonus of the discriminative adapter, a self-correction mechanism can leverage discriminative gradients to better align generated images to text prompts during inference. Comprehensive evaluations across three benchmark datasets, including both in-distribution and out-of-distribution scenarios, demonstrate our method&#39;s superior generation performance. Meanwhile, it achieves state-of-the-art discriminative performance on the two discriminative tasks compared to other generative models.
</details>

<details>
    <summary>Key points</summary>
    * Built discriminative adapter for T2I models
    * Discriminative fine-tuning on representative tasks
    * Self-correction mechanism using discriminative gradients
</details>
</details>

---


<details>
<summary><b> PixArt-Σ: Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation</b></summary>

* **Authors:** Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, Zhenguo Li
* **arXiv ID:** 2403.04692
* **One-liner:** Introduced PixArt-Σ, a Diffusion Transformer model for efficient 4K resolution image generation with high fidelity and text alignment.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2403.04692) | [[PDF]](https://arxiv.org/pdf/2403.04692)

> **Core Innovation**
> Achieved training efficiency via weak-to-strong training with high-quality data and efficient token compression.

<details>
    <summary>Abstract</summary>
    In this paper, we introduce PixArt-\Sigma, a Diffusion Transformer model~(DiT) capable of directly generating images at 4K resolution. PixArt-\Sigma represents a significant advancement over its predecessor, PixArt-\alpha, offering images of markedly higher fidelity and improved alignment with text prompts. A key feature of PixArt-\Sigma is its training efficiency. Leveraging the foundational pre-training of PixArt-\alpha, it evolves from the `weaker&#39; baseline to a `stronger&#39; model via incorporating higher quality data, a process we term &#34;weak-to-strong training&#34;. The advancements in PixArt-\Sigma are twofold: (1) High-Quality Training Data: PixArt-\Sigma incorporates superior-quality image data, paired with more precise and detailed image captions. (2) Efficient Token Compression: we propose a novel attention module within the DiT framework that compresses both keys and values, significantly improving efficiency and facilitating ultra-high-resolution image generation. Thanks to these improvements, PixArt-\Sigma achieves superior image quality and user prompt adherence capabilities with significantly smaller model size (0.6B parameters) than existing text-to-image diffusion models, such as SDXL (2.6B parameters) and SD Cascade (5.1B parameters). Moreover, PixArt-\Sigma&#39;s capability to generate 4K images supports the creation of high-resolution posters and wallpapers, efficiently bolstering the production of high-quality visual content in industries such as film and gaming.
</details>

<details>
    <summary>Key points</summary>
    * Incorporated high-quality training data with detailed captions
    * Proposed efficient token compression in DiT framework
    * Enabled 4K image generation with smaller model size
</details>
</details>

---


<details>
<summary><b> A 28.6 mJ/iter Stable Diffusion Processor for Text-to-Image Generation with Patch Similarity-based Sparsity Augmentation and Text-based Mixed-Precision</b></summary>

* **Authors:** Jiwon Choi, Wooyoung Jo, Seongyon Hong, Beomseok Kwon, Wonhoon Park, Hoi-Jun Yoo
* **arXiv ID:** 2403.04982
* **One-liner:** Designed an energy-efficient stable diffusion processor for mobile deployment with high throughput and reduced power consumption.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2403.04982) | [[PDF]](https://arxiv.org/pdf/2403.04982)

> **Core Innovation**
> Implemented hardware optimizations including sparsity augmentation, important pixel spotting, and dual-mode core architecture.

<details>
    <summary>Abstract</summary>
    This paper presents an energy-efficient stable diffusion processor for text-to-image generation. While stable diffusion attained attention for high-quality image synthesis results, its inherent characteristics hinder its deployment on mobile platforms. The proposed processor achieves high throughput and energy efficiency with three key features as solutions: 1) Patch similarity-based sparsity augmentation (PSSA) to reduce external memory access (EMA) energy of self-attention score by 60.3 %, leading to 37.8 % total EMA energy reduction. 2) Text-based important pixel spotting (TIPS) to allow 44.8 % of the FFN layer workload to be processed with low-precision activation. 3) Dual-mode bit-slice core (DBSC) architecture to enhance energy efficiency in FFN layers by 43.0 %. The proposed processor is implemented in 28 nm CMOS technology and achieves 3.84 TOPS peak throughput with 225.6 mW average power consumption. In sum, 28.6 mJ/iteration highly energy-efficient text-to-image generation processor can be achieved at MS-COCO dataset.
</details>

<details>
    <summary>Key points</summary>
    * Patch similarity-based sparsity augmentation for memory energy reduction
    * Text-based important pixel spotting for low-precision processing
    * Dual-mode bit-slice core for enhanced energy efficiency in FFN layers
</details>
</details>

---


<details>
<summary><b> CogView3: Finer and Faster Text-to-Image Generation via Relay Diffusion</b></summary>

* **Authors:** Wendi Zheng, Jiayan Teng, Zhuoyi Yang, Weihan Wang, Jidong Chen, Xiaotao Gu, Yuxiao Dong, Ming Ding, Jie Tang
* **arXiv ID:** 2403.05121
* **One-liner:** Proposed CogView3, a cascaded framework using relay diffusion for efficient and high-quality text-to-image generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2403.05121) | [[PDF]](https://arxiv.org/pdf/2403.05121)

> **Core Innovation**
> Introduced relay-based super-resolution to reduce training and inference costs while improving performance.

<details>
    <summary>Abstract</summary>
    Recent advancements in text-to-image generative systems have been largely driven by diffusion models. However, single-stage text-to-image diffusion models still face challenges, in terms of computational efficiency and the refinement of image details. To tackle the issue, we propose CogView3, an innovative cascaded framework that enhances the performance of text-to-image diffusion. CogView3 is the first model implementing relay diffusion in the realm of text-to-image generation, executing the task by first creating low-resolution images and subsequently applying relay-based super-resolution. This methodology not only results in competitive text-to-image outputs but also greatly reduces both training and inference costs. Our experimental results demonstrate that CogView3 outperforms SDXL, the current state-of-the-art open-source text-to-image diffusion model, by 77.0\% in human evaluations, all while requiring only about 1/2 of the inference time. The distilled variant of CogView3 achieves comparable performance while only utilizing 1/10 of the inference time by SDXL.
</details>

<details>
    <summary>Key points</summary>
    * Implemented relay diffusion for low-resolution to high-resolution generation
    * Achieved competitive outputs with reduced inference time
    * Distilled variant for further efficiency gains
</details>
</details>

---


<details>
<summary><b> DivCon: Divide and Conquer for Complex Numerical and Spatial Reasoning in Text-to-Image Generation</b></summary>

* **Authors:** Yuhao Jia, Wenhan Tan
* **arXiv ID:** 2403.06400
* **One-liner:** Introduced a divide-and-conquer approach for layout-based text-to-image generation to handle complex spatial relationships with lightweight LLMs.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2403.06400) | [[PDF]](https://arxiv.org/pdf/2403.06400)

> **Core Innovation**
> Decoupled layout prediction and image generation into subtasks for improved accuracy and perceptual quality.

<details>
    <summary>Abstract</summary>
    Diffusion-driven text-to-image (T2I) generation has achieved remarkable advancements in recent years. To further improve T2I models&#39; capability in numerical and spatial reasoning, layout is employed as an intermedium to bridge large language models and layout-based diffusion models. However, these methods often rely on closed-source, large-scale LLMs for layout prediction, limiting accessibility and scalability. They also struggle with generating images from prompts with multiple objects and complicated spatial relationships. To tackle these challenges, we introduce a divide-and-conquer approach which decouples the generation task into multiple subtasks. First, the layout prediction stage is divided into numerical &amp; spatial reasoning and bounding box visual planning, enabling even lightweight LLMs to achieve layout accuracy comparable to large-scale models. Second, the layout-to-image generation stage is divided into two steps to synthesize objects from easy ones to difficult ones. Experiments are conducted on the HRS and NSR-1K benchmarks and our method outperforms previous approaches with notable margins. In addition, visual results and user study demonstrate that our approach significantly improves the perceptual quality, especially when generating multiple objects from complex textural prompts.
</details>

<details>
    <summary>Key points</summary>
    * Divided layout prediction into reasoning and visual planning
    * Sequential object synthesis from easy to difficult
    * Enhanced performance on benchmarks with multiple objects
</details>
</details>

---


<details>
<summary><b> Block-wise LoRA: Revisiting Fine-grained LoRA for Effective Personalization and Stylization in Text-to-Image Generation</b></summary>

* **Authors:** Likun Li, Haoqi Zeng, Changpeng Yang, Haozhe Jia, Di Xu
* **arXiv ID:** 2403.07500
* **One-liner:** Proposed block-wise Low-Rank Adaptation for effective personalization and stylization in text-to-image generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2403.07500) | [[PDF]](https://arxiv.org/pdf/2403.07500)

> **Core Innovation**
> Enabled fine-grained fine-tuning of different SD blocks to achieve faithful and stylized image generation.

<details>
    <summary>Abstract</summary>
    The objective of personalization and stylization in text-to-image is to instruct a pre-trained diffusion model to analyze new concepts introduced by users and incorporate them into expected styles. Recently, parameter-efficient fine-tuning (PEFT) approaches have been widely adopted to address this task and have greatly propelled the development of this field. Despite their popularity, existing efficient fine-tuning methods still struggle to achieve effective personalization and stylization in T2I generation. To address this issue, we propose block-wise Low-Rank Adaptation (LoRA) to perform fine-grained fine-tuning for different blocks of SD, which can generate images faithful to input prompts and target identity and also with desired style. Extensive experiments demonstrate the effectiveness of the proposed method.
</details>

<details>
    <summary>Key points</summary>
    * Applied block-wise LoRA for parameter-efficient fine-tuning
    * Fine-tuned different blocks of Stable Diffusion
    * Generated images aligned with prompts, identity, and style
</details>
</details>

---


<details>
<summary><b> Optimizing Negative Prompts for Enhanced Aesthetics and Fidelity in Text-To-Image Generation</b></summary>

* **Authors:** Michael Ogezi, Ning Shi
* **arXiv ID:** 2403.07605
* **One-liner:** Proposed NegOpt for automated negative prompt optimization, enhancing image quality by 25% in Inception Score.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2403.07605) | [[PDF]](https://arxiv.org/pdf/2403.07605)

> **Core Innovation**
> Developed a method combining supervised fine-tuning and reinforcement learning to generate effective negative prompts automatically, outperforming manual approaches.

<details>
    <summary>Abstract</summary>
    In text-to-image generation, using negative prompts, which describe undesirable image characteristics, can significantly boost image quality. However, producing good negative prompts is manual and tedious. To address this, we propose NegOpt, a novel method for optimizing negative prompt generation toward enhanced image generation, using supervised fine-tuning and reinforcement learning. Our combined approach results in a substantial increase of 25% in Inception Score compared to other approaches and surpasses ground-truth negative prompts from the test set. Furthermore, with NegOpt we can preferentially optimize the metrics most important to us. Finally, we construct Negative Prompts DB (<a href="https://huggingface.co/datasets/mikeogezi/negopt_full" rel="external noopener nofollow" class="link-external link-https">this https URL</a>), a publicly available dataset of negative prompts.
</details>

<details>
    <summary>Key points</summary>
    * Supervised fine-tuning for initial model training
    * Reinforcement learning for optimizing negative prompts
    * Creation of a public dataset (Negative Prompts DB)
</details>
</details>

---


<details>
<summary><b> Bridging Different Language Models and Generative Vision Models for Text-to-Image Generation</b></summary>

* **Authors:** Shihao Zhao, Shaozhe Hao, Bojia Zi, Huaizhe Xu, Kwan-Yee K. Wong
* **arXiv ID:** 2403.07860
* **One-liner:** Introduced LaVi-Bridge, enabling flexible integration of diverse language and vision models for improved text-to-image generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2403.07860) | [[PDF]](https://arxiv.org/pdf/2403.07860)

> **Core Innovation**
> Created a plug-and-play pipeline using LoRA and adapters to combine pre-trained models without modifying original weights, enhancing capabilities like text alignment and image quality.

<details>
    <summary>Abstract</summary>
    Text-to-image generation has made significant advancements with the introduction of text-to-image diffusion models. These models typically consist of a language model that interprets user prompts and a vision model that generates corresponding images. As language and vision models continue to progress in their respective domains, there is a great potential in exploring the replacement of components in text-to-image diffusion models with more advanced counterparts. A broader research objective would therefore be to investigate the integration of any two unrelated language and generative vision models for text-to-image generation. In this paper, we explore this objective and propose LaVi-Bridge, a pipeline that enables the integration of diverse pre-trained language models and generative vision models for text-to-image generation. By leveraging LoRA and adapters, LaVi-Bridge offers a flexible and plug-and-play approach without requiring modifications to the original weights of the language and vision models. Our pipeline is compatible with various language models and generative vision models, accommodating different structures. Within this framework, we demonstrate that incorporating superior modules, such as more advanced language models or generative vision models, results in notable improvements in capabilities like text alignment or image quality. Extensive evaluations have been conducted to verify the effectiveness of LaVi-Bridge. Code is available at <a href="https://github.com/ShihaoZhaoZSH/LaVi-Bridge" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Use of LoRA and adapters for integration
    * Compatibility with various language and generative vision models
    * Demonstration of improvements with superior modules
</details>
</details>

---


<details>
<summary><b> DialogGen: Multi-modal Interactive Dialogue System for Multi-turn Text-to-Image Generation</b></summary>

* **Authors:** Minbin Huang, Yanxin Long, Xinchi Deng, Ruihang Chu, Jiangfeng Xiong, Xiaodan Liang, Hong Cheng, Qinglin Lu, Wei Liu
* **arXiv ID:** 2403.08857
* **One-liner:** Proposed DialogGen for multi-turn text-to-image generation via MLLM alignment, improving output coherence and modality switching.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2403.08857) | [[PDF]](https://arxiv.org/pdf/2403.08857)

> **Core Innovation**
> Developed a pipeline with drawing prompt alignment, data curation, and error correction to build a multi-modal interactive dialogue system, validated on the DialogBen benchmark.

<details>
    <summary>Abstract</summary>
    Text-to-image (T2I) generation models have significantly advanced in recent years. However, effective interaction with these models is challenging for average users due to the need for specialized prompt engineering knowledge and the inability to perform multi-turn image generation, hindering a dynamic and iterative creation process. Recent attempts have tried to equip Multi-modal Large Language Models (MLLMs) with T2I models to bring the user&#39;s natural language instructions into reality. Hence, the output modality of MLLMs is extended, and the multi-turn generation quality of T2I models is enhanced thanks to the strong multi-modal comprehension ability of MLLMs. However, many of these works face challenges in identifying correct output modalities and generating coherent images accordingly as the number of output modalities increases and the conversations go deeper. Therefore, we propose DialogGen, an effective pipeline to align off-the-shelf MLLMs and T2I models to build a Multi-modal Interactive Dialogue System (MIDS) for multi-turn Text-to-Image generation. It is composed of drawing prompt alignment, careful training data curation, and error correction. Moreover, as the field of MIDS flourishes, comprehensive benchmarks are urgently needed to evaluate MIDS fairly in terms of output modality correctness and multi-modal output coherence. To address this issue, we introduce the Multi-modal Dialogue Benchmark (DialogBen), a comprehensive bilingual benchmark designed to assess the ability of MLLMs to generate accurate and coherent multi-modal content that supports image editing. It contains two evaluation metrics to measure the model&#39;s ability to switch modalities and the coherence of the output images. Our extensive experiments on DialogBen and user study demonstrate the effectiveness of DialogGen compared with other State-of-the-Art models.
</details>

<details>
    <summary>Key points</summary>
    * Drawing prompt alignment for MLLM-T2I integration
    * Careful training data curation
    * Introduction of DialogBen benchmark for evaluation
</details>
</details>

---


<details>
<summary><b> CLIP-VQDiffusion : Langauge Free Training of Text To Image generation using CLIP and vector quantized diffusion model</b></summary>

* **Authors:** Seungdae Han, Joohee Kim
* **arXiv ID:** 2403.14944
* **One-liner:** Introduced CLIP-VQDiffusion for text-to-image generation without paired captions, achieving state-of-the-art performance on FFHQ.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2403.14944) | [[PDF]](https://arxiv.org/pdf/2403.14944)

> **Core Innovation**
> Leveraged pretrained CLIP for multimodal representations to generate realistic images from text, even out-of-distribution, outperforming previous methods by 4.4% in clipscore.

<details>
    <summary>Abstract</summary>
    There has been a significant progress in text conditional image generation models. Recent advancements in this field depend not only on improvements in model structures, but also vast quantities of text-image paired datasets. However, creating these kinds of datasets is very costly and requires a substantial amount of labor. Famous face datasets don&#39;t have corresponding text captions, making it difficult to develop text conditional image generation models on these datasets. Some research has focused on developing text to image generation models using only images without text captions. Here, we propose CLIP-VQDiffusion, which leverage the pretrained CLIP model to provide multimodal text-image representations and strong image generation capabilities. On the FFHQ dataset, our model outperformed previous state-of-the-art methods by 4.4% in clipscore and generated very realistic images even when the text was both in and out of distribution. The pretrained models and codes will soon be available at <a href="https://github.com/INFINIQ-AI1/CLIPVQDiffusion" rel="external noopener nofollow" class="link-external link-https">this https URL</a>
</details>

<details>
    <summary>Key points</summary>
    * Use of CLIP for text-image representations
    * Integration with VQDiffusion for image generation
    * Evaluation on FFHQ dataset
</details>
</details>

---


<details>
<summary><b> FlashEval: Towards Fast and Accurate Evaluation of Text-to-image Diffusion Generative Models</b></summary>

* **Authors:** Lin Zhao, Tianchen Zhao, Zinan Lin, Xuefei Ning, Guohao Dai, Huazhong Yang, Yu Wang
* **arXiv ID:** 2403.16379
* **One-liner:** Developed FlashEval for efficient evaluation of text-to-image models, achieving 10x speedup with representative subset selection.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2403.16379) | [[PDF]](https://arxiv.org/pdf/2403.16379)

> **Core Innovation**
> Proposed an iterative search algorithm to select subsets of text-image datasets, enabling comparable evaluation quality with fewer samples on COCO and DiffusionDB.

<details>
    <summary>Abstract</summary>
    In recent years, there has been significant progress in the development of text-to-image generative models. Evaluating the quality of the generative models is one essential step in the development process. Unfortunately, the evaluation process could consume a significant amount of computational resources, making the required periodic evaluation of model performance (e.g., monitoring training progress) impractical. Therefore, we seek to improve the evaluation efficiency by selecting the representative subset of the text-image dataset. We systematically investigate the design choices, including the selection criteria (textural features or image-based metrics) and the selection granularity (prompt-level or set-level). We find that the insights from prior work on subset selection for training data do not generalize to this problem, and we propose FlashEval, an iterative search algorithm tailored to evaluation data selection. We demonstrate the effectiveness of FlashEval on ranking diffusion models with various configurations, including architectures, quantization levels, and sampler schedules on COCO and DiffusionDB datasets. Our searched 50-item subset could achieve comparable evaluation quality to the randomly sampled 500-item subset for COCO annotations on unseen models, achieving a 10x evaluation speedup. We release the condensed subset of these commonly used datasets to help facilitate diffusion algorithm design and evaluation, and open-source FlashEval as a tool for condensing future datasets, accessible at <a href="https://github.com/thu-nics/FlashEval" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Iterative search algorithm for subset selection
    * Investigation of selection criteria and granularity
    * Application to various model configurations
</details>
</details>

---


<details>
<summary><b> Skews in the Phenomenon Space Hinder Generalization in Text-to-Image Generation</b></summary>

* **Authors:** Yingshan Chang, Yasi Zhang, Zhiyuan Fang, Yingnian Wu, Yonatan Bisk, Feng Gao
* **arXiv ID:** 2403.16394
* **One-liner:** Identified dataset skew as a cause of generalization failures in entity-relation compositions and proposed metrics to quantify it.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2403.16394) | [[PDF]](https://arxiv.org/pdf/2403.16394)

> **Core Innovation**
> Introduced statistical metrics for linguistic and visual skew, showing that controlled perturbations improve generalization without increasing data size.

<details>
    <summary>Abstract</summary>
    The literature on text-to-image generation is plagued by issues of faithfully composing entities with relations. But there lacks a formal understanding of how entity-relation compositions can be effectively learned. Moreover, the underlying phenomenon space that meaningfully reflects the problem structure is not well-defined, leading to an arms race for larger quantities of data in the hope that generalization emerges out of large-scale pretraining. We hypothesize that the underlying phenomenological coverage has not been proportionally scaled up, leading to a skew of the presented phenomenon which harms generalization. We introduce statistical metrics that quantify both the linguistic and visual skew of a dataset for relational learning, and show that generalization failures of text-to-image generation are a direct result of incomplete or unbalanced phenomenological coverage. We first perform experiments in a synthetic domain and demonstrate that systematically controlled metrics are strongly predictive of generalization performance. Then we move to natural images and show that simple distribution perturbations in light of our theories boost generalization without enlarging the absolute data size. This work informs an important direction towards quality-enhancing the data diversity or balance orthogonal to scaling up the absolute size. Our discussions point out important open questions on 1) Evaluation of generated entity-relation compositions, and 2) Better models for reasoning with abstract relations.
</details>

<details>
    <summary>Key points</summary>
    * Development of skew metrics for datasets
    * Experiments in synthetic and natural domains
    * Demonstration of generalization improvements via distribution perturbations
</details>
</details>

---


<details>
<summary><b> Refining Text-to-Image Generation: Towards Accurate Training-Free Glyph-Enhanced Image Generation</b></summary>

* **Authors:** Sanyam Lakhanpal, Shivang Chopra, Vinija Jain, Aman Chadha, Man Luo
* **arXiv ID:** 2403.16422
* **One-liner:** Proposed a training-free framework to enhance visual text generation, improving OCR metrics by over 23% on LenCom-Eval benchmark.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2403.16422) | [[PDF]](https://arxiv.org/pdf/2403.16422)

> **Core Innovation**
> Introduced LenCom-Eval benchmark for lengthy and complex text images and a method to boost two-stage generation approaches without retraining.

<details>
    <summary>Abstract</summary>
    Over the past few years, Text-to-Image (T2I) generation approaches based on diffusion models have gained significant attention. However, vanilla diffusion models often suffer from spelling inaccuracies in the text displayed within the generated images. The capability to generate visual text is crucial, offering both academic interest and a wide range of practical applications. To produce accurate visual text images, state-of-the-art techniques adopt a glyph-controlled image generation approach, consisting of a text layout generator followed by an image generator that is conditioned on the generated text layout. Nevertheless, our study reveals that these models still face three primary challenges, prompting us to develop a testbed to facilitate future research. We introduce a benchmark, LenCom-Eval, specifically designed for testing models&#39; capability in generating images with Lengthy and Complex visual text. Subsequently, we introduce a training-free framework to enhance the two-stage generation approaches. We examine the effectiveness of our approach on both LenCom-Eval and MARIO-Eval benchmarks and demonstrate notable improvements across a range of evaluation metrics, including CLIPScore, OCR precision, recall, F1 score, accuracy, and edit distance scores. For instance, our proposed framework improves the backbone model, TextDiffuser, by more than 23\% and 13.5\% in terms of OCR word F1 on LenCom-Eval and MARIO-Eval, respectively. Our work makes a unique contribution to the field by focusing on generating images with long and rare text sequences, a niche previously unexplored by existing literature
</details>

<details>
    <summary>Key points</summary>
    * Creation of LenCom-Eval benchmark
    * Training-free enhancement of glyph-controlled generation
    * Evaluation on multiple OCR metrics
</details>
</details>

---


<details>
<summary><b> Isolated Diffusion: Optimizing Multi-Concept Text-to-Image Generation Training-Freely with Isolated Diffusion Guidance</b></summary>

* **Authors:** Jingyuan Zhu, Huimin Ma, Jiansheng Chen, Jian Yuan
* **arXiv ID:** 2403.16954
* **One-liner:** Introduced Isolated Diffusion to address concept bleeding in multi-concept generation, improving text-image consistency.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2403.16954) | [[PDF]](https://arxiv.org/pdf/2403.16954)

> **Core Innovation**
> Proposed a training-free strategy using split prompts and subject isolation with pre-trained models, compatible with SD and SDXL.

<details>
    <summary>Abstract</summary>
    Large-scale text-to-image diffusion models have achieved great success in synthesizing high-quality and diverse images given target text prompts. Despite the revolutionary image generation ability, current state-of-the-art models still struggle to deal with multi-concept generation accurately in many cases. This phenomenon is known as ``concept bleeding&#34; and displays as the unexpected overlapping or merging of various concepts. This paper presents a general approach for text-to-image diffusion models to address the mutual interference between different subjects and their attachments in complex scenes, pursuing better text-image consistency. The core idea is to isolate the synthesizing processes of different concepts. We propose to bind each attachment to corresponding subjects separately with split text prompts. Besides, we introduce a revision method to fix the concept bleeding problem in multi-subject synthesis. We first depend on pre-trained object detection and segmentation models to obtain the layouts of subjects. Then we isolate and resynthesize each subject individually with corresponding text prompts to avoid mutual interference. Overall, we achieve a training-free strategy, named Isolated Diffusion, to optimize multi-concept text-to-image synthesis. It is compatible with the latest Stable Diffusion XL (SDXL) and prior Stable Diffusion (SD) models. We compare our approach with alternative methods using a variety of multi-concept text prompts and demonstrate its effectiveness with clear advantages in text-image consistency and user study.
</details>

<details>
    <summary>Key points</summary>
    * Split text prompts for subject binding
    * Use of object detection and segmentation for layouts
    * Isolation and resynthesis of subjects
</details>
</details>

---


<details>
<summary><b> Be Yourself: Bounded Attention for Multi-Subject Text-to-Image Generation</b></summary>

* **Authors:** Omer Dahary, Or Patashnik, Kfir Aberman, Daniel Cohen-Or
* **arXiv ID:** 2403.16990
* **One-liner:** Developed Bounded Attention to prevent semantic leakage in multi-subject generation, enhancing subject individuality.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2403.16990) | [[PDF]](https://arxiv.org/pdf/2403.16990)

> **Core Innovation**
> Analyzed attention layer issues and introduced a training-free method to bound information flow during sampling, improving alignment with prompts and layouts.

<details>
    <summary>Abstract</summary>
    Text-to-image diffusion models have an unprecedented ability to generate diverse and high-quality images. However, they often struggle to faithfully capture the intended semantics of complex input prompts that include multiple subjects. Recently, numerous layout-to-image extensions have been introduced to improve user control, aiming to localize subjects represented by specific tokens. Yet, these methods often produce semantically inaccurate images, especially when dealing with multiple semantically or visually similar subjects. In this work, we study and analyze the causes of these limitations. Our exploration reveals that the primary issue stems from inadvertent semantic leakage between subjects in the denoising process. This leakage is attributed to the diffusion model&#39;s attention layers, which tend to blend the visual features of different subjects. To address these issues, we introduce Bounded Attention, a training-free method for bounding the information flow in the sampling process. Bounded Attention prevents detrimental leakage among subjects and enables guiding the generation to promote each subject&#39;s individuality, even with complex multi-subject conditioning. Through extensive experimentation, we demonstrate that our method empowers the generation of multiple subjects that better align with given prompts and layouts.
</details>

<details>
    <summary>Key points</summary>
    * Analysis of semantic leakage in attention layers
    * Introduction of Bounded Attention method
    * Experimentation with complex multi-subject conditioning
</details>
</details>

---


<details>
<summary><b> Capability-aware Prompt Reformulation Learning for Text-to-Image Generation</b></summary>

* **Authors:** Jingtao Zhan, Qingyao Ai, Yiqun Liu, Jia Chen, Shaoping Ma
* **arXiv ID:** 2403.19716
* **One-liner:** Proposed CAPR for automatic prompt reformulation based on user capability, improving interaction with text-to-image systems.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2403.19716) | [[PDF]](https://arxiv.org/pdf/2403.19716)

> **Core Innovation**
> Developed a framework with Conditional Reformulation Model and Configurable Capability Features to learn diverse strategies and simulate high-capability users.

<details>
    <summary>Abstract</summary>
    Text-to-image generation systems have emerged as revolutionary tools in the realm of artistic creation, offering unprecedented ease in transforming textual prompts into visual art. However, the efficacy of these systems is intricately linked to the quality of user-provided prompts, which often poses a challenge to users unfamiliar with prompt crafting. This paper addresses this challenge by leveraging user reformulation data from interaction logs to develop an automatic prompt reformulation model. Our in-depth analysis of these logs reveals that user prompt reformulation is heavily dependent on the individual user&#39;s capability, resulting in significant variance in the quality of reformulation pairs. To effectively use this data for training, we introduce the Capability-aware Prompt Reformulation (CAPR) framework. CAPR innovatively integrates user capability into the reformulation process through two key components: the Conditional Reformulation Model (CRM) and Configurable Capability Features (CCF). CRM reformulates prompts according to a specified user capability, as represented by CCF. The CCF, in turn, offers the flexibility to tune and guide the CRM&#39;s behavior. This enables CAPR to effectively learn diverse reformulation strategies across various user capacities and to simulate high-capability user reformulation during inference. Extensive experiments on standard text-to-image generation benchmarks showcase CAPR&#39;s superior performance over existing baselines and its remarkable robustness on unseen systems. Furthermore, comprehensive analyses validate the effectiveness of different components. CAPR can facilitate user-friendly interaction with text-to-image systems and make advanced artistic creation more achievable for a broader range of users.
</details>

<details>
    <summary>Key points</summary>
    * Integration of user capability into reformulation
    * Use of Conditional Reformulation Model
    * Configurable Capability Features for tuning behavior
</details>
</details>

---


<details>
<summary><b> Evaluating Text-to-Visual Generation with Image-to-Text Generation</b></summary>

* **Authors:** Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide Xia, Graham Neubig, Pengchuan Zhang, Deva Ramanan
* **arXiv ID:** 2404.01291
* **One-liner:** Introduced VQAScore for improved image-text alignment evaluation using VQA models.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2404.01291) | [[PDF]](https://arxiv.org/pdf/2404.01291)

> **Core Innovation**
> VQAScore addresses limitations of CLIPScore by using VQA models to compute alignment scores, achieving state-of-the-art results on benchmarks and enabling evaluation with complex compositional prompts.

<details>
    <summary>Abstract</summary>
    Despite significant progress in generative AI, comprehensive evaluation remains challenging because of the lack of effective metrics and standardized benchmarks. For instance, the widely-used CLIPScore measures the alignment between a (generated) image and text prompt, but it fails to produce reliable scores for complex prompts involving compositions of objects, attributes, and relations. One reason is that text encoders of CLIP can notoriously act as a &#34;bag of words&#34;, conflating prompts such as &#34;the horse is eating the grass&#34; with &#34;the grass is eating the horse&#34;. To address this, we introduce the VQAScore, which uses a visual-question-answering (VQA) model to produce an alignment score by computing the probability of a &#34;Yes&#34; answer to a simple &#34;Does this figure show &#39;{text}&#39;?&#34; question. Though simpler than prior art, VQAScore computed with off-the-shelf models produces state-of-the-art results across many (8) image-text alignment benchmarks. We also compute VQAScore with an in-house model that follows best practices in the literature. For example, we use a bidirectional image-question encoder that allows image embeddings to depend on the question being asked (and vice versa). Our in-house model, CLIP-FlanT5, outperforms even the strongest baselines that make use of the proprietary GPT-4V. Interestingly, although we train with only images, VQAScore can also align text with video and 3D models. VQAScore allows researchers to benchmark text-to-visual generation using complex texts that capture the compositional structure of real-world prompts. We introduce GenAI-Bench, a more challenging benchmark with 1,600 compositional text prompts that require parsing scenes, objects, attributes, relationships, and high-order reasoning like comparison and logic. GenAI-Bench also offers over 15,000 human ratings for leading image and video generation models such as Stable Diffusion, DALL-E 3, and Gen2.
</details>

<details>
    <summary>Key points</summary>
    * Uses VQA model for 'Yes' answer probability to 'Does this figure show {text}?' question
    * Developed in-house CLIP-FlanT5 model with bidirectional encoder
    * Introduced GenAI-Bench with 1,600 compositional prompts for benchmarking
</details>
</details>

---


<details>
<summary><b> InstantStyle: Free Lunch towards Style-Preserving in Text-to-Image Generation</b></summary>

* **Authors:** Haofan Wang, Matteo Spinelli, Qixun Wang, Xu Bai, Zekui Qin, Anthony Chen
* **arXiv ID:** 2404.02733
* **One-liner:** Proposed InstantStyle for tuning-free style-consistent image generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2404.02733) | [[PDF]](https://arxiv.org/pdf/2404.02733)

> **Core Innovation**
> InstantStyle decouples style and content in feature space and injects features into style-specific blocks to prevent style degradation and avoid weight tuning.

<details>
    <summary>Abstract</summary>
    Tuning-free diffusion-based models have demonstrated significant potential in the realm of image personalization and customization. However, despite this notable progress, current models continue to grapple with several complex challenges in producing style-consistent image generation. Firstly, the concept of style is inherently underdetermined, encompassing a multitude of elements such as color, material, atmosphere, design, and structure, among others. Secondly, inversion-based methods are prone to style degradation, often resulting in the loss of fine-grained details. Lastly, adapter-based approaches frequently require meticulous weight tuning for each reference image to achieve a balance between style intensity and text controllability. In this paper, we commence by examining several compelling yet frequently overlooked observations. We then proceed to introduce InstantStyle, a framework designed to address these issues through the implementation of two key strategies: 1) A straightforward mechanism that decouples style and content from reference images within the feature space, predicated on the assumption that features within the same space can be either added to or subtracted from one another. 2) The injection of reference image features exclusively into style-specific blocks, thereby preventing style leaks and eschewing the need for cumbersome weight tuning, which often characterizes more parameter-heavy <a href="http://designs.Our" rel="external noopener nofollow" class="link-external link-http">this http URL</a> work demonstrates superior visual stylization outcomes, striking an optimal balance between the intensity of style and the controllability of textual elements. Our codes will be available at <a href="https://github.com/InstantStyle/InstantStyle" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Decouples style and content via feature addition/subtraction
    * Injects reference image features only into style-specific blocks
    * Eliminates need for weight tuning and prevents style leaks
</details>
</details>

---


<details>
<summary><b> MULAN: A Multi Layer Annotated Dataset for Controllable Text-to-Image Generation</b></summary>

* **Authors:** Petru-Daniel Tudosiu, Yongxin Yang, Shifeng Zhang, Fei Chen, Steven McDonagh, Gerasimos Lampouras, Ignacio Iacobacci, Sarah Parisot
* **arXiv ID:** 2404.02790
* **One-liner:** Created MuLAn dataset for instance-wise image decomposition to aid text-to-image generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2404.02790) | [[PDF]](https://arxiv.org/pdf/2404.02790)

> **Core Innovation**
> MuLAn provides multi-layer annotations of RGB images as RGBA decompositions, enabling research into layer-wise generation and editing without training.

<details>
    <summary>Abstract</summary>
    Text-to-image generation has achieved astonishing results, yet precise spatial controllability and prompt fidelity remain highly challenging. This limitation is typically addressed through cumbersome prompt engineering, scene layout conditioning, or image editing techniques which often require hand drawn masks. Nonetheless, pre-existing works struggle to take advantage of the natural instance-level compositionality of scenes due to the typically flat nature of rasterized RGB output images. Towards adressing this challenge, we introduce MuLAn: a novel dataset comprising over 44K MUlti-Layer ANnotations of RGB images as multilayer, instance-wise RGBA decompositions, and over 100K instance images. To build MuLAn, we developed a training free pipeline which decomposes a monocular RGB image into a stack of RGBA layers comprising of background and isolated instances. We achieve this through the use of pretrained general-purpose models, and by developing three modules: image decomposition for instance discovery and extraction, instance completion to reconstruct occluded areas, and image re-assembly. We use our pipeline to create MuLAn-COCO and MuLAn-LAION datasets, which contain a variety of image decompositions in terms of style, composition and complexity. With MuLAn, we provide the first photorealistic resource providing instance decomposition and occlusion information for high quality images, opening up new avenues for text-to-image generative AI research. With this, we aim to encourage the development of novel generation and editing technology, in particular layer-wise solutions. MuLAn data resources are available at <a href="https://MuLAn-dataset.github.io/" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Developed training-free pipeline for image decomposition into RGBA layers
    * Includes modules for instance discovery, completion, and reassembly
    * Built MuLAn-COCO and MuLAn-LAION datasets with over 44K annotations
</details>
</details>

---


<details>
<summary><b> On the Scalability of Diffusion-based Text-to-Image Generation</b></summary>

* **Authors:** Hao Li, Yang Zou, Ying Wang, Orchid Majumder, Yusheng Xie, R. Manmatha, Ashwin Swaminathan, Zhuowen Tu, Stefano Ermon, Stefano Soatto
* **arXiv ID:** 2404.02883
* **One-liner:** Empirically studied scaling laws for diffusion-based text-to-image models to optimize performance and cost.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2404.02883) | [[PDF]](https://arxiv.org/pdf/2404.02883)

> **Core Innovation**
> Identified efficient UNet designs and data scaling strategies, showing that transformer blocks and dataset quality/diversity are key for text-image alignment.

<details>
    <summary>Abstract</summary>
    Scaling up model and data size has been quite successful for the evolution of LLMs. However, the scaling law for the diffusion based text-to-image (T2I) models is not fully explored. It is also unclear how to efficiently scale the model for better performance at reduced cost. The different training settings and expensive training cost make a fair model comparison extremely difficult. In this work, we empirically study the scaling properties of diffusion based T2I models by performing extensive and rigours ablations on scaling both denoising backbones and training set, including training scaled UNet and Transformer variants ranging from 0.4B to 4B parameters on datasets upto 600M images. For model scaling, we find the location and amount of cross attention distinguishes the performance of existing UNet designs. And increasing the transformer blocks is more parameter-efficient for improving text-image alignment than increasing channel numbers. We then identify an efficient UNet variant, which is 45% smaller and 28% faster than SDXL&#39;s UNet. On the data scaling side, we show the quality and diversity of the training set matters more than simply dataset size. Increasing caption density and diversity improves text-image alignment performance and the learning efficiency. Finally, we provide scaling functions to predict the text-image alignment performance as functions of the scale of model size, compute and dataset size.
</details>

<details>
    <summary>Key points</summary>
    * Ablated UNet and Transformer variants from 0.4B to 4B parameters
    * Found increasing transformer blocks is more parameter-efficient than channel numbers
    * Showed caption density and diversity improve alignment and learning efficiency
</details>
</details>

---


<details>
<summary><b> RL for Consistency Models: Faster Reward Guided Text-to-Image Generation</b></summary>

* **Authors:** Owen Oertell, Jonathan D. Chang, Yiyi Zhang, Kianté Brantley, Wen Sun
* **arXiv ID:** 2404.03673
* **One-liner:** Developed RLCM for fast reinforcement learning fine-tuning of consistency models in text-to-image generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2404.03673) | [[PDF]](https://arxiv.org/pdf/2404.03673)

> **Core Innovation**
> RLCM frames consistency model inference as RL, enabling faster training and inference with high-quality image generation in few steps, adaptable to various rewards.

<details>
    <summary>Abstract</summary>
    Reinforcement learning (RL) has improved guided image generation with diffusion models by directly optimizing rewards that capture image quality, aesthetics, and instruction following capabilities. However, the resulting generative policies inherit the same iterative sampling process of diffusion models that causes slow generation. To overcome this limitation, consistency models proposed learning a new class of generative models that directly map noise to data, resulting in a model that can generate an image in as few as one sampling iteration. In this work, to optimize text-to-image generative models for task specific rewards and enable fast training and inference, we propose a framework for fine-tuning consistency models via RL. Our framework, called Reinforcement Learning for Consistency Model (RLCM), frames the iterative inference process of a consistency model as an RL procedure. Comparing to RL finetuned diffusion models, RLCM trains significantly faster, improves the quality of the generation measured under the reward objectives, and speeds up the inference procedure by generating high quality images with as few as two inference steps. Experimentally, we show that RLCM can adapt text-to-image consistency models to objectives that are challenging to express with prompting, such as image compressibility, and those derived from human feedback, such as aesthetic quality. Our code is available at <a href="https://rlcm.owenoertell.com" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Frames iterative inference of consistency models as RL procedure
    * Enables generation in as few as two steps
    * Adapts to objectives like image compressibility and human feedback
</details>
</details>

---


<details>
<summary><b> Dynamic Prompt Optimizing for Text-to-Image Generation</b></summary>

* **Authors:** Wenyi Mo, Tianyu Zhang, Yalong Bai, Bing Su, Ji-Rong Wen, Qing Yang
* **arXiv ID:** 2404.04095
* **One-liner:** Introduced PAE for automatic prompt editing to improve image generation quality.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2404.04095) | [[PDF]](https://arxiv.org/pdf/2404.04095)

> **Core Innovation**
> PAE uses reinforcement learning to dynamically adjust word weights and injection time steps in prompts, enhancing aesthetics and semantic alignment without manual intervention.

<details>
    <summary>Abstract</summary>
    Text-to-image generative models, specifically those based on diffusion models like Imagen and Stable Diffusion, have made substantial advancements. Recently, there has been a surge of interest in the delicate refinement of text prompts. Users assign weights or alter the injection time steps of certain words in the text prompts to improve the quality of generated images. However, the success of fine-control prompts depends on the accuracy of the text prompts and the careful selection of weights and time steps, which requires significant manual intervention. To address this, we introduce the \textbf{P}rompt \textbf{A}uto-\textbf{E}diting (PAE) method. Besides refining the original prompts for image generation, we further employ an online reinforcement learning strategy to explore the weights and injection time steps of each word, leading to the dynamic fine-control prompts. The reward function during training encourages the model to consider aesthetic score, semantic consistency, and user preferences. Experimental results demonstrate that our proposed method effectively improves the original prompts, generating visually more appealing images while maintaining semantic alignment. Code is available at <a href="https://github.com/Mowenyii/PAE" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Employs online RL to explore word weights and injection time steps
    * Reward function considers aesthetic score, semantic consistency, and user preferences
    * Improves prompts for better visual appeal and alignment
</details>
</details>

---


<details>
<summary><b> SafeGen: Mitigating Sexually Explicit Content Generation in Text-to-Image Models</b></summary>

* **Authors:** Xinfeng Li, Yuchen Yang, Jiangyi Deng, Chen Yan, Yanjiao Chen, Xiaoyu Ji, Wenyuan Xu
* **arXiv ID:** 2404.06666
* **One-liner:** Proposed SafeGen to mitigate NSFW content generation in text-to-image models in a text-agnostic manner.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2404.06666) | [[PDF]](https://arxiv.org/pdf/2404.06666)

> **Core Innovation**
> SafeGen eliminates explicit visual representations from the model internally, making it resistant to adversarial prompts while preserving benign image fidelity.

<details>
    <summary>Abstract</summary>
    Text-to-image (T2I) models, such as Stable Diffusion, have exhibited remarkable performance in generating high-quality images from text descriptions in recent years. However, text-to-image models may be tricked into generating not-safe-for-work (NSFW) content, particularly in sexually explicit scenarios. Existing countermeasures mostly focus on filtering inappropriate inputs and outputs, or suppressing improper text embeddings, which can block sexually explicit content (e.g., naked) but may still be vulnerable to adversarial prompts -- inputs that appear innocent but are ill-intended. In this paper, we present SafeGen, a framework to mitigate sexual content generation by text-to-image models in a text-agnostic manner. The key idea is to eliminate explicit visual representations from the model regardless of the text input. In this way, the text-to-image model is resistant to adversarial prompts since such unsafe visual representations are obstructed from within. Extensive experiments conducted on four datasets and large-scale user studies demonstrate SafeGen&#39;s effectiveness in mitigating sexually explicit content generation while preserving the high-fidelity of benign images. SafeGen outperforms eight state-of-the-art baseline methods and achieves 99.4% sexual content removal performance. Furthermore, our constructed benchmark of adversarial prompts provides a basis for future development and evaluation of anti-NSFW-generation methods.
</details>

<details>
    <summary>Key points</summary>
    * Obstructs unsafe visual representations regardless of text input
    * Achieves 99.4% sexual content removal performance
    * Outperforms eight baseline methods and provides adversarial prompt benchmark
</details>
</details>

---


<details>
<summary><b> TextCenGen: Attention-Guided Text-Centric Background Adaptation for Text-to-Image Generation</b></summary>

* **Authors:** Tianyi Liang, Jiangqi Liu, Yifei Huang, Shiqi Jiang, Jianshen Shi, Changbo Wang, Chenhui Li
* **arXiv ID:** 2404.11824
* **One-liner:** Developed TextCenGen for generating text-friendly backgrounds in text-to-image models without training.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2404.11824) | [[PDF]](https://arxiv.org/pdf/2404.11824)

> **Core Innovation**
> TextCenGen relocates conflicting objects using cross-attention maps and force-directed graphs, ensuring smooth backgrounds for text placement while maintaining semantic fidelity.

<details>
    <summary>Abstract</summary>
    Text-to-image (T2I) generation has made remarkable progress in producing high-quality images, but a fundamental challenge remains: creating backgrounds that naturally accommodate text placement without compromising image quality. This capability is non-trivial for real-world applications like graphic design, where clear visual hierarchy between content and text is essential. Prior work has primarily focused on arranging layouts within existing static images, leaving unexplored the potential of T2I models for generating text-friendly backgrounds. We present TextCenGen, a training-free dynamic background adaptation in the blank region for text-friendly image generation. Instead of directly reducing attention in text areas, which degrades image quality, we relocate conflicting objects before background optimization. Our method analyzes cross-attention maps to identify conflicting objects overlapping with text regions and uses a force-directed graph approach to guide their relocation, followed by attention excluding constraints to ensure smooth backgrounds. Our method is plug-and-play, requiring no additional training while well balancing both semantic fidelity and visual quality. Evaluated on our proposed text-friendly T2I benchmark of 27,000 images across four seed datasets, TextCenGen outperforms existing methods by achieving 23% lower saliency overlap in text regions while maintaining 98% of the semantic fidelity measured by CLIP score and our proposed Visual-Textual Concordance Metric (VTCM).
</details>

<details>
    <summary>Key points</summary>
    * Analyzes cross-attention maps to identify conflicting objects
    * Uses force-directed graph for object relocation
    * Applies attention excluding constraints for background optimization
</details>
</details>

---


<details>
<summary><b> EdgeFusion: On-Device Text-to-Image Generation</b></summary>

* **Authors:** Thibault Castells, Hyoung-Kyu Song, Tairen Piao, Shinkook Choi, Bo-Kyeong Kim, Hanyoung Yim, Changgwun Lee, Jae Gon Kim, Tae-Ho Kim
* **arXiv ID:** 2404.11925
* **One-liner:** Optimized Stable Diffusion for fast, high-quality image generation on edge devices.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2404.11925) | [[PDF]](https://arxiv.org/pdf/2404.11925)

> **Core Innovation**
> Enhanced BK-SDM with LCM using high-quality data and advanced distillation, achieving sub-second latency and photo-realistic images in two steps on resource-limited devices.

<details>
    <summary>Abstract</summary>
    The intensive computational burden of Stable Diffusion (SD) for text-to-image generation poses a significant hurdle for its practical application. To tackle this challenge, recent research focuses on methods to reduce sampling steps, such as Latent Consistency Model (LCM), and on employing architectural optimizations, including pruning and knowledge distillation. Diverging from existing approaches, we uniquely start with a compact SD variant, BK-SDM. We observe that directly applying LCM to BK-SDM with commonly used crawled datasets yields unsatisfactory results. It leads us to develop two strategies: (1) leveraging high-quality image-text pairs from leading generative models and (2) designing an advanced distillation process tailored for LCM. Through our thorough exploration of quantization, profiling, and on-device deployment, we achieve rapid generation of photo-realistic, text-aligned images in just two steps, with latency under one second on resource-limited edge devices.
</details>

<details>
    <summary>Key points</summary>
    * Leveraged high-quality image-text pairs from generative models
    * Designed advanced distillation process for LCM
    * Achieved under one-second latency on edge devices with two-step generation
</details>
</details>

---


<details>
<summary><b> Object-Attribute Binding in Text-to-Image Generation: Evaluation and Control</b></summary>

* **Authors:** Maria Mihaela Trusca, Wolf Nuyts, Jonathan Thomm, Robert Honig, Thomas Hofmann, Tinne Tuytelaars, Marie-Francine Moens
* **arXiv ID:** 2404.13766
* **One-liner:** Proposed FCA and DisCLIP embeddings to improve attribute-object binding in text-to-image generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2404.13766) | [[PDF]](https://arxiv.org/pdf/2404.13766)

> **Core Innovation**
> FCA controls visual attention maps using syntactic constraints, and DisCLIP disentangles CLIP embeddings, enhancing alignment without additional model training.

<details>
    <summary>Abstract</summary>
    Current diffusion models create photorealistic images given a text prompt as input but struggle to correctly bind attributes mentioned in the text to the right objects in the image. This is evidenced by our novel image-graph alignment model called EPViT (Edge Prediction Vision Transformer) for the evaluation of image-text alignment. To alleviate the above problem, we propose focused cross-attention (FCA) that controls the visual attention maps by syntactic constraints found in the input sentence. Additionally, the syntax structure of the prompt helps to disentangle the multimodal CLIP embeddings that are commonly used in T2I generation. The resulting DisCLIP embeddings and FCA are easily integrated in state-of-the-art diffusion models without additional training of these models. We show substantial improvements in T2I generation and especially its attribute-object binding on several datasets.\footnote{Code and data will be made available upon acceptance.
</details>

<details>
    <summary>Key points</summary>
    * Uses syntactic constraints to control cross-attention maps
    * Disentangles multimodal CLIP embeddings into DisCLIP
    * Integrates easily into existing diffusion models for better binding
</details>
</details>

---


<details>
<summary><b> Towards Better Text-to-Image Generation Alignment via Attention Modulation</b></summary>

* **Authors:** Yihang Wu, Xiao Cao, Kaixin Li, Zitan Chen, Haonan Wang, Lei Meng, Zhiyong Huang
* **arXiv ID:** 2404.13899
* **One-liner:** Proposed a training-free attribution-focusing mechanism for diffusion models to improve text-image alignment.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2404.13899) | [[PDF]](https://arxiv.org/pdf/2404.13899)

> **Core Innovation**
> Enhanced attention modulation in diffusion models to address entity leakage and attribute misalignment without additional training.

<details>
    <summary>Abstract</summary>
    In text-to-image generation tasks, the advancements of diffusion models have facilitated the fidelity of generated results. However, these models encounter challenges when processing text prompts containing multiple entities and attributes. The uneven distribution of attention results in the issues of entity leakage and attribute misalignment. Training from scratch to address this issue requires numerous labeled data and is resource-consuming. Motivated by this, we propose an attribution-focusing mechanism, a training-free phase-wise mechanism by modulation of attention for diffusion model. One of our core ideas is to guide the model to concentrate on the corresponding syntactic components of the prompt at distinct timesteps. To achieve this, we incorporate a temperature control mechanism within the early phases of the self-attention modules to mitigate entity leakage issues. An object-focused masking scheme and a phase-wise dynamic weight control mechanism are integrated into the cross-attention modules, enabling the model to discern the affiliation of semantic information between entities more effectively. The experimental results in various alignment scenarios demonstrate that our model attain better image-text alignment with minimal additional computational cost.
</details>

<details>
    <summary>Key points</summary>
    * Phase-wise attention modulation
    * Temperature control in self-attention
    * Object-focused masking in cross-attention
    * Dynamic weight control
</details>
</details>

---


<details>
<summary><b> Multimodal Large Language Model is a Human-Aligned Annotator for Text-to-Image Generation</b></summary>

* **Authors:** Xun Wu, Shaohan Huang, Furu Wei
* **arXiv ID:** 2404.15100
* **One-liner:** Created VisionPrefer, a high-quality preference dataset using multimodal LLMs to improve text-to-image model alignment.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2404.15100) | [[PDF]](https://arxiv.org/pdf/2404.15100)

> **Core Innovation**
> Leveraged AI-generated synthetic data for instruction tuning to enhance model alignment across multiple preference aspects.

<details>
    <summary>Abstract</summary>
    Recent studies have demonstrated the exceptional potentials of leveraging human preference datasets to refine text-to-image generative models, enhancing the alignment between generated images and textual prompts. Despite these advances, current human preference datasets are either prohibitively expensive to construct or suffer from a lack of diversity in preference dimensions, resulting in limited applicability for instruction tuning in open-source text-to-image generative models and hinder further exploration. To address these challenges and promote the alignment of generative models through instruction tuning, we leverage multimodal large language models to create VisionPrefer, a high-quality and fine-grained preference dataset that captures multiple preference aspects. We aggregate feedback from AI annotators across four aspects: prompt-following, aesthetic, fidelity, and harmlessness to construct VisionPrefer. To validate the effectiveness of VisionPrefer, we train a reward model VP-Score over VisionPrefer to guide the training of text-to-image generative models and the preference prediction accuracy of VP-Score is comparable to human annotators. Furthermore, we use two reinforcement learning methods to supervised fine-tune generative models to evaluate the performance of VisionPrefer, and extensive experimental results demonstrate that VisionPrefer significantly improves text-image alignment in compositional image generation across diverse aspects, e.g., aesthetic, and generalizes better than previous human-preference metrics across various image distributions. Moreover, VisionPrefer indicates that the integration of AI-generated synthetic data as a supervisory signal is a promising avenue for achieving improved alignment with human preferences in vision generative models.
</details>

<details>
    <summary>Key points</summary>
    * Dataset construction with AI annotators
    * Training of VP-Score reward model
    * Use of reinforcement learning for fine-tuning
    * Evaluation on compositional generation
</details>
</details>

---


<details>
<summary><b> ID-Aligner: Enhancing Identity-Preserving Text-to-Image Generation with Reward Feedback Learning</b></summary>

* **Authors:** Weifeng Chen, Jiacheng Zhang, Jie Wu, Hefeng Wu, Xuefeng Xiao, Liang Lin
* **arXiv ID:** 2404.15449
* **One-liner:** Introduced ID-Aligner, a feedback learning framework for identity-preserving text-to-image generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2404.15449) | [[PDF]](https://arxiv.org/pdf/2404.15449)

> **Core Innovation**
> Improved identity retention and aesthetic appeal in generated images using reward-based fine-tuning.

<details>
    <summary>Abstract</summary>
    The rapid development of diffusion models has triggered diverse applications. Identity-preserving text-to-image generation (ID-T2I) particularly has received significant attention due to its wide range of application scenarios like AI portrait and advertising. While existing ID-T2I methods have demonstrated impressive results, several key challenges remain: (1) It is hard to maintain the identity characteristics of reference portraits accurately, (2) The generated images lack aesthetic appeal especially while enforcing identity retention, and (3) There is a limitation that cannot be compatible with LoRA-based and Adapter-based methods simultaneously. To address these issues, we present \textbf{ID-Aligner}, a general feedback learning framework to enhance ID-T2I performance. To resolve identity features lost, we introduce identity consistency reward fine-tuning to utilize the feedback from face detection and recognition models to improve generated identity preservation. Furthermore, we propose identity aesthetic reward fine-tuning leveraging rewards from human-annotated preference data and automatically constructed feedback on character structure generation to provide aesthetic tuning signals. Thanks to its universal feedback fine-tuning framework, our method can be readily applied to both LoRA and Adapter models, achieving consistent performance gains. Extensive experiments on SD1.5 and SDXL diffusion models validate the effectiveness of our approach. \textbf{Project Page: \url{<a href="https://idaligner.github.io/" rel="external noopener nofollow" class="link-external link-https">this https URL</a>}}
</details>

<details>
    <summary>Key points</summary>
    * Identity consistency reward fine-tuning
    * Identity aesthetic reward fine-tuning
    * Compatibility with LoRA and Adapter models
    * Validation on SD1.5 and SDXL
</details>
</details>

---


<details>
<summary><b> G-Refine: A General Quality Refiner for Text-to-Image Generation</b></summary>

* **Authors:** Chunyi Li, Haoning Wu, Hongkun Hao, Zicheng Zhang, Tengchaun Kou, Chaofeng Chen, Lei Bai, Xiaohong Liu, Weisi Lin, Guangtao Zhai
* **arXiv ID:** 2404.18343
* **One-liner:** Developed G-Refine, a general image quality refiner for enhancing AI-generated images.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2404.18343) | [[PDF]](https://arxiv.org/pdf/2404.18343)

> **Core Innovation**
> Addressed perception and alignment quality defects through modular indicators and enhancement.

<details>
    <summary>Abstract</summary>
    With the evolution of Text-to-Image (T2I) models, the quality defects of AI-Generated Images (AIGIs) pose a significant barrier to their widespread adoption. In terms of both perception and alignment, existing models cannot always guarantee high-quality results. To mitigate this limitation, we introduce G-Refine, a general image quality refiner designed to enhance low-quality images without compromising the integrity of high-quality ones. The model is composed of three interconnected modules: a perception quality indicator, an alignment quality indicator, and a general quality enhancement module. Based on the mechanisms of the Human Visual System (HVS) and syntax trees, the first two indicators can respectively identify the perception and alignment deficiencies, and the last module can apply targeted quality enhancement accordingly. Extensive experimentation reveals that when compared to alternative optimization methods, AIGIs after G-Refine outperform in 10+ quality metrics across 4 databases. This improvement significantly contributes to the practical application of contemporary T2I models, paving the way for their broader adoption. The code will be released on <a href="https://github.com/Q-Future/Q-Refine" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Perception quality indicator based on HVS
    * Alignment quality indicator using syntax trees
    * General quality enhancement module
    * Evaluation on multiple quality metrics
</details>
</details>

---


<details>
<summary><b> On Mechanistic Knowledge Localization in Text-to-Image Generative Models</b></summary>

* **Authors:** Samyadeep Basu, Keivan Rezaei, Priyatham Kattakinda, Ryan Rossi, Cherry Zhao, Vlad Morariu, Varun Manjunatha, Soheil Feizi
* **arXiv ID:** 2405.01008
* **One-liner:** Proposed Mechanistic Localization for efficient model editing in text-to-image models.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2405.01008) | [[PDF]](https://arxiv.org/pdf/2405.01008)

> **Core Innovation**
> Localized knowledge to specific UNet layers to facilitate closed-form updates and editing.

<details>
    <summary>Abstract</summary>
    Identifying layers within text-to-image models which control visual attributes can facilitate efficient model editing through closed-form updates. Recent work, leveraging causal tracing show that early Stable-Diffusion variants confine knowledge primarily to the first layer of the CLIP text-encoder, while it diffuses throughout the <a href="http://UNet.Extending" rel="external noopener nofollow" class="link-external link-http">this http URL</a> this framework, we observe that for recent models (e.g., SD-XL, DeepFloyd), causal tracing fails in pinpointing localized knowledge, highlighting challenges in model editing. To address this issue, we introduce the concept of Mechanistic Localization in text-to-image models, where knowledge about various visual attributes (e.g., &#34;style&#34;, &#34;objects&#34;, &#34;facts&#34;) can be mechanistically localized to a small fraction of layers in the UNet, thus facilitating efficient model editing. We localize knowledge using our method LocoGen which measures the direct effect of intermediate layers to output generation by performing interventions in the cross-attention layers of the UNet. We then employ LocoEdit, a fast closed-form editing method across popular open-source text-to-image models (including the latest SD-XL)and explore the possibilities of neuron-level model editing. Using Mechanistic Localization, our work offers a better view of successes and failures in localization-based text-to-image model editing. Code will be available at <a href="https://github.com/samyadeepbasu/LocoGen" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * LocoGen method for localization
    * Interventions in cross-attention layers
    * LocoEdit for fast model editing
    * Application to SD-XL and other models
</details>
</details>

---


<details>
<summary><b> FlexEControl: Flexible and Efficient Multimodal Control for Text-to-Image Generation</b></summary>

* **Authors:** Xuehai He, Jian Zheng, Jacob Zhiyuan Fang, Robinson Piramuthu, Mohit Bansal, Vicente Ordonez, Gunnar A Sigurdsson, Nanyun Peng, Xin Eric Wang
* **arXiv ID:** 2405.04834
* **One-liner:** Designed FlexEControl, an efficient method for controllable text-to-image generation with multimodal inputs.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2405.04834) | [[PDF]](https://arxiv.org/pdf/2405.04834)

> **Core Innovation**
> Enhanced faithfulness and reduced computational overhead through weight decomposition.

<details>
    <summary>Abstract</summary>
    Controllable text-to-image (T2I) diffusion models generate images conditioned on both text prompts and semantic inputs of other modalities like edge maps. Nevertheless, current controllable T2I methods commonly face challenges related to efficiency and faithfulness, especially when conditioning on multiple inputs from either the same or diverse modalities. In this paper, we propose a novel Flexible and Efficient method, FlexEControl, for controllable T2I generation. At the core of FlexEControl is a unique weight decomposition strategy, which allows for streamlined integration of various input types. This approach not only enhances the faithfulness of the generated image to the control, but also significantly reduces the computational overhead typically associated with multimodal conditioning. Our approach achieves a reduction of 41% in trainable parameters and 30% in memory usage compared with Uni-ControlNet. Moreover, it doubles data efficiency and can flexibly generate images under the guidance of multiple input conditions of various modalities.
</details>

<details>
    <summary>Key points</summary>
    * Weight decomposition strategy
    * Streamlined integration of various inputs
    * Reduction in parameters and memory usage
    * Flexibility with multiple modalities
</details>
</details>

---


<details>
<summary><b> TriLoRA: Integrating SVD for Advanced Style Personalization in Text-to-Image Generation</b></summary>

* **Authors:** Chengcheng Feng, Mu He, Qiuyu Tian, Haojie Yin, Xiaofang Zhao, Hongwei Tang, Xingqiang Wei
* **arXiv ID:** 2405.11236
* **One-liner:** Integrated SVD into LoRA for improved fine-tuning of image generation models.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2405.11236) | [[PDF]](https://arxiv.org/pdf/2405.11236)

> **Core Innovation**
> Enhanced stability, reduced overfitting, and better feature capture in model outputs.

<details>
    <summary>Abstract</summary>
    As deep learning technology continues to advance, image generation models, especially models like Stable Diffusion, are finding increasingly widespread application in visual arts creation. However, these models often face challenges such as overfitting, lack of stability in generated results, and difficulties in accurately capturing the features desired by creators during the fine-tuning process. In response to these challenges, we propose an innovative method that integrates Singular Value Decomposition (SVD) into the Low-Rank Adaptation (LoRA) parameter update strategy, aimed at enhancing the fine-tuning efficiency and output quality of image generation models. By incorporating SVD within the LoRA framework, our method not only effectively reduces the risk of overfitting but also enhances the stability of model outputs, and captures subtle, creator-desired feature adjustments more accurately. We evaluated our method on multiple datasets, and the results show that, compared to traditional fine-tuning methods, our approach significantly improves the model&#39;s generalization ability and creative flexibility while maintaining the quality of generation. Moreover, this method maintains LoRA&#39;s excellent performance under resource-constrained conditions, allowing for significant improvements in image generation quality without sacrificing the original efficiency and resource advantages.
</details>

<details>
    <summary>Key points</summary>
    * SVD incorporation in LoRA framework
    * Risk reduction of overfitting
    * Improved output stability
    * Maintained efficiency under constraints
</details>
</details>

---


<details>
<summary><b> An Empirical Study and Analysis of Text-to-Image Generation Using Large Language Model-Powered Textual Representation</b></summary>

* **Authors:** Zhiyu Tan, Mengping Yang, Luozheng Qin, Hao Yang, Ye Qian, Qiang Zhou, Cheng Zhang, Hao Li
* **arXiv ID:** 2405.12914
* **One-liner:** Investigated LLMs as text encoders for multilingual and longer-context text-to-image generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2405.12914) | [[PDF]](https://arxiv.org/pdf/2405.12914)

> **Core Innovation**
> Enabled superior text representation and generation quality with a lightweight adapter.

<details>
    <summary>Abstract</summary>
    One critical prerequisite for faithful text-to-image generation is the accurate understanding of text inputs. Existing methods leverage the text encoder of the CLIP model to represent input prompts. However, the pre-trained CLIP model can merely encode English with a maximum token length of 77. Moreover, the model capacity of the text encoder from CLIP is relatively limited compared to Large Language Models (LLMs), which offer multilingual input, accommodate longer context, and achieve superior text representation. In this paper, we investigate LLMs as the text encoder to improve the language understanding in text-to-image generation. Unfortunately, training text-to-image generative model with LLMs from scratch demands significant computational resources and data. To this end, we introduce a three-stage training pipeline that effectively and efficiently integrates the existing text-to-image model with LLMs. Specifically, we propose a lightweight adapter that enables fast training of the text-to-image model using the textual representations from LLMs. Extensive experiments demonstrate that our model supports not only multilingual but also longer input context with superior image generation quality.
</details>

<details>
    <summary>Key points</summary>
    * Three-stage training pipeline
    * Lightweight adapter for LLM integration
    * Support for multilingual inputs
    * Accommodation of longer contexts
</details>
</details>

---


<details>
<summary><b> Personalized Residuals for Concept-Driven Text-to-Image Generation</b></summary>

* **Authors:** Cusuh Ham, Matthew Fisher, James Hays, Nicholas Kolkin, Yuchen Liu, Richard Zhang, Tobias Hinz
* **arXiv ID:** 2405.12978
* **One-liner:** Introduced personalized residuals and localized sampling for efficient concept-driven generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2405.12978) | [[PDF]](https://arxiv.org/pdf/2405.12978)

> **Core Innovation**
> Achieved fast concept learning and sampling with minimal parameters and computational cost.

<details>
    <summary>Abstract</summary>
    We present personalized residuals and localized attention-guided sampling for efficient concept-driven generation using text-to-image diffusion models. Our method first represents concepts by freezing the weights of a pretrained text-conditioned diffusion model and learning low-rank residuals for a small subset of the model&#39;s layers. The residual-based approach then directly enables application of our proposed sampling technique, which applies the learned residuals only in areas where the concept is localized via cross-attention and applies the original diffusion weights in all other regions. Localized sampling therefore combines the learned identity of the concept with the existing generative prior of the underlying diffusion model. We show that personalized residuals effectively capture the identity of a concept in ~3 minutes on a single GPU without the use of regularization images and with fewer parameters than previous models, and localized sampling allows using the original model as strong prior for large parts of the image.
</details>

<details>
    <summary>Key points</summary>
    * Low-rank residual learning
    * Localized attention-guided sampling
    * Freezing of pretrained model weights
    * Application without regularization images
</details>
</details>

---


<details>
<summary><b> Kaleido Diffusion: Improving Conditional Diffusion Models with Autoregressive Latent Modeling</b></summary>

* **Authors:** Jiatao Gu, Ying Shen, Shuangfei Zhai, Yizhe Zhang, Navdeep Jaitly, Joshua M. Susskind
* **arXiv ID:** 2405.21048
* **One-liner:** Presented Kaleido to enhance diversity in diffusion model image generation using autoregressive latent priors.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2405.21048) | [[PDF]](https://arxiv.org/pdf/2405.21048)

> **Core Innovation**
> Broadened output diversity while maintaining quality through latent variable guidance.

<details>
    <summary>Abstract</summary>
    Diffusion models have emerged as a powerful tool for generating high-quality images from textual descriptions. Despite their successes, these models often exhibit limited diversity in the sampled images, particularly when sampling with a high classifier-free guidance weight. To address this issue, we present Kaleido, a novel approach that enhances the diversity of samples by incorporating autoregressive latent priors. Kaleido integrates an autoregressive language model that encodes the original caption and generates latent variables, serving as abstract and intermediary representations for guiding and facilitating the image generation process. In this paper, we explore a variety of discrete latent representations, including textual descriptions, detection bounding boxes, object blobs, and visual tokens. These representations diversify and enrich the input conditions to the diffusion models, enabling more diverse outputs. Our experimental results demonstrate that Kaleido effectively broadens the diversity of the generated image samples from a given textual description while maintaining high image quality. Furthermore, we show that Kaleido adheres closely to the guidance provided by the generated latent variables, demonstrating its capability to effectively control and direct the image generation process.
</details>

<details>
    <summary>Key points</summary>
    * Integration of autoregressive language model
    * Use of discrete latent representations
    * Diversification of input conditions
    * Control over generation process
</details>
</details>

---


<details>
<summary><b> AttnDreamBooth: Towards Text-Aligned Personalized Text-to-Image Generation</b></summary>

* **Authors:** Lianyu Pang, Jian Yin, Baoquan Zhao, Feize Wu, Fu Lee Wang, Qing Li, Xudong Mao
* **arXiv ID:** 2406.05000
* **One-liner:** Introduced AttnDreamBooth to improve text-to-image personalization by addressing embedding alignment issues.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2406.05000) | [[PDF]](https://arxiv.org/pdf/2406.05000)

> **Core Innovation**
> Separated the learning of embedding alignment, attention map, and subject identity in different training stages with cross-attention regularization.

<details>
    <summary>Abstract</summary>
    Recent advances in text-to-image models have enabled high-quality personalized image synthesis of user-provided concepts with flexible textual control. In this work, we analyze the limitations of two primary techniques in text-to-image personalization: Textual Inversion and DreamBooth. When integrating the learned concept into new prompts, Textual Inversion tends to overfit the concept, while DreamBooth often overlooks it. We attribute these issues to the incorrect learning of the embedding alignment for the concept. We introduce AttnDreamBooth, a novel approach that addresses these issues by separately learning the embedding alignment, the attention map, and the subject identity in different training stages. We also introduce a cross-attention map regularization term to enhance the learning of the attention map. Our method demonstrates significant improvements in identity preservation and text alignment compared to the baseline methods.
</details>

<details>
    <summary>Key points</summary>
    * Analyzed limitations of Textual Inversion and DreamBooth
    * Attributed issues to incorrect embedding alignment
    * Proposed multi-stage training for separate components
    * Introduced cross-attention map regularization
</details>
</details>

---


<details>
<summary><b> Ctrl-X: Controlling Structure and Appearance for Text-To-Image Generation Without Guidance</b></summary>

* **Authors:** Kuan Heng Lin, Sicheng Mo, Ben Klingher, Fangzhou Mu, Bolei Zhou
* **arXiv ID:** 2406.07540
* **One-liner:** Developed Ctrl-X for efficient structure and appearance control in text-to-image generation without training.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2406.07540) | [[PDF]](https://arxiv.org/pdf/2406.07540)

> **Core Innovation**
> Enabled feed-forward structure alignment and semantic-aware appearance transfer for plug-and-play functionality.

<details>
    <summary>Abstract</summary>
    Recent controllable generation approaches such as FreeControl and Diffusion Self-Guidance bring fine-grained spatial and appearance control to text-to-image (T2I) diffusion models without training auxiliary modules. However, these methods optimize the latent embedding for each type of score function with longer diffusion steps, making the generation process time-consuming and limiting their flexibility and use. This work presents Ctrl-X, a simple framework for T2I diffusion controlling structure and appearance without additional training or guidance. Ctrl-X designs feed-forward structure control to enable the structure alignment with a structure image and semantic-aware appearance transfer to facilitate the appearance transfer from a user-input image. Extensive qualitative and quantitative experiments illustrate the superior performance of Ctrl-X on various condition inputs and model checkpoints. In particular, Ctrl-X supports novel structure and appearance control with arbitrary condition images of any modality, exhibits superior image quality and appearance transfer compared to existing works, and provides instant plug-and-play functionality to any T2I and text-to-video (T2V) diffusion model. See our project page for an overview of the results: <a href="https://genforce.github.io/ctrl-x" rel="external noopener nofollow" class="link-external link-https">this https URL</a>
</details>

<details>
    <summary>Key points</summary>
    * Designed feed-forward structure control
    * Implemented semantic-aware appearance transfer
    * Supported arbitrary condition images
    * Provided instant plug-and-play to T2I and T2V models
</details>
</details>

---


<details>
<summary><b> Commonsense-T2I Challenge: Can Text-to-Image Generation Models Understand Commonsense?</b></summary>

* **Authors:** Xingyu Fu, Muyu He, Yujie Lu, William Yang Wang, Dan Roth
* **arXiv ID:** 2406.07546
* **One-liner:** Created Commonsense-T2I benchmark to evaluate text-to-image models on commonsense reasoning.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2406.07546) | [[PDF]](https://arxiv.org/pdf/2406.07546)

> **Core Innovation**
> Provided adversarial text prompts with fine-grained annotations to assess model alignment with real-life scenarios.

<details>
    <summary>Abstract</summary>
    We present a novel task and benchmark for evaluating the ability of text-to-image(T2I) generation models to produce images that align with commonsense in real life, which we call Commonsense-T2I. Given two adversarial text prompts containing an identical set of action words with minor differences, such as &#34;a lightbulb without electricity&#34; v.s. &#34;a lightbulb with electricity&#34;, we evaluate whether T2I models can conduct visual-commonsense reasoning, e.g. produce images that fit &#34;the lightbulb is unlit&#34; vs. &#34;the lightbulb is lit&#34; correspondingly. Commonsense-T2I presents an adversarial challenge, providing pairwise text prompts along with expected outputs. The dataset is carefully hand-curated by experts and annotated with fine-grained labels, such as commonsense type and likelihood of the expected outputs, to assist analyzing model behavior. We benchmark a variety of state-of-the-art (sota) T2I models and surprisingly find that, there is still a large gap between image synthesis and real life photos--even the DALL-E 3 model could only achieve 48.92% on Commonsense-T2I, and the stable diffusion XL model only achieves 24.92% accuracy. Our experiments show that GPT-enriched prompts cannot solve this challenge, and we include a detailed analysis about possible reasons for such deficiency. We aim for Commonsense-T2I to serve as a high-quality evaluation benchmark for T2I commonsense checking, fostering advancements in real life image generation.
</details>

<details>
    <summary>Key points</summary>
    * Defined adversarial pairwise text prompts
    * Hand-curated dataset with expert annotations
    * Benchmarked state-of-the-art T2I models
    * Analyzed gaps in commonsense reasoning
</details>
</details>

---


<details>
<summary><b> Improving Compositional Attribute Binding in Text-to-Image Generative Models via Enhanced Text Embeddings</b></summary>

* **Authors:** Arman Zarei, Keivan Rezaei, Samyadeep Basu, Mehrdad Saberi, Mazda Moayeri, Priyatham Kattakinda, Soheil Feizi
* **arXiv ID:** 2406.07844
* **One-liner:** Identified and addressed compositional attribute binding failures in text-to-image models.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2406.07844) | [[PDF]](https://arxiv.org/pdf/2406.07844)

> **Core Innovation**
> Fine-tuned a linear projection on CLIP's representation to improve attribute-object association without harming FID.

<details>
    <summary>Abstract</summary>
    Text-to-image diffusion-based generative models have the stunning ability to generate photo-realistic images and achieve state-of-the-art low FID scores on challenging image generation benchmarks. However, one of the primary failure modes of these text-to-image generative models is in composing attributes, objects, and their associated relationships accurately into an image. In our paper, we investigate compositional attribute binding failures, where the model fails to correctly associate descriptive attributes (such as color, shape, or texture) with the corresponding objects in the generated images, and highlight that imperfect text conditioning with CLIP text-encoder is one of the primary reasons behind the inability of these models to generate high-fidelity compositional scenes. In particular, we show that (i) there exists an optimal text-embedding space that can generate highly coherent compositional scenes showing that the output space of the CLIP text-encoder is sub-optimal, and (ii) the final token embeddings in CLIP are erroneous as they often include attention contributions from unrelated tokens in compositional prompts. Our main finding shows that significant compositional improvements can be achieved (without harming the model&#39;s FID score) by fine-tuning only a simple and parameter-efficient linear projection on CLIP&#39;s representation space in Stable-Diffusion variants using a small set of compositional image-text pairs.
</details>

<details>
    <summary>Key points</summary>
    * Investigated compositional attribute binding failures
    * Showed sub-optimal CLIP text-encoder output
    * Proposed fine-tuning linear projection
    * Used small set of compositional image-text pairs
</details>
</details>

---


<details>
<summary><b> FairCoT: Enhancing Fairness in Text-to-Image Generation via Chain of Thought Reasoning with Multimodal Large Language Models</b></summary>

* **Authors:** Zahraa Al Sahili, Ioannis Patras, Matthew Purver
* **arXiv ID:** 2406.09070
* **One-liner:** Introduced FairCoT to enhance fairness in text-to-image models using Chain of Thought reasoning.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2406.09070) | [[PDF]](https://arxiv.org/pdf/2406.09070)

> **Core Innovation**
> Employed iterative CoT refinement to mitigate biases and adjust prompts for diverse representation.

<details>
    <summary>Abstract</summary>
    In the domain of text-to-image generative models, biases inherent in training datasets often propagate into generated content, posing significant ethical challenges, particularly in socially sensitive contexts. We introduce FairCoT, a novel framework that enhances fairness in text to image models through Chain of Thought (CoT) reasoning within multimodal generative large language models. FairCoT employs iterative CoT refinement to systematically mitigate biases, and dynamically adjusts textual prompts in real time, ensuring diverse and equitable representation in generated images. By integrating iterative reasoning processes, FairCoT addresses the limitations of zero shot CoT in sensitive scenarios, balancing creativity with ethical responsibility. Experimental evaluations across popular text-to-image systems including DALLE and various Stable Diffusion variants, demonstrate that FairCoT significantly enhances fairness and diversity without sacrificing image quality or semantic fidelity. By combining robust reasoning, lightweight deployment, and extensibility to multiple models, FairCoT represents a promising step toward more socially responsible and transparent AI driven content generation.
</details>

<details>
    <summary>Key points</summary>
    * Integrated iterative Chain of Thought reasoning
    * Dynamically adjusted textual prompts
    * Addressed biases in sensitive contexts
    * Balanced creativity with ethical responsibility
</details>
</details>

---


<details>
<summary><b> STAR: Scale-wise Text-conditioned AutoRegressive image generation</b></summary>

* **Authors:** Xiaoxiao Ma, Mohan Zhou, Tao Liang, Yalong Bai, Tiejun Zhao, Biye Li, Huaian Chen, Yi Jin
* **arXiv ID:** 2406.10797
* **One-liner:** Developed STAR, a scale-wise auto-regressive model for high-resolution text-to-image generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2406.10797) | [[PDF]](https://arxiv.org/pdf/2406.10797)

> **Core Innovation**
> Enabled text-driven 1024x1024 image synthesis with stable sampling and normalized RoPE for structural consistency.

<details>
    <summary>Abstract</summary>
    We introduce STAR, a text-to-image model that employs a scale-wise auto-regressive paradigm. Unlike VAR, which is constrained to class-conditioned synthesis for images up to 256$\times$256, STAR enables text-driven image generation up to 1024$\times$1024 through three key designs. First, we introduce a pre-trained text encoder to extract and adopt representations for textual constraints, enhancing details and generalizability. Second, given the inherent structural correlation across different scales, we leverage 2D Rotary Positional Encoding (RoPE) and tweak it into a normalized version, ensuring consistent interpretation of relative positions across token maps and stabilizing the training process. Third, we observe that simultaneously sampling all tokens within a single scale can disrupt inter-token relationships, leading to structural instability, particularly in high-resolution generation. To address this, we propose a novel stable sampling method that incorporates causal relationships into the sampling process, ensuring both rich details and stable structures. Compared to previous diffusion models and auto-regressive models, STAR surpasses existing benchmarks in fidelity, text-image consistency, and aesthetic quality, requiring just 2.21s for 1024$\times$1024 images on A100. This highlights the potential of auto-regressive methods in high-quality image synthesis, offering new directions for the text-to-image generation.
</details>

<details>
    <summary>Key points</summary>
    * Introduced pre-trained text encoder
    * Applied normalized 2D Rotary Positional Encoding
    * Proposed stable sampling with causal relationships
    * Achieved fast generation on A100 GPU
</details>
</details>

---


<details>
<summary><b> AITTI: Learning Adaptive Inclusive Token for Text-to-Image Generation</b></summary>

* **Authors:** Xinyu Hou, Xiaoming Li, Chen Change Loy
* **arXiv ID:** 2406.12805
* **One-liner:** Proposed adaptive inclusive tokens to mitigate stereotypical biases in text-to-image generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2406.12805) | [[PDF]](https://arxiv.org/pdf/2406.12805)

> **Core Innovation**
> Used a lightweight adaptive mapping network to customize tokens without attribute specification.

<details>
    <summary>Abstract</summary>
    Despite the high-quality results of text-to-image generation, stereotypical biases have been spotted in their generated contents, compromising the fairness of generative models. In this work, we propose to learn adaptive inclusive tokens to shift the attribute distribution of the final generative outputs. Unlike existing de-biasing approaches, our method requires neither explicit attribute specification nor prior knowledge of the bias distribution. Specifically, the core of our method is a lightweight adaptive mapping network, which can customize the inclusive tokens for the concepts to be de-biased, making the tokens generalizable to unseen concepts regardless of their original bias distributions. This is achieved by tuning the adaptive mapping network with a handful of balanced and inclusive samples using an anchor loss. Experimental results demonstrate that our method outperforms previous bias mitigation methods without attribute specification while preserving the alignment between generative results and text descriptions. Moreover, our method achieves comparable performance to models that require specific attributes or editing directions for generation. Extensive experiments showcase the effectiveness of our adaptive inclusive tokens in mitigating stereotypical bias in text-to-image generation. The code will be available at <a href="https://github.com/itsmag11/AITTI" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Designed adaptive mapping network
    * Tuned with balanced samples using anchor loss
    * Generalized to unseen concepts
    * Preserved text alignment while reducing bias
</details>
</details>

---


<details>
<summary><b> Beyond Thumbs Up/Down: Untangling Challenges of Fine-Grained Feedback for Text-to-Image Generation</b></summary>

* **Authors:** Katherine M. Collins, Najoung Kim, Yonatan Bitton, Verena Rieser, Shayegan Omidshafiei, Yushi Hu, Sherol Chen, Senjuti Dutta, Minsuk Chang, Kimin Lee, Youwei Liang, Georgina Evans, Sahil Singla, Gang Li, Adrian Weller, Junfeng He, Deepak Ramachandran, Krishnamurthy Dj Dvijotham
* **arXiv ID:** 2406.16807
* **One-liner:** Investigated the effectiveness of fine-grained vs. coarse-grained human feedback for text-to-image reward models.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2406.16807) | [[PDF]](https://arxiv.org/pdf/2406.16807)

> **Core Innovation**
> Analyzed complexities in feedback types and their impact on model accuracy under various settings.

<details>
    <summary>Abstract</summary>
    Human feedback plays a critical role in learning and refining reward models for text-to-image generation, but the optimal form the feedback should take for learning an accurate reward function has not been conclusively established. This paper investigates the effectiveness of fine-grained feedback which captures nuanced distinctions in image quality and prompt-alignment, compared to traditional coarse-grained feedback (for example, thumbs up/down or ranking between a set of options). While fine-grained feedback holds promise, particularly for systems catering to diverse societal preferences, we show that demonstrating its superiority to coarse-grained feedback is not automatic. Through experiments on real and synthetic preference data, we surface the complexities of building effective models due to the interplay of model choice, feedback type, and the alignment between human judgment and computational interpretation. We identify key challenges in eliciting and utilizing fine-grained feedback, prompting a reassessment of its assumed benefits and practicality. Our findings -- e.g., that fine-grained feedback can lead to worse models for a fixed budget, in some settings; however, in controlled settings with known attributes, fine grained rewards can indeed be more helpful -- call for careful consideration of feedback attributes and potentially beckon novel modeling approaches to appropriately unlock the potential value of fine-grained feedback in-the-wild.
</details>

<details>
    <summary>Key points</summary>
    * Compared fine-grained and coarse-grained feedback
    * Identified challenges in feedback elicitation
    * Showed mixed results depending on settings
    * Called for novel modeling approaches
</details>
</details>

---


<details>
<summary><b> MUMU: Bootstrapping Multimodal Image Generation from Text-to-Image Data</b></summary>

* **Authors:** William Berman, Alexander Peysakhovich
* **arXiv ID:** 2406.18790
* **One-liner:** Trained MUMU model for image generation from multimodal prompts with interleaved text and images.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2406.18790) | [[PDF]](https://arxiv.org/pdf/2406.18790)

> **Core Innovation**
> Enabled coherent composition of inputs from different images for tasks like style transfer and character consistency.

<details>
    <summary>Abstract</summary>
    We train a model to generate images from multimodal prompts of interleaved text and images such as &#34;a &lt;picture of a man&gt; man and his &lt;picture of a dog&gt; dog in an &lt;picture of a cartoon&gt; animated style.&#34; We bootstrap a multimodal dataset by extracting semantically meaningful image crops corresponding to words in the image captions of synthetically generated and publicly available text-image data. Our model, MUMU, is composed of a vision-language model encoder with a diffusion decoder and is trained on a single 8xH100 GPU node. Despite being only trained on crops from the same image, MUMU learns to compose inputs from different images into a coherent output. For example, an input of a realistic person and a cartoon will output the same person in the cartoon style, and an input of a standing subject and a scooter will output the subject riding the scooter. As a result, our model generalizes to tasks such as style transfer and character consistency. Our results show the promise of using multimodal models as general purpose controllers for image generation.
</details>

<details>
    <summary>Key points</summary>
    * Bootstrapped multimodal dataset from image crops
    * Used vision-language encoder with diffusion decoder
    * Learned to compose inputs from different images
    * Generalized to style transfer and consistency tasks
</details>
</details>

---


<details>
<summary><b> AnyControl: Create Your Artwork with Versatile Control on Text-to-Image Generation</b></summary>

* **Authors:** Yanan Sun, Yanchen Liu, Yinhao Tang, Wenjie Pei, Kai Chen
* **arXiv ID:** 2406.18958
* **One-liner:** Proposed AnyControl for multi-control image synthesis with arbitrary combinations of control signals.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2406.18958) | [[PDF]](https://arxiv.org/pdf/2406.18958)

> **Core Innovation**
> Developed a Multi-Control Encoder for unified embedding to handle diverse inputs and maintain semantic alignment.

<details>
    <summary>Abstract</summary>
    The field of text-to-image (T2I) generation has made significant progress in recent years, largely driven by advancements in diffusion models. Linguistic control enables effective content creation, but struggles with fine-grained control over image generation. This challenge has been explored, to a great extent, by incorporating additional user-supplied spatial conditions, such as depth maps and edge maps, into pre-trained T2I models through extra encoding. However, multi-control image synthesis still faces several challenges. Specifically, current approaches are limited in handling free combinations of diverse input control signals, overlook the complex relationships among multiple spatial conditions, and often fail to maintain semantic alignment with provided textual prompts. This can lead to suboptimal user experiences. To address these challenges, we propose AnyControl, a multi-control image synthesis framework that supports arbitrary combinations of diverse control signals. AnyControl develops a novel Multi-Control Encoder that extracts a unified multi-modal embedding to guide the generation process. This approach enables a holistic understanding of user inputs, and produces high-quality, faithful results under versatile control signals, as demonstrated by extensive quantitative and qualitative evaluations. Our project page is available in <a href="https://any-control.github.io" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Introduced Multi-Control Encoder
    * Supported arbitrary control signal combinations
    * Addressed relationships among spatial conditions
    * Ensured semantic alignment with text prompts
</details>
</details>

---


<details>
<summary><b> PopAlign: Population-Level Alignment for Fair Text-to-Image Generation</b></summary>

* **Authors:** Shufan Li, Harkanwar Singh, Aditya Grover
* **arXiv ID:** 2406.19668
* **One-liner:** Introduced PopAlign for population-level bias mitigation in T2I models.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2406.19668) | [[PDF]](https://arxiv.org/pdf/2406.19668)

> **Core Innovation**
> Developed a novel optimization method that addresses biases at the population level, unlike existing pairwise preference methods.

<details>
    <summary>Abstract</summary>
    Text-to-image (T2I) models achieve high-fidelity generation through extensive training on large datasets. However, these models may unintentionally pick up undesirable biases of their training data, such as over-representation of particular identities in gender or ethnicity neutral prompts. Existing alignment methods such as Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) fail to address this problem effectively because they operate on pairwise preferences consisting of individual samples, while the aforementioned biases can only be measured at a population level. For example, a single sample for the prompt &#34;doctor&#34; could be male or female, but a model generating predominantly male doctors even with repeated sampling reflects a gender bias. To address this limitation, we introduce PopAlign, a novel approach for population-level preference optimization, while standard optimization would prefer entire sets of samples over others. We further derive a stochastic lower bound that directly optimizes for individual samples from preferred populations over others for scalable training. Using human evaluation and standard image quality and bias metrics, we show that PopAlign significantly mitigates the bias of pretrained T2I models while largely preserving the generation quality. Code is available at <a href="https://github.com/jacklishufan/PopAlignSDXL" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Population-level preference optimization
    * Stochastic lower bound for scalable training
    * Mitigation of biases while preserving generation quality
</details>
</details>

---


<details>
<summary><b> Prompt Refinement with Image Pivot for Text-to-Image Generation</b></summary>

* **Authors:** Jingtao Zhan, Qingyao Ai, Yiqun Liu, Yingwei Pan, Ting Yao, Jiaxin Mao, Shaoping Ma, Tao Mei
* **arXiv ID:** 2407.00247
* **One-liner:** Proposed PRIP for zero-shot prompt refinement using image pivots.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2407.00247) | [[PDF]](https://arxiv.org/pdf/2407.00247)

> **Core Innovation**
> Innovated a method to translate user prompts into system languages by leveraging image representations as intermediaries.

<details>
    <summary>Abstract</summary>
    For text-to-image generation, automatically refining user-provided natural language prompts into the keyword-enriched prompts favored by systems is essential for the user experience. Such a prompt refinement process is analogous to translating the prompt from &#34;user languages&#34; into &#34;system languages&#34;. However, the scarcity of such parallel corpora makes it difficult to train a prompt refinement model. Inspired by zero-shot machine translation techniques, we introduce Prompt Refinement with Image Pivot (PRIP). PRIP innovatively uses the latent representation of a user-preferred image as an intermediary &#34;pivot&#34; between the user and system languages. It decomposes the refinement process into two data-rich tasks: inferring representations of user-preferred images from user languages and subsequently translating image representations into system languages. Thus, it can leverage abundant data for training. Extensive experiments show that PRIP substantially outperforms a wide range of baselines and effectively transfers to unseen systems in a zero-shot manner.
</details>

<details>
    <summary>Key points</summary>
    * Decomposition into inferring image representations and translating to system languages
    * Use of zero-shot machine translation techniques
    * Effective transfer to unseen systems
</details>
</details>

---


<details>
<summary><b> Efficient Personalized Text-to-image Generation by Leveraging Textual Subspace</b></summary>

* **Authors:** Shian Du, Xiaotian Cheng, Qi Qian, Henglu Wei, Yi Xu, Xiangyang Ji
* **arXiv ID:** 2407.00608
* **One-liner:** Efficient personalized T2I generation by optimizing in a textual subspace.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2407.00608) | [[PDF]](https://arxiv.org/pdf/2407.00608)

> **Core Innovation**
> Addressed inefficiencies in embedding optimization and improved alignment with novel prompts.

<details>
    <summary>Abstract</summary>
    Personalized text-to-image generation has attracted unprecedented attention in the recent few years due to its unique capability of generating highly-personalized images via using the input concept dataset and novel textual prompt. However, previous methods solely focus on the performance of the reconstruction task, degrading its ability to combine with different textual prompt. Besides, optimizing in the high-dimensional embedding space usually leads to unnecessary time-consuming training process and slow convergence. To address these issues, we propose an efficient method to explore the target embedding in a textual subspace, drawing inspiration from the self-expressiveness property. Additionally, we propose an efficient selection strategy for determining the basis vectors of the textual subspace. The experimental evaluations demonstrate that the learned embedding can not only faithfully reconstruct input image, but also significantly improves its alignment with novel input textual prompt. Furthermore, we observe that optimizing in the textual subspace leads to an significant improvement of the robustness to the initial word, relaxing the constraint that requires users to input the most relevant initial word. Our method opens the door to more efficient representation learning for personalized text-to-image generation.
</details>

<details>
    <summary>Key points</summary>
    * Exploration of target embedding in textual subspace
    * Basis vector selection strategy
    * Enhanced robustness to initial word input
</details>
</details>

---


<details>
<summary><b> LLM4GEN: Leveraging Semantic Representation of LLMs for Text-to-Image Generation</b></summary>

* **Authors:** Mushui Liu, Yuhang Ma, Yang Zhen, Jun Dan, Yunlong Yu, Zeng Zhao, Zhipeng Hu, Bai Liu, Changjie Fan
* **arXiv ID:** 2407.00737
* **One-liner:** Enhanced T2I diffusion models with LLM features via LLM4GEN framework.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2407.00737) | [[PDF]](https://arxiv.org/pdf/2407.00737)

> **Core Innovation**
> Improved semantic understanding and alignment in complex prompt scenarios using LLMs.

<details>
    <summary>Abstract</summary>
    Diffusion models have exhibited substantial success in text-to-image generation. However, they often encounter challenges when dealing with complex and dense prompts involving multiple objects, attribute binding, and long descriptions. In this paper, we propose a novel framework called \textbf{LLM4GEN}, which enhances the semantic understanding of text-to-image diffusion models by leveraging the representation of Large Language Models (LLMs). It can be seamlessly incorporated into various diffusion models as a plug-and-play component. A specially designed Cross-Adapter Module (CAM) integrates the original text features of text-to-image models with LLM features, thereby enhancing text-to-image generation. Additionally, to facilitate and correct entity-attribute relationships in text prompts, we develop an entity-guided regularization loss to further improve generation performance. We also introduce DensePrompts, which contains $7,000$ dense prompts to provide a comprehensive evaluation for the text-to-image generation task. Experiments indicate that LLM4GEN significantly improves the semantic alignment of SD1.5 and SDXL, demonstrating increases of 9.69\% and 12.90\% in color on T2I-CompBench, respectively. Moreover, it surpasses existing models in terms of sample quality, image-text alignment, and human evaluation.
</details>

<details>
    <summary>Key points</summary>
    * Integration of LLM features with Cross-Adapter Module
    * Entity-guided regularization loss
    * Introduction of DensePrompts dataset for evaluation
</details>
</details>

---


<details>
<summary><b> InstantStyle-Plus: Style Transfer with Content-Preserving in Text-to-Image Generation</b></summary>

* **Authors:** Haofan Wang, Peng Xing, Renyuan Huang, Hao Ai, Qixun Wang, Xu Bai
* **arXiv ID:** 2407.00788
* **One-liner:** Introduced InstantStyle-Plus for balanced style transfer in diffusion models.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2407.00788) | [[PDF]](https://arxiv.org/pdf/2407.00788)

> **Core Innovation**
> Achieved seamless integration of style while preserving content integrity through modular components.

<details>
    <summary>Abstract</summary>
    Style transfer is an inventive process designed to create an image that maintains the essence of the original while embracing the visual style of another. Although diffusion models have demonstrated impressive generative power in personalized subject-driven or style-driven applications, existing state-of-the-art methods still encounter difficulties in achieving a seamless balance between content preservation and style enhancement. For example, amplifying the style&#39;s influence can often undermine the structural integrity of the content. To address these challenges, we deconstruct the style transfer task into three core elements: 1) Style, focusing on the image&#39;s aesthetic characteristics; 2) Spatial Structure, concerning the geometric arrangement and composition of visual elements; and 3) Semantic Content, which captures the conceptual meaning of the image. Guided by these principles, we introduce InstantStyle-Plus, an approach that prioritizes the integrity of the original content while seamlessly integrating the target style. Specifically, our method accomplishes style injection through an efficient, lightweight process, utilizing the cutting-edge InstantStyle framework. To reinforce the content preservation, we initiate the process with an inverted content latent noise and a versatile plug-and-play tile ControlNet for preserving the original image&#39;s intrinsic layout. We also incorporate a global semantic adapter to enhance the semantic content&#39;s fidelity. To safeguard against the dilution of style information, a style extractor is employed as discriminator for providing supplementary style guidance. Codes will be available at <a href="https://github.com/instantX-research/InstantStyle-Plus" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Deconstruction into style, spatial structure, and semantic content
    * Use of inverted content latent noise and tile ControlNet
    * Global semantic adapter and style extractor for guidance
</details>
</details>

---


<details>
<summary><b> JeDi: Joint-Image Diffusion Models for Finetuning-Free Personalized Text-to-Image Generation</b></summary>

* **Authors:** Yu Zeng, Vishal M. Patel, Haochen Wang, Xun Huang, Ting-Chun Wang, Ming-Yu Liu, Yogesh Balaji
* **arXiv ID:** 2407.06187
* **One-liner:** Developed JEDI for finetuning-free personalized T2I generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2407.06187) | [[PDF]](https://arxiv.org/pdf/2407.06187)

> **Core Innovation**
> Enabled fast personalization by learning joint distributions of text-image pairs without optimization.

<details>
    <summary>Abstract</summary>
    Personalized text-to-image generation models enable users to create images that depict their individual possessions in diverse scenes, finding applications in various domains. To achieve the personalization capability, existing methods rely on finetuning a text-to-image foundation model on a user&#39;s custom dataset, which can be non-trivial for general users, resource-intensive, and time-consuming. Despite attempts to develop finetuning-free methods, their generation quality is much lower compared to their finetuning counterparts. In this paper, we propose Joint-Image Diffusion (\jedi), an effective technique for learning a finetuning-free personalization model. Our key idea is to learn the joint distribution of multiple related text-image pairs that share a common subject. To facilitate learning, we propose a scalable synthetic dataset generation technique. Once trained, our model enables fast and easy personalization at test time by simply using reference images as input during the sampling process. Our approach does not require any expensive optimization process or additional modules and can faithfully preserve the identity represented by any number of reference images. Experimental results show that our model achieves state-of-the-art generation quality, both quantitatively and qualitatively, significantly outperforming both the prior finetuning-based and finetuning-free personalization baselines.
</details>

<details>
    <summary>Key points</summary>
    * Learning joint distribution of related text-image pairs
    * Scalable synthetic dataset generation
    * Reference image input during sampling
</details>
</details>

---


<details>
<summary><b> Powerful and Flexible: Personalized Text-to-Image Generation via Reinforcement Learning</b></summary>

* **Authors:** Fanyue Wei, Wei Zeng, Zhenyang Li, Dawei Yin, Lixin Duan, Wen Li
* **arXiv ID:** 2407.06642
* **One-liner:** Applied reinforcement learning with DPG for improved personalized T2I generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2407.06642) | [[PDF]](https://arxiv.org/pdf/2407.06642)

> **Core Innovation**
> Enhanced structural consistency and visual fidelity through flexible objective incorporation.

<details>
    <summary>Abstract</summary>
    Personalized text-to-image models allow users to generate varied styles of images (specified with a sentence) for an object (specified with a set of reference images). While remarkable results have been achieved using diffusion-based generation models, the visual structure and details of the object are often unexpectedly changed during the diffusion process. One major reason is that these diffusion-based approaches typically adopt a simple reconstruction objective during training, which can hardly enforce appropriate structural consistency between the generated and the reference images. To this end, in this paper, we design a novel reinforcement learning framework by utilizing the deterministic policy gradient method for personalized text-to-image generation, with which various objectives, differential or even non-differential, can be easily incorporated to supervise the diffusion models to improve the quality of the generated images. Experimental results on personalized text-to-image generation benchmark datasets demonstrate that our proposed approach outperforms existing state-of-the-art methods by a large margin on visual fidelity while maintaining text-alignment. Our code is available at: \url{<a href="https://github.com/wfanyue/DPG-T2I-Personalization" rel="external noopener nofollow" class="link-external link-https">this https URL</a>}.
</details>

<details>
    <summary>Key points</summary>
    * Utilization of deterministic policy gradient method
    * Incorporation of various objectives
    * Improved text-alignment and visual fidelity
</details>
</details>

---


<details>
<summary><b> MARS: Mixture of Auto-Regressive Models for Fine-grained Text-to-image Synthesis</b></summary>

* **Authors:** Wanggui He, Siming Fu, Mushui Liu, Xierui Wang, Wenyi Xiao, Fangxun Shu, Yi Wang, Lei Zhang, Zhelun Yu, Haoyuan Li, Ziwei Huang, LeiLei Gan, Hao Jiang
* **arXiv ID:** 2407.07614
* **One-liner:** Introduced MARS for efficient bilingual T2I generation using LLMs.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2407.07614) | [[PDF]](https://arxiv.org/pdf/2407.07614)

> **Core Innovation**
> Combined linguistic and visual processing in a multi-stage training strategy for high efficiency.

<details>
    <summary>Abstract</summary>
    Auto-regressive models have made significant progress in the realm of language generation, yet they do not perform on par with diffusion models in the domain of image synthesis. In this work, we introduce MARS, a novel framework for T2I generation that incorporates a specially designed Semantic Vision-Language Integration Expert (SemVIE). This innovative component integrates pre-trained LLMs by independently processing linguistic and visual information, freezing the textual component while fine-tuning the visual component. This methodology preserves the NLP capabilities of LLMs while imbuing them with exceptional visual understanding. Building upon the powerful base of the pre-trained Qwen-7B, MARS stands out with its bilingual generative capabilities corresponding to both English and Chinese language prompts and the capacity for joint image and text generation. The flexibility of this framework lends itself to migration towards any-to-any task adaptability. Furthermore, MARS employs a multi-stage training strategy that first establishes robust image-text alignment through complementary bidirectional tasks and subsequently concentrates on refining the T2I generation process, significantly augmenting text-image synchrony and the granularity of image details. Notably, MARS requires only 9% of the GPU days needed by SD1.5, yet it achieves remarkable results across a variety of benchmarks, illustrating the training efficiency and the potential for swift deployment in various applications.
</details>

<details>
    <summary>Key points</summary>
    * Semantic Vision-Language Integration Expert (SemVIE)
    * Multi-stage training for image-text alignment
    * Bilingual capabilities and reduced GPU usage
</details>
</details>

---


<details>
<summary><b> Addressing Image Hallucination in Text-to-Image Generation through Factual Image Retrieval</b></summary>

* **Authors:** Youngsun Lim, Hyunjung Shim
* **arXiv ID:** 2407.10683
* **One-liner:** Addressed image hallucination in T2I models using factual image retrieval.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2407.10683) | [[PDF]](https://arxiv.org/pdf/2407.10683)

> **Core Innovation**
> Proposed a method to generate factually consistent images by leveraging external sources and editing tools.

<details>
    <summary>Abstract</summary>
    Text-to-image generation has shown remarkable progress with the emergence of diffusion models. However, these models often generate factually inconsistent images, failing to accurately reflect the factual information and common sense conveyed by the input text prompts. We refer to this issue as Image hallucination. Drawing from studies on hallucinations in language models, we classify this problem into three types and propose a methodology that uses factual images retrieved from external sources to generate realistic images. Depending on the nature of the hallucination, we employ off-the-shelf image editing tools, either InstructPix2Pix or IP-Adapter, to leverage factual information from the retrieved image. This approach enables the generation of images that accurately reflect the facts and common sense.
</details>

<details>
    <summary>Key points</summary>
    * Classification of hallucination types
    * Use of retrieved factual images with editing tools
    * Ensuring accuracy in fact and common sense reflection
</details>
</details>

---


<details>
<summary><b> Subject-driven Text-to-Image Generation via Preference-based Reinforcement Learning</b></summary>

* **Authors:** Yanting Miao, William Loh, Suraj Kothawade, Pascal Poupart, Abdullah Rashwan, Yeqing Li
* **arXiv ID:** 2407.12164
* **One-liner:** Proposed RPO with λ-Harmonic reward for efficient subject-driven T2I generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2407.12164) | [[PDF]](https://arxiv.org/pdf/2407.12164)

> **Core Innovation**
> Simplified training setup and improved regularization for better text-image alignment.

<details>
    <summary>Abstract</summary>
    Text-to-image generative models have recently attracted considerable interest, enabling the synthesis of high-quality images from textual prompts. However, these models often lack the capability to generate specific subjects from given reference images or to synthesize novel renditions under varying conditions. Methods like DreamBooth and Subject-driven Text-to-Image (SuTI) have made significant progress in this area. Yet, both approaches primarily focus on enhancing similarity to reference images and require expensive setups, often overlooking the need for efficient training and avoiding overfitting to the reference images. In this work, we present the $\lambda$-Harmonic reward function, which provides a reliable reward signal and enables early stopping for faster training and effective regularization. By combining the Bradley-Terry preference model, the $\lambda$-Harmonic reward function also provides preference labels for subject-driven generation tasks. We propose Reward Preference Optimization (RPO), which offers a simpler setup (requiring only $3\%$ of the negative samples used by DreamBooth) and fewer gradient steps for fine-tuning. Unlike most existing methods, our approach does not require training a text encoder or optimizing text embeddings and achieves text-image alignment by fine-tuning only the U-Net component. Empirically, $\lambda$-Harmonic proves to be a reliable approach for model selection in subject-driven generation tasks. Based on preference labels and early stopping validation from the $\lambda$-Harmonic reward function, our algorithm achieves a state-of-the-art CLIP-I score of 0.833 and a CLIP-T score of 0.314 on DreamBench.
</details>

<details>
    <summary>Key points</summary>
    * λ-Harmonic reward function for reliable signals
    * Reward Preference Optimization (RPO) with early stopping
    * Fine-tuning only U-Net component for efficiency
</details>
</details>

---


<details>
<summary><b> GreenStableYolo: Optimizing Inference Time and Image Quality of Text-to-Image Generation</b></summary>

* **Authors:** Jingzhi Gong, Sisi Li, Giordano d&#39;Aloisio, Zishuo Ding, Yulong Ye, William B. Langdon, Federica Sarro
* **arXiv ID:** 2407.14982
* **One-liner:** Improved text-to-image generation efficiency and quality with multi-objective optimization.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2407.14982) | [[PDF]](https://arxiv.org/pdf/2407.14982)

> **Core Innovation**
> GreenStableYolo uses NSGA-II and Yolo to optimize parameters and prompts for Stable Diffusion, reducing GPU inference time by 266% and increasing hypervolume by 526% with a slight trade-off in image quality.

<details>
    <summary>Abstract</summary>
    Tuning the parameters and prompts for improving AI-based text-to-image generation has remained a substantial yet unaddressed challenge. Hence we introduce GreenStableYolo, which improves the parameters and prompts for Stable Diffusion to both reduce GPU inference time and increase image generation quality using NSGA-II and Yolo.
<br>Our experiments show that despite a relatively slight trade-off (18%) in image quality compared to StableYolo (which only considers image quality), GreenStableYolo achieves a substantial reduction in inference time (266% less) and a 526% higher hypervolume, thereby advancing the state-of-the-art for text-to-image generation.
</details>

<details>
    <summary>Key points</summary>
    * Parameter and prompt tuning using NSGA-II
    * Integration with Yolo for optimization
    * Multi-objective optimization balancing time and quality
</details>
</details>

---


<details>
<summary><b> VersusDebias: Universal Zero-Shot Debiasing for Text-to-Image Models via SLM-Based Prompt Engineering and Generative Adversary</b></summary>

* **Authors:** Hanjun Luo, Ziye Deng, Haoyu Huang, Xuecheng Liu, Ruizhe Chen, Zuozhu Liu
* **arXiv ID:** 2407.19524
* **One-liner:** Introduced a universal debiasing framework for text-to-image models without model-specific training.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2407.19524) | [[PDF]](https://arxiv.org/pdf/2407.19524)

> **Core Innovation**
> VersusDebias employs an array generation module to handle hallucinations and debias multiple attributes, and an image generation module with a small language model for prompt modification, enabling zero-shot debiasing across gender, race, and age.

<details>
    <summary>Abstract</summary>
    With the rapid development of Text-to-Image (T2I) models, biases in human image generation against demographic social groups become a significant concern, impacting fairness and ethical standards in AI. Some researchers propose their methods to tackle with the issue. However, existing methods are designed for specific models with fixed prompts, limiting their adaptability to the fast-evolving models and diverse practical scenarios. Moreover, they neglect the impact of hallucinations, leading to discrepancies between expected and actual results. To address these issues, we introduce VersusDebias, a novel and universal debiasing framework for biases in arbitrary T2I models, consisting of an array generation (AG) module and an image generation (IG) module. The self-adaptive AG module generates specialized attribute arrays to post-process hallucinations and debias multiple attributes simultaneously. The IG module employs a small language model to modify prompts according to the arrays and drives the T2I model to generate debiased images, enabling zero-shot debiasing. Extensive experiments demonstrate VersusDebias&#39;s capability to debias any models across gender, race, and age simultaneously. In both zero-shot and few-shot scenarios, VersusDebias outperforms existing methods, showcasing its exceptional utility. Our work is accessible at <a href="https://github.com/VersusDebias/VersusDebias" rel="external noopener nofollow" class="link-external link-https">this https URL</a> to ensure reproducibility and facilitate further research.
</details>

<details>
    <summary>Key points</summary>
    * Array generation module for post-processing hallucinations
    * Image generation module with prompt modification
    * Zero-shot and few-shot debiasing capabilities
</details>
</details>

---


<details>
<summary><b> Reproducibility Study of &#34;ITI-GEN: Inclusive Text-to-Image Generation&#34;</b></summary>

* **Authors:** Daniel Gallo Fernández, Răzvan-Andrei Matisan, Alejandro Monroy Muñoz, Janusz Partyka
* **arXiv ID:** 2407.19996
* **One-liner:** Reproduced and improved upon ITI-GEN for inclusive text-to-image generation, addressing limitations with Hard Prompt Search.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2407.19996) | [[PDF]](https://arxiv.org/pdf/2407.19996)

> **Core Innovation**
> The study validates ITI-GEN's improvements in diversity and scalability but identifies issues with proxy features and attribute disentanglement; proposes combining ITI-GEN with Hard Prompt Search and negative prompting for better handling of negation.

<details>
    <summary>Abstract</summary>
    Text-to-image generative models often present issues regarding fairness with respect to certain sensitive attributes, such as gender or skin tone. This study aims to reproduce the results presented in &#34;ITI-GEN: Inclusive Text-to-Image Generation&#34; by Zhang et al. (2023a), which introduces a model to improve inclusiveness in these kinds of models. We show that most of the claims made by the authors about ITI-GEN hold: it improves the diversity and quality of generated images, it is scalable to different domains, it has plug-and-play capabilities, and it is efficient from a computational point of view. However, ITI-GEN sometimes uses undesired attributes as proxy features and it is unable to disentangle some pairs of (correlated) attributes such as gender and baldness. In addition, when the number of considered attributes increases, the training time grows exponentially and ITI-GEN struggles to generate inclusive images for all elements in the joint distribution. To solve these issues, we propose using Hard Prompt Search with negative prompting, a method that does not require training and that handles negation better than vanilla Hard Prompt Search. Nonetheless, Hard Prompt Search (with or without negative prompting) cannot be used for continuous attributes that are hard to express in natural language, an area where ITI-GEN excels as it is guided by images during training. Finally, we propose combining ITI-GEN and Hard Prompt Search with negative prompting.
</details>

<details>
    <summary>Key points</summary>
    * Reproduction of ITI-GEN claims
    * Identification of limitations in ITI-GEN
    * Proposal of Hard Prompt Search with negative prompting
    * Combination of methods for enhanced inclusiveness
</details>
</details>

---


<details>
<summary><b> VAR-CLIP: Text-to-Image Generator with Visual Auto-Regressive Modeling</b></summary>

* **Authors:** Qian Zhang, Xiangzi Dai, Ninghua Yang, Xiang An, Ziyong Feng, Xingyu Ren
* **arXiv ID:** 2408.01181
* **One-liner:** Developed a text-to-image model integrating visual auto-regressive techniques with CLIP for enhanced generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2408.01181) | [[PDF]](https://arxiv.org/pdf/2408.01181)

> **Core Innovation**
> VAR-CLIP uses next-scale prediction in auto-regressive transformers, encodes captions into text embeddings with CLIP, and trains on large datasets like ImageNet, achieving high fidelity and textual congruence in fantasy images.

<details>
    <summary>Abstract</summary>
    VAR is a new generation paradigm that employs &#39;next-scale prediction&#39; as opposed to &#39;next-token prediction&#39;. This innovative transformation enables auto-regressive (AR) transformers to rapidly learn visual distributions and achieve robust generalization. However, the original VAR model is constrained to class-conditioned synthesis, relying solely on textual captions for guidance. In this paper, we introduce VAR-CLIP, a novel text-to-image model that integrates Visual Auto-Regressive techniques with the capabilities of CLIP. The VAR-CLIP framework encodes captions into text embeddings, which are then utilized as textual conditions for image generation. To facilitate training on extensive datasets, such as ImageNet, we have constructed a substantial image-text dataset leveraging BLIP2. Furthermore, we delve into the significance of word positioning within CLIP for the purpose of caption guidance. Extensive experiments confirm VAR-CLIP&#39;s proficiency in generating fantasy images with high fidelity, textual congruence, and aesthetic excellence. Our project page are <a href="https://github.com/daixiangzi/VAR-CLIP" rel="external noopener nofollow" class="link-external link-https">this https URL</a>
</details>

<details>
    <summary>Key points</summary>
    * Integration of VAR with CLIP
    * Use of next-scale prediction
    * Training on large image-text datasets
    * Exploration of word positioning in CLIP
</details>
</details>

---


<details>
<summary><b> Lumina-mGPT: Illuminate Flexible Photorealistic Text-to-Image Generation with Multimodal Generative Pretraining</b></summary>

* **Authors:** Dongyang Liu, Shitian Zhao, Le Zhuo, Weifeng Lin, Yi Xin, Xinyue Li, Qi Qin, Yu Qiao, Hongsheng Li, Peng Gao
* **arXiv ID:** 2408.02657
* **One-liner:** Created a multimodal autoregressive model for versatile image generation and other vision-language tasks.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2408.02657) | [[PDF]](https://arxiv.org/pdf/2408.02657)

> **Core Innovation**
> Lumina-mGPT initializes from mGPT, uses flexible progressive supervised fine-tuning and unambiguous image representation to generate high-quality images of varying aspect ratios, and extends to omnipotent supervised fine-tuning for unified multimodal capabilities.

<details>
    <summary>Abstract</summary>
    We present Lumina-mGPT, a family of multimodal autoregressive models capable of various vision and language tasks, particularly excelling in generating flexible photorealistic images from text descriptions. By initializing from multimodal Generative PreTraining (mGPT), we demonstrate that decoder-only Autoregressive (AR) model can achieve image generation performance comparable to modern diffusion models with high efficiency through Flexible Progressive Supervised Fine-tuning (FP-SFT). Equipped with our proposed Unambiguous image Representation (UniRep), Lumina-mGPT can flexibly generate high-quality images of varying aspect ratios. Building on the strong image generation capabilities, we further explore Ominiponent Supervised Fine-tuning (Omni-SFT), an initial attempt to elevate Lumina-mGPT into a unified multi-modal generalist. The resulting model demonstrates versatile multimodal capabilities, including visual generation tasks like text-to-image/multiview generation and controllable generation, visual recognition tasks like segmentation and depth estimation, and vision-language tasks like multi-turn visual question answering, showing the rosy potential of the technical direction. Codes and checkpoints are available at <a href="https://github.com/Alpha-VLLM/Lumina-mGPT" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Initialization from multimodal Generative PreTraining
    * Flexible Progressive Supervised Fine-tuning
    * Unambiguous image Representation
    * Omnipotent Supervised Fine-tuning for generalist capabilities
</details>
</details>

---


<details>
<summary><b> FRAP: Faithful and Realistic Text-to-Image Generation with Adaptive Prompt Weighting</b></summary>

* **Authors:** Liyao Jiang, Negar Hassanpour, Mohammad Salameh, Mohan Sai Singamsetti, Fengyu Sun, Wei Lu, Di Niu
* **arXiv ID:** 2408.11706
* **One-liner:** Proposed a method to improve prompt-image alignment and authenticity in text-to-image generation without latent code optimization.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2408.11706) | [[PDF]](https://arxiv.org/pdf/2408.11706)

> **Core Innovation**
> FRAP adaptively adjusts per-token prompt weights using an online algorithm to minimize an objective function for object presence and binding, reducing latency and improving alignment and realism.

<details>
    <summary>Abstract</summary>
    Text-to-image (T2I) diffusion models have demonstrated impressive capabilities in generating high-quality images given a text prompt. However, ensuring the prompt-image alignment remains a considerable challenge, i.e., generating images that faithfully align with the prompt&#39;s semantics. Recent works attempt to improve the faithfulness by optimizing the latent code, which potentially could cause the latent code to go out-of-distribution and thus produce unrealistic images. In this paper, we propose FRAP, a simple, yet effective approach based on adaptively adjusting the per-token prompt weights to improve prompt-image alignment and authenticity of the generated images. We design an online algorithm to adaptively update each token&#39;s weight coefficient, which is achieved by minimizing a unified objective function that encourages object presence and the binding of object-modifier pairs. Through extensive evaluations, we show FRAP generates images with significantly higher prompt-image alignment to prompts from complex datasets, while having a lower average latency compared to recent latent code optimization methods, e.g., 4 seconds faster than D&amp;B on the COCO-Subject dataset. Furthermore, through visual comparisons and evaluation of the CLIP-IQA-Real metric, we show that FRAP not only improves prompt-image alignment but also generates more authentic images with realistic appearances. We also explore combining FRAP with prompt rewriting LLM to recover their degraded prompt-image alignment, where we observe improvements in both prompt-image alignment and image quality. We release the code at the following link: <a href="https://github.com/LiyaoJiang1998/FRAP/" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Adaptive per-token weight adjustment
    * Online algorithm for weight updates
    * Unified objective function for object presence and binding
    * Combination with prompt rewriting LLM
</details>
</details>

---


<details>
<summary><b> Rethinking Training for De-biasing Text-to-Image Generation: Unlocking the Potential of Stable Diffusion</b></summary>

* **Authors:** Eunji Kim, Siwon Kim, Minjun Park, Rahim Entezari, Sungroh Yoon
* **arXiv ID:** 2408.12692
* **One-liner:** Discovered and leveraged minority regions in Stable Diffusion for bias reduction without additional training.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2408.12692) | [[PDF]](https://arxiv.org/pdf/2408.12692)

> **Core Innovation**
> The method identifies clustered initial noises for minority attributes and uses weak guidance to steer random noise to these regions, preserving semantic integrity and core functionality while reducing bias.

<details>
    <summary>Abstract</summary>
    Recent advancements in text-to-image models, such as Stable Diffusion, show significant demographic biases. Existing de-biasing techniques rely heavily on additional training, which imposes high computational costs and risks of compromising core image generation functionality. This hinders them from being widely adopted to real-world applications. In this paper, we explore Stable Diffusion&#39;s overlooked potential to reduce bias without requiring additional training. Through our analysis, we uncover that initial noises associated with minority attributes form &#34;minority regions&#34; rather than scattered. We view these &#34;minority regions&#34; as opportunities in SD to reduce bias. To unlock the potential, we propose a novel de-biasing method called &#39;weak guidance,&#39; carefully designed to guide a random noise to the minority regions without compromising semantic integrity. Through analysis and experiments on various versions of SD, we demonstrate that our proposed approach effectively reduces bias without additional training, achieving both efficiency and preservation of core image generation functionality.
</details>

<details>
    <summary>Key points</summary>
    * Identification of minority regions in noise
    * Weak guidance technique for bias reduction
    * No additional training required
    * Preservation of image generation functionality
</details>
</details>

---


<details>
<summary><b> Taming Text-to-Image Synthesis for Novices: User-centric Prompt Generation via Multi-turn Guidance</b></summary>

* **Authors:** Yilun Liu, Minggui He, Feiyu Yao, Yuhe Ji, Shimin Tao, Jingzhou Du, Duan Li, Jian Gao, Li Zhang, Hao Yang, Boxing Chen, Osamu Yoshie
* **arXiv ID:** 2408.12910
* **One-liner:** Introduced a dialogue-based prompt generation model to enhance user-centricity for novice users in text-to-image synthesis.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2408.12910) | [[PDF]](https://arxiv.org/pdf/2408.12910)

> **Core Innovation**
> DialPrompt uses a multi-turn dialogue workflow to guide users in expressing preferences across 15 dimensions, trained on a curated dataset, improving user-centricity while maintaining image quality.

<details>
    <summary>Abstract</summary>
    The emergence of text-to-image synthesis (TIS) models has significantly influenced digital image creation by producing high-quality visuals from written descriptions. Yet these models are sensitive on textual prompts, posing a challenge for novice users who may not be familiar with TIS prompt writing. Existing solutions relieve this via automatic prompt expansion or generation from a user query. However, this single-turn manner suffers from limited user-centricity in terms of result interpretability and user interactivity. Thus, we propose DialPrompt, a dialogue-based TIS prompt generation model that emphasizes user experience for novice users. DialPrompt is designed to follow a multi-turn workflow, where in each round of dialogue the model guides user to express their preferences on possible optimization dimensions before generating the final TIS prompt. To achieve this, we mined 15 essential dimensions for high-quality prompts from advanced users and curated a multi-turn dataset. Through training on this dataset, DialPrompt improves user-centricity by allowing users to perceive and control the creation process of TIS prompts. Experiments indicate that DialPrompt improves significantly in user-centricity score compared with existing approaches while maintaining a competitive quality of synthesized images. In our user evaluation, DialPrompt is highly rated by 19 human reviewers (especially novices).
</details>

<details>
    <summary>Key points</summary>
    * Multi-turn dialogue workflow
    * Guidance on optimization dimensions
    * Training on curated multi-turn dataset
    * Focus on user-centricity and interpretability
</details>
</details>

---


<details>
<summary><b> Focus on Neighbors and Know the Whole: Towards Consistent Dense Multiview Text-to-Image Generator for 3D Creation</b></summary>

* **Authors:** Bonan Li, Zicheng Zhang, Xingyi Yang, Xinchao Wang
* **arXiv ID:** 2408.13149
* **One-liner:** Developed a consistent dense multiview text-to-image generator for high-quality 3D asset creation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2408.13149) | [[PDF]](https://arxiv.org/pdf/2408.13149)

> **Core Innovation**
> CoSER achieves neighbor-view consistency through dense interactions and motion path aggregation, and enhances cross-view consistency with spiral bidirectional scanning and weighted down-sampling, integrated with attention and state space models.

<details>
    <summary>Abstract</summary>
    Generating dense multiview images from text prompts is crucial for creating high-fidelity 3D assets. Nevertheless, existing methods struggle with space-view correspondences, resulting in sparse and low-quality outputs. In this paper, we introduce CoSER, a novel consistent dense Multiview Text-to-Image Generator for Text-to-3D, achieving both efficiency and quality by meticulously learning neighbor-view coherence and further alleviating ambiguity through the swift traversal of all views. For achieving neighbor-view consistency, each viewpoint densely interacts with adjacent viewpoints to perceive the global spatial structure, and aggregates information along motion paths explicitly defined by physical principles to refine details. To further enhance cross-view consistency and alleviate content drift, CoSER rapidly scan all views in spiral bidirectional manner to aware holistic information and then scores each point based on semantic material. Subsequently, we conduct weighted down-sampling along the spatial dimension based on scores, thereby facilitating prominent information fusion across all views with lightweight computation. Technically, the core module is built by integrating the attention mechanism with a selective state space model, exploiting the robust learning capabilities of the former and the low overhead of the latter. Extensive evaluation shows that CoSER is capable of producing dense, high-fidelity, content-consistent multiview images that can be flexibly integrated into various 3D generation models.
</details>

<details>
    <summary>Key points</summary>
    * Neighbor-view consistency via dense interactions
    * Spiral bidirectional scanning for holistic awareness
    * Weighted down-sampling for information fusion
    * Integration of attention and state space models
</details>
</details>

---


<details>
<summary><b> CSGO: Content-Style Composition in Text-to-Image Generation</b></summary>

* **Authors:** Peng Xing, Haofan Wang, Yanpeng Sun, Qixun Wang, Xu Bai, Hao Ai, Renyuan Huang, Zechao Li
* **arXiv ID:** 2408.16766
* **One-liner:** Constructed a large-scale style transfer dataset and proposed an end-to-end model for enhanced style control.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2408.16766) | [[PDF]](https://arxiv.org/pdf/2408.16766)

> **Core Innovation**
> IMAGStyle dataset with 210k triplets enables training of CSGO, which decouples content and style features for unified style transfer, text-driven synthesis, and text editing-driven synthesis.

<details>
    <summary>Abstract</summary>
    The diffusion model has shown exceptional capabilities in controlled image generation, which has further fueled interest in image style transfer. Existing works mainly focus on training free-based methods (e.g., image inversion) due to the scarcity of specific data. In this study, we present a data construction pipeline for content-style-stylized image triplets that generates and automatically cleanses stylized data triplets. Based on this pipeline, we construct a dataset IMAGStyle, the first large-scale style transfer dataset containing 210k image triplets, available for the community to explore and research. Equipped with IMAGStyle, we propose CSGO, a style transfer model based on end-to-end training, which explicitly decouples content and style features employing independent feature injection. The unified CSGO implements image-driven style transfer, text-driven stylized synthesis, and text editing-driven stylized synthesis. Extensive experiments demonstrate the effectiveness of our approach in enhancing style control capabilities in image generation. Additional visualization and access to the source code can be located on the project page: \url{<a href="https://csgo-gen.github.io/" rel="external noopener nofollow" class="link-external link-https">this https URL</a>}.
</details>

<details>
    <summary>Key points</summary>
    * Data construction pipeline for style transfer triplets
    * IMAGStyle dataset creation
    * CSGO model with feature decoupling
    * Unified implementation for multiple style transfer tasks
</details>
</details>

---


<details>
<summary><b> Text-to-Image Generation Via Energy-Based CLIP</b></summary>

* **Authors:** Roy Ganz, Michael Elad
* **arXiv ID:** 2408.17046
* **One-liner:** Scaled Joint Energy Models to multimodal vision-language domain using CLIP.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2408.17046) | [[PDF]](https://arxiv.org/pdf/2408.17046)

> **Core Innovation**
> Extended JEMs with CLIP by integrating generative and discriminative objectives, enabling realistic image generation from text and competitive performance on benchmarks.

<details>
    <summary>Abstract</summary>
    Joint Energy Models (JEMs), while drawing significant research attention, have not been successfully scaled to real-world, high-resolution datasets. We present CLIP-JEM, a novel approach extending JEMs to the multimodal vision-language domain using CLIP, integrating both generative and discriminative objectives. For the generative one, we introduce an image-text joint-energy function based on Cosine similarity in the CLIP space, training CLIP to assign low energy to real image-caption pairs and high energy otherwise. For the discriminative one, we employ contrastive adversarial loss, extending the adversarial training objective to the multimodal domain. CLIP-JEM not only generates realistic images from text but also achieves competitive results on the compositionality benchmark, outperforming leading methods with fewer parameters. Additionally, we demonstrate the superior guidance capability of CLIP-JEM by enhancing CLIP-based generative frameworks and converting unconditional diffusion models to text-based ones. Lastly, we show that our model can serve as a more robust evaluation metric for text-to-image generative tasks than CLIP.
</details>

<details>
    <summary>Key points</summary>
    * Introduced image-text joint-energy function based on Cosine similarity in CLIP space
    * Employed contrastive adversarial loss for multimodal adversarial training
    * Demonstrated superior guidance for enhancing CLIP-based frameworks and converting diffusion models
</details>
</details>

---


<details>
<summary><b> SPDiffusion: Semantic Protection Diffusion Models for Multi-concept Text-to-image Generation</b></summary>

* **Authors:** Yang Zhang, Rui Zhang, Xuecheng Nie, Haochen Li, Jikun Chen, Yifan Hao, Xin Zhang, Luoqi Liu, Ling Li
* **arXiv ID:** 2409.01327
* **One-liner:** Addressed semantic entanglement in multi-concept text-to-image generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2409.01327) | [[PDF]](https://arxiv.org/pdf/2409.01327)

> **Core Innovation**
> Proposed SPDiffusion with concept region extraction and protection mechanisms to resolve concept and attribute entanglement, achieving state-of-the-art results.

<details>
    <summary>Abstract</summary>
    Recent text-to-image models have achieved impressive results in generating high-quality images. However, when tasked with multi-concept generation creating images that contain multiple characters or objects, existing methods often suffer from semantic entanglement, including concept entanglement and improper attribute binding, leading to significant text-image inconsistency. We identify that semantic entanglement arises when certain regions of the latent features attend to incorrect concept and attribute tokens. In this work, we propose the Semantic Protection Diffusion Model (SPDiffusion) to address both concept entanglement and improper attribute binding using only a text prompt as input. The SPDiffusion framework introduces a novel concept region extraction method SP-Extraction to resolve region entanglement in cross-attention, along with SP-Attn, which protects concept regions from the influence of irrelevant attributes and concepts. To evaluate our method, we test it on existing benchmarks, where SPDiffusion achieves state-of-the-art results, demonstrating its effectiveness.
</details>

<details>
    <summary>Key points</summary>
    * Introduced SP-Extraction for concept region extraction
    * Developed SP-Attn to protect concept regions from irrelevant influences
    * Tested on benchmarks for text-image consistency
</details>
</details>

---


<details>
<summary><b> Qihoo-T2X: An Efficient Proxy-Tokenized Diffusion Transformer for Text-to-Any-Task</b></summary>

* **Authors:** Jing Wang, Ao Ma, Jiasong Feng, Dawei Leng, Yuhui Yin, Xiaodan Liang
* **arXiv ID:** 2409.04005
* **One-liner:** Reduced computational complexity in diffusion transformers with proxy tokens.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2409.04005) | [[PDF]](https://arxiv.org/pdf/2409.04005)

> **Core Innovation**
> Designed PT-DiT using sparse representative token attention to capture global semantics efficiently, leading to significant computation reduction in image and video generation.

<details>
    <summary>Abstract</summary>
    The global self-attention mechanism in diffusion transformers involves redundant computation due to the sparse and redundant nature of visual information, and the attention map of tokens within a spatial window shows significant similarity. To address this redundancy, we propose the Proxy-Tokenized Diffusion Transformer (PT-DiT), which employs sparse representative token attention (where the number of representative tokens is much smaller than the total number of tokens) to model global visual information efficiently. Specifically, within each transformer block, we compute an averaging token from each spatial-temporal window to serve as a proxy token for that region. The global semantics are captured through the self-attention of these proxy tokens and then injected into all latent tokens via cross-attention. Simultaneously, we introduce window and shift window attention to address the limitations in detail modeling caused by the sparse attention mechanism. Building on the well-designed PT-DiT, we further develop the Qihoo-T2X family, which includes a variety of models for T2I, T2V, and T2MV tasks. Experimental results show that PT-DiT achieves competitive performance while reducing the computational complexity in both image and video generation tasks (e.g., a 49% reduction compared to DiT and a 34% reduction compared to PixArt-$\alpha$). The visual exhibition and source code of Qihoo-T2X is available at <a href="https://360cvgroup.github.io/Qihoo-T2X/" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Employed proxy tokens from spatial-temporal windows for global attention
    * Introduced window and shift window attention for detail modeling
    * Built Qihoo-T2X family for various generation tasks
</details>
</details>

---


<details>
<summary><b> IFAdapter: Instance Feature Control for Grounded Text-to-Image Generation</b></summary>

* **Authors:** Yinwei Wu, Xianpan Zhou, Bing Ma, Xuefeng Su, Kai Ma, Xinchao Wang
* **arXiv ID:** 2409.08240
* **One-liner:** Enhanced instance feature generation with accurate positioning and fidelity.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2409.08240) | [[PDF]](https://arxiv.org/pdf/2409.08240)

> **Core Innovation**
> Introduced IFG task and IFAdapter to align instance-level features with spatial locations, outperforming other models in quantitative and qualitative evaluations.

<details>
    <summary>Abstract</summary>
    While Text-to-Image (T2I) diffusion models excel at generating visually appealing images of individual instances, they struggle to accurately position and control the features generation of multiple instances. The Layout-to-Image (L2I) task was introduced to address the positioning challenges by incorporating bounding boxes as spatial control signals, but it still falls short in generating precise instance features. In response, we propose the Instance Feature Generation (IFG) task, which aims to ensure both positional accuracy and feature fidelity in generated instances. To address the IFG task, we introduce the Instance Feature Adapter (IFAdapter). The IFAdapter enhances feature depiction by incorporating additional appearance tokens and utilizing an Instance Semantic Map to align instance-level features with spatial locations. The IFAdapter guides the diffusion process as a plug-and-play module, making it adaptable to various community models. For evaluation, we contribute an IFG benchmark and develop a verification pipeline to objectively compare models&#39; abilities to generate instances with accurate positioning and features. Experimental results demonstrate that IFAdapter outperforms other models in both quantitative and qualitative evaluations.
</details>

<details>
    <summary>Key points</summary>
    * Proposed Instance Feature Adapter with appearance tokens
    * Utilized Instance Semantic Map for feature alignment
    * Developed IFG benchmark and verification pipeline
</details>
</details>

---


<details>
<summary><b> Generalizing Alignment Paradigm of Text-to-Image Generation with Preferences through $f$-divergence Minimization</b></summary>

* **Authors:** Haoyuan Sun, Bo Xia, Yongzhe Chang, Xueqian Wang
* **arXiv ID:** 2409.09774
* **One-liner:** Extended alignment paradigm to f-divergence for better trade-off in text-to-image models.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2409.09774) | [[PDF]](https://arxiv.org/pdf/2409.09774)

> **Core Innovation**
> Generalized DPO to f-divergence, analyzing gradient fields and finding Jensen-Shannon divergence optimal for alignment and diversity.

<details>
    <summary>Abstract</summary>
    Direct Preference Optimization (DPO) has recently expanded its successful application from aligning large language models (LLMs) to aligning text-to-image models with human preferences, which has generated considerable interest within the community. However, we have observed that these approaches rely solely on minimizing the reverse Kullback-Leibler divergence during alignment process between the fine-tuned model and the reference model, neglecting the incorporation of other divergence constraints. In this study, we focus on extending reverse Kullback-Leibler divergence in the alignment paradigm of text-to-image models to $f$-divergence, which aims to garner better alignment performance as well as good generation diversity. We provide the generalized formula of the alignment paradigm under the $f$-divergence condition and thoroughly analyze the impact of different divergence constraints on alignment process from the perspective of gradient fields. We conduct comprehensive evaluation on image-text alignment performance, human value alignment performance and generation diversity performance under different divergence constraints, and the results indicate that alignment based on Jensen-Shannon divergence achieves the best trade-off among them. The option of divergence employed for aligning text-to-image models significantly impacts the trade-off between alignment performance (especially human value alignment) and generation diversity, which highlights the necessity of selecting an appropriate divergence for practical applications.
</details>

<details>
    <summary>Key points</summary>
    * Provided generalized formula under f-divergence condition
    * Analyzed impact of different divergences on alignment process
    * Conducted evaluations on alignment performance and diversity
</details>
</details>

---


<details>
<summary><b> Evaluating Image Hallucination in Text-to-Image Generation with Question-Answering</b></summary>

* **Authors:** Youngsun Lim, Hojun Choi, Hyunjung Shim
* **arXiv ID:** 2409.12784
* **One-liner:** Introduced automated metric for evaluating factual accuracy in generated images.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2409.12784) | [[PDF]](https://arxiv.org/pdf/2409.12784)

> **Core Innovation**
> Developed I-HallA using VQA to measure image hallucination, with a benchmark dataset showing strong correlation with human judgments.

<details>
    <summary>Abstract</summary>
    Despite the impressive success of text-to-image (TTI) generation models, existing studies overlook the issue of whether these models accurately convey factual information. In this paper, we focus on the problem of image hallucination, where images created by generation models fail to faithfully depict factual content. To address this, we introduce I-HallA (Image Hallucination evaluation with Question Answering), a novel automated evaluation metric that measures the factuality of generated images through visual question answering (VQA). We also introduce I-HallA v1.0, a curated benchmark dataset for this purpose. As part of this process, we develop a pipeline that generates high-quality question-answer pairs using multiple GPT-4 Omni-based agents, with human judgments to ensure accuracy. Our evaluation protocols measure image hallucination by testing if images from existing TTI models can correctly respond to these questions. The I-HallA v1.0 dataset comprises 1.2K diverse image-text pairs across nine categories with 1,000 rigorously curated questions covering various compositional challenges. We evaluate five TTI models using I-HallA and reveal that these state-of-the-art models often fail to accurately convey factual information. Moreover, we validate the reliability of our metric by demonstrating a strong Spearman correlation ($\rho$=0.95) with human judgments. We believe our benchmark dataset and metric can serve as a foundation for developing factually accurate TTI generation models. Additional resources can be found on our project page: <a href="https://sgt-lim.github.io/I-HallA/" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Created I-HallA v1.0 dataset with curated question-answer pairs
    * Used GPT-4 Omni-based agents for pipeline development
    * Evaluated multiple TTI models for factuality
</details>
</details>

---


<details>
<summary><b> Text Image Generation for Low-Resource Languages with Dual Translation Learning</b></summary>

* **Authors:** Chihiro Noguchi, Shun Fukuda, Shoichiro Mihara, Masao Yamanaka
* **arXiv ID:** 2409.17747
* **One-liner:** Improved scene text recognition for low-resource languages via synthetic image generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2409.17747) | [[PDF]](https://arxiv.org/pdf/2409.17747)

> **Core Innovation**
> Proposed diffusion model conditioned on binary states to generate text images, enhancing recognition model performance.

<details>
    <summary>Abstract</summary>
    Scene text recognition in low-resource languages frequently faces challenges due to the limited availability of training datasets derived from real-world scenes. This study proposes a novel approach that generates text images in low-resource languages by emulating the style of real text images from high-resource languages. Our approach utilizes a diffusion model that is conditioned on binary states: ``synthetic&#39;&#39; and ``real.&#39;&#39; The training of this model involves dual translation tasks, where it transforms plain text images into either synthetic or real text images, based on the binary states. This approach not only effectively differentiates between the two domains but also facilitates the model&#39;s explicit recognition of characters in the target language. Furthermore, to enhance the accuracy and variety of generated text images, we introduce two guidance techniques: Fidelity-Diversity Balancing Guidance and Fidelity Enhancement Guidance. Our experimental results demonstrate that the text images generated by our proposed framework can significantly improve the performance of scene text recognition models for low-resource languages.
</details>

<details>
    <summary>Key points</summary>
    * Utilized dual translation tasks for domain differentiation
    * Introduced Fidelity-Diversity Balancing and Enhancement Guidance
    * Generated text images emulating real styles from high-resource languages
</details>
</details>

---


<details>
<summary><b> MCGM: Mask Conditional Text-to-Image Generative Model</b></summary>

* **Authors:** Rami Skaik, Leonardo Rossi, Tomaso Fontanini, Andrea Prati
* **arXiv ID:** 2410.00483
* **One-liner:** Enabled pose-specific image generation with mask conditioning.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2410.00483) | [[PDF]](https://arxiv.org/pdf/2410.00483)

> **Core Innovation**
> Developed MCGM by incorporating mask embedding injection into diffusion models, allowing flexible control over subject poses from a single image.

<details>
    <summary>Abstract</summary>
    Recent advancements in generative models have revolutionized the field of artificial intelligence, enabling the creation of highly-realistic and detailed images. In this study, we propose a novel Mask Conditional Text-to-Image Generative Model (MCGM) that leverages the power of conditional diffusion models to generate pictures with specific poses. Our model builds upon the success of the Break-a-scene [1] model in generating new scenes using a single image with multiple subjects and incorporates a mask embedding injection that allows the conditioning of the generation process. By introducing this additional level of control, MCGM offers a flexible and intuitive approach for generating specific poses for one or more subjects learned from a single image, empowering users to influence the output based on their requirements. Through extensive experimentation and evaluation, we demonstrate the effectiveness of our proposed model in generating high-quality images that meet predefined mask conditions and improving the current Break-a-scene generative model.
</details>

<details>
    <summary>Key points</summary>
    * Built upon Break-a-scene model for scene generation
    * Introduced mask embedding injection for conditioning
    * Demonstrated effectiveness in high-quality image generation
</details>
</details>

---


<details>
<summary><b> Accelerating Auto-regressive Text-to-Image Generation with Training-free Speculative Jacobi Decoding</b></summary>

* **Authors:** Yao Teng, Han Shi, Xian Liu, Xuefei Ning, Guohao Dai, Yu Wang, Zhenguo Li, Xihui Liu
* **arXiv ID:** 2410.01699
* **One-liner:** Accelerated auto-regressive text-to-image generation with probabilistic parallel decoding.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2410.01699) | [[PDF]](https://arxiv.org/pdf/2410.01699)

> **Core Innovation**
> Proposed SJD algorithm using probabilistic convergence criterion to reduce inference steps while maintaining diversity and quality.

<details>
    <summary>Abstract</summary>
    The current large auto-regressive models can generate high-quality, high-resolution images, but these models require hundreds or even thousands of steps of next-token prediction during inference, resulting in substantial time consumption. In existing studies, Jacobi decoding, an iterative parallel decoding algorithm, has been used to accelerate the auto-regressive generation and can be executed without training. However, the Jacobi decoding relies on a deterministic criterion to determine the convergence of iterations. Thus, it works for greedy decoding but is incompatible with sampling-based decoding which is crucial for visual quality and diversity in the current auto-regressive text-to-image generation. In this paper, we propose a training-free probabilistic parallel decoding algorithm, Speculative Jacobi Decoding (SJD), to accelerate auto-regressive text-to-image generation. By introducing a probabilistic convergence criterion, our SJD accelerates the inference of auto-regressive text-to-image generation while maintaining the randomness in sampling-based token decoding and allowing the model to generate diverse images. Specifically, SJD facilitates the model to predict multiple tokens at each step and accepts tokens based on the probabilistic criterion, enabling the model to generate images with fewer steps than the conventional next-token-prediction paradigm. We also investigate the token initialization strategies that leverage the spatial locality of visual data to further improve the acceleration ratio under specific scenarios. We conduct experiments for our proposed SJD on multiple auto-regressive text-to-image generation models, showing the effectiveness of model acceleration without sacrificing the visual quality. The code of our work is available here: <a href="https://github.com/tyshiwo1/Accelerating-T2I-AR-with-SJD/" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Introduced probabilistic criterion for token acceptance
    * Investigated token initialization strategies for acceleration
    * Conducted experiments on multiple models without training
</details>
</details>

---


<details>
<summary><b> ComfyGen: Prompt-Adaptive Workflows for Text-to-Image Generation</b></summary>

* **Authors:** Rinon Gal, Adi Haviv, Yuval Alaluf, Amit H. Bermano, Daniel Cohen-Or, Gal Chechik
* **arXiv ID:** 2410.01731
* **One-liner:** Automated prompt-adaptive workflow generation for text-to-image tasks.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2410.01731) | [[PDF]](https://arxiv.org/pdf/2410.01731)

> **Core Innovation**
> Introduced LLM-based methods to tailor workflows to user prompts, improving image quality over monolithic models.

<details>
    <summary>Abstract</summary>
    The practical use of text-to-image generation has evolved from simple, monolithic models to complex workflows that combine multiple specialized components. While workflow-based approaches can lead to improved image quality, crafting effective workflows requires significant expertise, owing to the large number of available components, their complex inter-dependence, and their dependence on the generation prompt. Here, we introduce the novel task of prompt-adaptive workflow generation, where the goal is to automatically tailor a workflow to each user prompt. We propose two LLM-based approaches to tackle this task: a tuning-based method that learns from user-preference data, and a training-free method that uses the LLM to select existing flows. Both approaches lead to improved image quality when compared to monolithic models or generic, prompt-independent workflows. Our work shows that prompt-dependent flow prediction offers a new pathway to improving text-to-image generation quality, complementing existing research directions in the field.
</details>

<details>
    <summary>Key points</summary>
    * Proposed tuning-based and training-free LLM approaches
    * Addressed component selection and interdependence
    * Showed effectiveness in prompt-dependent flow prediction
</details>
</details>

---


<details>
<summary><b> A Spark of Vision-Language Intelligence: 2-Dimensional Autoregressive Transformer for Efficient Finegrained Image Generation</b></summary>

* **Authors:** Liang Chen, Sinan Tan, Zefan Cai, Weichu Xie, Haozhe Zhao, Yichi Zhang, Junyang Lin, Jinze Bai, Tianyu Liu, Baobao Chang
* **arXiv ID:** 2410.01912
* **One-liner:** Introduced the DnD-Transformer to address information loss in VQ autoregressive image generation by adding a model depth direction.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2410.01912) | [[PDF]](https://arxiv.org/pdf/2410.01912)

> **Core Innovation**
> Enhanced autoregressive image generation with a 2D autoregression approach, improving quality and enabling self-supervised generation of images with text and graphics.

<details>
    <summary>Abstract</summary>
    This work tackles the information loss bottleneck of vector-quantization (VQ) autoregressive image generation by introducing a novel model architecture called the 2-Dimensional Autoregression (DnD) Transformer. The DnD-Transformer predicts more codes for an image by introducing a new autoregression direction, \textit{model depth}, along with the sequence length direction. Compared to traditional 1D autoregression and previous work utilizing similar 2D image decomposition such as RQ-Transformer, the DnD-Transformer is an end-to-end model that can generate higher quality images with the same backbone model size and sequence length, opening a new optimization perspective for autoregressive image generation. Furthermore, our experiments reveal that the DnD-Transformer&#39;s potential extends beyond generating natural images. It can even generate images with rich text and graphical elements in a self-supervised manner, demonstrating an understanding of these combined modalities. This has not been previously demonstrated for popular vision generative models such as diffusion models, showing a spark of vision-language intelligence when trained solely on images. Code, datasets and models are open at <a href="https://github.com/chenllliang/DnD-Transformer" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Introduces a 2-dimensional autoregression with model depth direction
    * End-to-end model for higher quality images
    * Demonstrates vision-language intelligence from image-only training
</details>
</details>

---


<details>
<summary><b> EvolveDirector: Approaching Advanced Text-to-Image Generation with Large Vision-Language Models</b></summary>

* **Authors:** Rui Zhao, Hangjie Yuan, Yujie Wei, Shiwei Zhang, Yuchao Gu, Lingmin Ran, Xiang Wang, Zhangjie Wu, Junhao Zhang, Yingya Zhang, Mike Zheng Shou
* **arXiv ID:** 2410.07133
* **One-liner:** Developed EvolveDirector to train text-to-image models using public APIs and VLM guidance, reducing data needs and costs.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2410.07133) | [[PDF]](https://arxiv.org/pdf/2410.07133)

> **Core Innovation**
> Proposed a framework that uses API-generated data and VLM feedback to evolve a base model, achieving superior performance with less data.

<details>
    <summary>Abstract</summary>
    Recent advancements in generation models have showcased remarkable capabilities in generating fantastic content. However, most of them are trained on proprietary high-quality data, and some models withhold their parameters and only provide accessible application programming interfaces (APIs), limiting their benefits for downstream tasks. To explore the feasibility of training a text-to-image generation model comparable to advanced models using publicly available resources, we introduce EvolveDirector. This framework interacts with advanced models through their public APIs to obtain text-image data pairs to train a base model. Our experiments with extensive data indicate that the model trained on generated data of the advanced model can approximate its generation capability. However, it requires large-scale samples of 10 million or more. This incurs significant expenses in time, computational resources, and especially the costs associated with calling fee-based APIs. To address this problem, we leverage pre-trained large vision-language models (VLMs) to guide the evolution of the base model. VLM continuously evaluates the base model during training and dynamically updates and refines the training dataset by the discrimination, expansion, deletion, and mutation operations. Experimental results show that this paradigm significantly reduces the required data volume. Furthermore, when approaching multiple advanced models, EvolveDirector can select the best samples generated by them to learn powerful and balanced abilities. The final trained model Edgen is demonstrated to outperform these advanced models. The code and model weights are available at <a href="https://github.com/showlab/EvolveDirector" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Interacts with advanced models via APIs for data collection
    * Uses VLM for dynamic dataset refinement
    * Reduces required data volume to under 10 million
</details>
</details>

---


<details>
<summary><b> IterComp: Iterative Composition-Aware Feedback Learning from Model Gallery for Text-to-Image Generation</b></summary>

* **Authors:** Xinchen Zhang, Ling Yang, Guohao Li, Yaqi Cai, Jiake Xie, Yong Tang, Yujiu Yang, Mengdi Wang, Bin Cui
* **arXiv ID:** 2410.07171
* **One-liner:** Introduced IterComp to enhance compositional text-to-image generation by aggregating model preferences and iterative feedback learning.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2410.07171) | [[PDF]](https://arxiv.org/pdf/2410.07171)

> **Core Innovation**
> Improved compositional capabilities in diffusion models through a framework that trains reward models and refines generation iteratively.

<details>
    <summary>Abstract</summary>
    Advanced diffusion models like RPG, Stable Diffusion 3 and FLUX have made notable strides in compositional text-to-image generation. However, these methods typically exhibit distinct strengths for compositional generation, with some excelling in handling attribute binding and others in spatial relationships. This disparity highlights the need for an approach that can leverage the complementary strengths of various models to comprehensively improve the composition capability. To this end, we introduce IterComp, a novel framework that aggregates composition-aware model preferences from multiple models and employs an iterative feedback learning approach to enhance compositional generation. Specifically, we curate a gallery of six powerful open-source diffusion models and evaluate their three key compositional metrics: attribute binding, spatial relationships, and non-spatial relationships. Based on these metrics, we develop a composition-aware model preference dataset comprising numerous image-rank pairs to train composition-aware reward models. Then, we propose an iterative feedback learning method to enhance compositionality in a closed-loop manner, enabling the progressive self-refinement of both the base diffusion model and reward models over multiple iterations. Theoretical proof demonstrates the effectiveness and extensive experiments show our significant superiority over previous SOTA methods (e.g., Omost and FLUX), particularly in multi-category object composition and complex semantic alignment. IterComp opens new research avenues in reward feedback learning for diffusion models and compositional generation. Code: <a href="https://github.com/YangLing0818/IterComp" rel="external noopener nofollow" class="link-external link-https">this https URL</a>
</details>

<details>
    <summary>Key points</summary>
    * Curates a gallery of diffusion models for evaluation
    * Develops composition-aware reward models
    * Employs iterative feedback learning for self-refinement
</details>
</details>

---


<details>
<summary><b> Minority-Focused Text-to-Image Generation via Prompt Optimization</b></summary>

* **Authors:** Soobin Um, Jong Chul Ye
* **arXiv ID:** 2410.07838
* **One-liner:** Presented a framework to generate minority samples in T2I diffusion models via prompt optimization and likelihood objectives.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2410.07838) | [[PDF]](https://arxiv.org/pdf/2410.07838)

> **Core Innovation**
> Addressed the high-density focus of T2I models by optimizing prompts to encourage minority feature generation.

<details>
    <summary>Abstract</summary>
    We investigate the generation of minority samples using pretrained text-to-image (T2I) latent diffusion models. Minority instances, in the context of T2I generation, can be defined as ones living on low-density regions of text-conditional data distributions. They are valuable for various applications of modern T2I generators, such as data augmentation and creative AI. Unfortunately, existing pretrained T2I diffusion models primarily focus on high-density regions, largely due to the influence of guided samplers (like CFG) that are essential for high-quality generation. To address this, we present a novel framework to counter the high-density-focus of T2I diffusion models. Specifically, we first develop an online prompt optimization framework that encourages emergence of desired properties during inference while preserving semantic contents of user-provided prompts. We subsequently tailor this generic prompt optimizer into a specialized solver that promotes generation of minority features by incorporating a carefully-crafted likelihood objective. Extensive experiments conducted across various types of T2I models demonstrate that our approach significantly enhances the capability to produce high-quality minority instances compared to existing samplers. Code is available at <a href="https://github.com/soobin-um/MinorityPrompt" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Develops online prompt optimization framework
    * Incorporates likelihood objective for minority features
    * Enhances generation of low-density instances
</details>
</details>

---


<details>
<summary><b> DART: Denoising Autoregressive Transformer for Scalable Text-to-Image Generation</b></summary>

* **Authors:** Jiatao Gu, Yuyang Wang, Yizhe Zhang, Qihang Zhang, Dinghuai Zhang, Navdeep Jaitly, Josh Susskind, Shuangfei Zhai
* **arXiv ID:** 2410.08159
* **One-liner:** Proposed DART, a transformer-based model unifying autoregressive and diffusion approaches in a non-Markovian framework.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2410.08159) | [[PDF]](https://arxiv.org/pdf/2410.08159)

> **Core Innovation**
> Unified AR and diffusion for image generation without quantization, enabling efficient training with text and image data.

<details>
    <summary>Abstract</summary>
    Diffusion models have become the dominant approach for visual generation. They are trained by denoising a Markovian process which gradually adds noise to the input. We argue that the Markovian property limits the model&#39;s ability to fully utilize the generation trajectory, leading to inefficiencies during training and inference. In this paper, we propose DART, a transformer-based model that unifies autoregressive (AR) and diffusion within a non-Markovian framework. DART iteratively denoises image patches spatially and spectrally using an AR model that has the same architecture as standard language models. DART does not rely on image quantization, which enables more effective image modeling while maintaining flexibility. Furthermore, DART seamlessly trains with both text and image data in a unified model. Our approach demonstrates competitive performance on class-conditioned and text-to-image generation tasks, offering a scalable, efficient alternative to traditional diffusion models. Through this unified framework, DART sets a new benchmark for scalable, high-quality image synthesis.
</details>

<details>
    <summary>Key points</summary>
    * Uses non-Markovian framework for denoising
    * Iteratively denoises patches spatially and spectrally
    * Seamlessly trains with multimodal data
</details>
</details>

---


<details>
<summary><b> Meissonic: Revitalizing Masked Generative Transformers for Efficient High-Resolution Text-to-Image Synthesis</b></summary>

* **Authors:** Jinbin Bai, Tian Ye, Wei Chow, Enxin Song, Xiangtai Li, Zhen Dong, Lei Zhu, Shuicheng Yan
* **arXiv ID:** 2410.08261
* **One-liner:** Elevated MIM text-to-image generation to SOTA levels with Meissonic, incorporating architectural innovations and high-quality data.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2410.08261) | [[PDF]](https://arxiv.org/pdf/2410.08261)

> **Core Innovation**
> Improved non-autoregressive MIM to match or exceed diffusion models in quality and resolution through various optimizations.

<details>
    <summary>Abstract</summary>
    We present Meissonic, which elevates non-autoregressive masked image modeling (MIM) text-to-image to a level comparable with state-of-the-art diffusion models like SDXL. By incorporating a comprehensive suite of architectural innovations, advanced positional encoding strategies, and optimized sampling conditions, Meissonic substantially improves MIM&#39;s performance and efficiency. Additionally, we leverage high-quality training data, integrate micro-conditions informed by human preference scores, and employ feature compression layers to further enhance image fidelity and resolution. Our model not only matches but often exceeds the performance of existing models like SDXL in generating high-quality, high-resolution images. Extensive experiments validate Meissonic&#39;s capabilities, demonstrating its potential as a new standard in text-to-image synthesis. We release a model checkpoint capable of producing $1024 \times 1024$ resolution images.
</details>

<details>
    <summary>Key points</summary>
    * Incorporates architectural and positional encoding innovations
    * Uses micro-conditions from human preferences
    * Enhances image fidelity and resolution up to 1024x1024
</details>
</details>

---


<details>
<summary><b> Text-To-Image with Generative Adversarial Networks</b></summary>

* **Authors:** Mehrshad Momen-Tayefeh
* **arXiv ID:** 2410.08608
* **One-liner:** Conducted a comparison of GAN-based text-to-image methods, identifying the best model based on accuracy metrics.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2410.08608) | [[PDF]](https://arxiv.org/pdf/2410.08608)

> **Core Innovation**
> Analyzed five GAN architectures for text-to-image synthesis, focusing on resolution and accuracy comparisons.

<details>
    <summary>Abstract</summary>
    Generating realistic images from human texts is one of the most challenging problems in the field of computer vision (CV). The meaning of descriptions given can be roughly reflected by existing text-to-image approaches. In this paper, our main purpose is to propose a brief comparison between five different methods base on the Generative Adversarial Networks (GAN) to make image from the text. In addition, each model architectures synthesis images with different resolution. Furthermore, the best and worst obtained resolutions is 64*64, 256*256 respectively. However, we checked and compared some metrics that introduce the accuracy of each model. Also, by doing this study, we found out the best model for this problem by comparing these different approaches essential metrics.
</details>

<details>
    <summary>Key points</summary>
    * Compares five different GAN models
    * Evaluates image resolutions and accuracy metrics
    * Identifies best-performing model
</details>
</details>

---


<details>
<summary><b> Generating Intermediate Representations for Compositional Text-To-Image Generation</b></summary>

* **Authors:** Ran Galun, Sagie Benaim
* **arXiv ID:** 2410.09792
* **One-liner:** Proposed a compositional approach for text-to-image generation using intermediate representations to improve spatial accuracy.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2410.09792) | [[PDF]](https://arxiv.org/pdf/2410.09792)

> **Core Innovation**
> Enhanced diffusion models by generating aligned intermediate maps first, then mapping to final images for better FID scores.

<details>
    <summary>Abstract</summary>
    Text-to-image diffusion models have demonstrated an impressive ability to produce high-quality outputs. However, they often struggle to accurately follow fine-grained spatial information in an input text. To this end, we propose a compositional approach for text-to-image generation based on two stages. In the first stage, we design a diffusion-based generative model to produce one or more aligned intermediate representations (such as depth or segmentation maps) conditioned on text. In the second stage, we map these representations, together with the text, to the final output image using a separate diffusion-based generative model. Our findings indicate that such compositional approach can improve image generation, resulting in a notable improvement in FID score and a comparable CLIP score, when compared to the standard non-compositional baseline.
</details>

<details>
    <summary>Key points</summary>
    * Uses two-stage diffusion process
    * Generates intermediate representations like depth maps
    * Improves FID score compared to baseline
</details>
</details>

---


<details>
<summary><b> FlexGen: Flexible Multi-View Generation from Text and Image Inputs</b></summary>

* **Authors:** Xinli Xu, Wenhang Ge, Jiantao Lin, Jiawei Feng, Lie Xu, HanFeng Zhao, Shunsi Zhang, Ying-Cong Chen
* **arXiv ID:** 2410.10745
* **One-liner:** Introduced FlexGen for controllable multi-view image generation using 3D-aware text annotations from GPT-4V.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2410.10745) | [[PDF]](https://arxiv.org/pdf/2410.10745)

> **Core Innovation**
> Enabled flexible multi-view synthesis from single-view or text inputs with enhanced controllability over attributes.

<details>
    <summary>Abstract</summary>
    In this work, we introduce FlexGen, a flexible framework designed to generate controllable and consistent multi-view images, conditioned on a single-view image, or a text prompt, or both. FlexGen tackles the challenges of controllable multi-view synthesis through additional conditioning on 3D-aware text annotations. We utilize the strong reasoning capabilities of GPT-4V to generate 3D-aware text annotations. By analyzing four orthogonal views of an object arranged as tiled multi-view images, GPT-4V can produce text annotations that include 3D-aware information with spatial relationship. By integrating the control signal with proposed adaptive dual-control module, our model can generate multi-view images that correspond to the specified text. FlexGen supports multiple controllable capabilities, allowing users to modify text prompts to generate reasonable and corresponding unseen parts. Additionally, users can influence attributes such as appearance and material properties, including metallic and roughness. Extensive experiments demonstrate that our approach offers enhanced multiple controllability, marking a significant advancement over existing multi-view diffusion models. This work has substantial implications for fields requiring rapid and flexible 3D content creation, including game development, animation, and virtual reality. Project page: <a href="https://xxu068.github.io/flexgen.github.io/" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Utilizes GPT-4V for 3D-aware text annotations
    * Incorporates adaptive dual-control module
    * Supports multiple controllable capabilities
</details>
</details>

---


<details>
<summary><b> HART: Efficient Visual Generation with Hybrid Autoregressive Transformer</b></summary>

* **Authors:** Haotian Tang, Yecheng Wu, Shang Yang, Enze Xie, Junsong Chen, Junyu Chen, Zhuoyang Zhang, Han Cai, Yao Lu, Song Han
* **arXiv ID:** 2410.10812
* **One-liner:** Developed HART, a hybrid autoregressive model combining discrete and continuous tokens for high-quality 1024x1024 image generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2410.10812) | [[PDF]](https://arxiv.org/pdf/2410.10812)

> **Core Innovation**
> Overcame AR model limitations with a hybrid tokenizer, achieving better FID and efficiency than diffusion models.

<details>
    <summary>Abstract</summary>
    We introduce Hybrid Autoregressive Transformer (HART), an autoregressive (AR) visual generation model capable of directly generating 1024x1024 images, rivaling diffusion models in image generation quality. Existing AR models face limitations due to the poor image reconstruction quality of their discrete tokenizers and the prohibitive training costs associated with generating 1024px images. To address these challenges, we present the hybrid tokenizer, which decomposes the continuous latents from the autoencoder into two components: discrete tokens representing the big picture and continuous tokens representing the residual components that cannot be represented by the discrete tokens. The discrete component is modeled by a scalable-resolution discrete AR model, while the continuous component is learned with a lightweight residual diffusion module with only 37M parameters. Compared with the discrete-only VAR tokenizer, our hybrid approach improves reconstruction FID from 2.11 to 0.30 on MJHQ-30K, leading to a 31% generation FID improvement from 7.85 to 5.38. HART also outperforms state-of-the-art diffusion models in both FID and CLIP score, with 4.5-7.7x higher throughput and 6.9-13.4x lower MACs. Our code is open sourced at <a href="https://github.com/mit-han-lab/hart" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Uses hybrid tokenizer with discrete and continuous components
    * Models discrete tokens with AR and continuous with residual diffusion
    * Achieves high throughput and low computational cost
</details>
</details>

---


<details>
<summary><b> SAFREE: Training-Free and Adaptive Guard for Safe Text-to-Image And Video Generation</b></summary>

* **Authors:** Jaehong Yoon, Shoubin Yu, Vaidehi Patil, Huaxiu Yao, Mohit Bansal
* **arXiv ID:** 2410.12761
* **One-liner:** Proposed SAFREE, a training-free method for safe text-to-image and text-to-video generation without altering model weights.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2410.12761) | [[PDF]](https://arxiv.org/pdf/2410.12761)

> **Core Innovation**
> Detects a subspace for toxic concepts in text embeddings and steers prompts away, incorporating self-validating filtering and adaptive re-attention mechanisms to balance safety and quality.

<details>
    <summary>Abstract</summary>
    Recent advances in diffusion models have significantly enhanced their ability to generate high-quality images and videos, but they have also increased the risk of producing unsafe content. Existing unlearning/editing-based methods for safe generation remove harmful concepts from models but face several challenges: (1) They cannot instantly remove harmful concepts without training. (2) Their safe generation capabilities depend on collected training data. (3) They alter model weights, risking degradation in quality for content unrelated to toxic concepts. To address these, we propose SAFREE, a novel, training-free approach for safe T2I and T2V, that does not alter the model&#39;s weights. Specifically, we detect a subspace corresponding to a set of toxic concepts in the text embedding space and steer prompt embeddings away from this subspace, thereby filtering out harmful content while preserving intended semantics. To balance the trade-off between filtering toxicity and preserving safe concepts, SAFREE incorporates a novel self-validating filtering mechanism that dynamically adjusts the denoising steps when applying the filtered embeddings. Additionally, we incorporate adaptive re-attention mechanisms within the diffusion latent space to selectively diminish the influence of features related to toxic concepts at the pixel level. In the end, SAFREE ensures coherent safety checking, preserving the fidelity, quality, and safety of the output. SAFREE achieves SOTA performance in suppressing unsafe content in T2I generation compared to training-free baselines and effectively filters targeted concepts while maintaining high-quality images. It also shows competitive results against training-based methods. We extend SAFREE to various T2I backbones and T2V tasks, showcasing its flexibility and generalization. SAFREE provides a robust and adaptable safeguard for ensuring safe visual generation.
</details>

<details>
    <summary>Key points</summary>
    * Subspace detection for toxic concepts in text embedding space
    * Steering prompt embeddings away from toxic subspace
    * Self-validating filtering mechanism for dynamic denoising adjustment
    * Adaptive re-attention mechanisms in diffusion latent space
</details>
</details>

---


<details>
<summary><b> Fluid: Scaling Autoregressive Text-to-image Generative Models with Continuous Tokens</b></summary>

* **Authors:** Lijie Fan, Tianhong Li, Siyang Qin, Yuanzhen Li, Chen Sun, Michael Rubinstein, Deqing Sun, Kaiming He, Yonglong Tian
* **arXiv ID:** 2410.13863
* **One-liner:** Introduced Fluid, a random-order autoregressive model on continuous tokens, achieving state-of-the-art zero-shot FID and GenEval scores.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2410.13863) | [[PDF]](https://arxiv.org/pdf/2410.13863)

> **Core Innovation**
> Empirically showed that continuous tokens and random generation order improve scaling and performance in text-to-image generation.

<details>
    <summary>Abstract</summary>
    Scaling up autoregressive models in vision has not proven as beneficial as in large language models. In this work, we investigate this scaling problem in the context of text-to-image generation, focusing on two critical factors: whether models use discrete or continuous tokens, and whether tokens are generated in a random or fixed raster order using BERT- or GPT-like transformer architectures. Our empirical results show that, while all models scale effectively in terms of validation loss, their evaluation performance -- measured by FID, GenEval score, and visual quality -- follows different trends. Models based on continuous tokens achieve significantly better visual quality than those using discrete tokens. Furthermore, the generation order and attention mechanisms significantly affect the GenEval score: random-order models achieve notably better GenEval scores compared to raster-order models. Inspired by these findings, we train Fluid, a random-order autoregressive model on continuous tokens. Fluid 10.5B model achieves a new state-of-the-art zero-shot FID of 6.16 on MS-COCO 30K, and 0.69 overall score on the GenEval benchmark. We hope our findings and results will encourage future efforts to further bridge the scaling gap between vision and language models.
</details>

<details>
    <summary>Key points</summary>
    * Investigation of discrete vs. continuous tokens and generation orders
    * Training Fluid model with random-order autoregression on continuous tokens
    * Achieving SOTA zero-shot FID of 6.16 on MS-COCO and 0.69 on GenEval
</details>
</details>

---


<details>
<summary><b> Synergistic Dual Spatial-aware Generation of Image-to-Text and Text-to-Image</b></summary>

* **Authors:** Yu Zhao, Hao Fei, Xiangtai Li, Libo Qin, Jiayi Ji, Hongyuan Zhu, Meishan Zhang, Min Zhang, Jianguo Wei
* **arXiv ID:** 2410.15312
* **One-liner:** Developed a dual learning framework with 3D scene graphs for spatial image-to-text and text-to-image tasks, improving spatial understanding.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2410.15312) | [[PDF]](https://arxiv.org/pdf/2410.15312)

> **Core Innovation**
> Used Spatial Dual Discrete Diffusion (SD^3) to leverage intermediate 3D features for guiding hard processes, enhancing mutual benefits between tasks.

<details>
    <summary>Abstract</summary>
    In the visual spatial understanding (VSU) area, spatial image-to-text (SI2T) and spatial text-to-image (ST2I) are two fundamental tasks that appear in dual form. Existing methods for standalone SI2T or ST2I perform imperfectly in spatial understanding, due to the difficulty of 3D-wise spatial feature modeling. In this work, we consider modeling the SI2T and ST2I together under a dual learning framework. During the dual framework, we then propose to represent the 3D spatial scene features with a novel 3D scene graph (3DSG) representation that can be shared and beneficial to both tasks. Further, inspired by the intuition that the easier 3D$\to$image and 3D$\to$text processes also exist symmetrically in the ST2I and SI2T, respectively, we propose the Spatial Dual Discrete Diffusion (SD$^3$) framework, which utilizes the intermediate features of the 3D$\to$X processes to guide the hard X$\to$3D processes, such that the overall ST2I and SI2T will benefit each other. On the visual spatial understanding dataset VSD, our system outperforms the mainstream T2I and I2T methods significantly. Further in-depth analysis reveals how our dual learning strategy advances.
</details>

<details>
    <summary>Key points</summary>
    * Dual learning framework for SI2T and ST2I
    * 3D scene graph representation for spatial features
    * Spatial Dual Discrete Diffusion (SD^3) framework
    * Utilization of 3D→X processes to guide X→3D processes
</details>
</details>

---


<details>
<summary><b> Progressive Compositionality in Text-to-Image Generative Models</b></summary>

* **Authors:** Evans Xu Han, Linghao Jin, Xiaofeng Liu, Paul Pu Liang
* **arXiv ID:** 2410.16719
* **One-liner:** Created ConPair dataset using LLMs and VQA for contrastive learning, and proposed EvoGen curriculum to improve compositional T2I generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2410.16719) | [[PDF]](https://arxiv.org/pdf/2410.16719)

> **Core Innovation**
> Automatically curated high-quality contrastive image pairs and applied multi-stage curriculum learning to address compositional challenges.

<details>
    <summary>Abstract</summary>
    Despite the impressive text-to-image (T2I) synthesis capabilities of diffusion models, they often struggle to understand compositional relationships between objects and attributes, especially in complex settings. Existing solutions have tackled these challenges by optimizing the cross-attention mechanism or learning from the caption pairs with minimal semantic changes. However, can we generate high-quality complex contrastive images that diffusion models can directly discriminate based on visual representations? In this work, we leverage large-language models (LLMs) to compose realistic, complex scenarios and harness Visual-Question Answering (VQA) systems alongside diffusion models to automatically curate a contrastive dataset, ConPair, consisting of 15k pairs of high-quality contrastive images. These pairs feature minimal visual discrepancies and cover a wide range of attribute categories, especially complex and natural scenarios. To learn effectively from these error cases, i.e., hard negative images, we propose EvoGen, a new multi-stage curriculum for contrastive learning of diffusion models. Through extensive experiments across a wide range of compositional scenarios, we showcase the effectiveness of our proposed framework on compositional T2I benchmarks.
</details>

<details>
    <summary>Key points</summary>
    * Leveraging LLMs and VQA to compose and curate ConPair dataset
    * Proposing EvoGen multi-stage curriculum for contrastive learning
    * Focus on hard negative images for effective learning
</details>
</details>

---


<details>
<summary><b> FairQueue: Rethinking Prompt Learning for Fair Text-to-Image Generation</b></summary>

* **Authors:** Christopher T.H Teo, Milad Abdollahzadeh, Xinda Ma, Ngai-man Cheung
* **arXiv ID:** 2410.18615
* **One-liner:** Identified quality degradation in prompt learning for fair T2I generation and proposed Prompt Queuing and Attention Amplification to improve it.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2410.18615) | [[PDF]](https://arxiv.org/pdf/2410.18615)

> **Core Innovation**
> Analyzed cross-attention maps to reveal abnormalities and introduced methods to enhance generation quality while maintaining fairness.

<details>
    <summary>Abstract</summary>
    Recently, prompt learning has emerged as the state-of-the-art (SOTA) for fair text-to-image (T2I) generation. Specifically, this approach leverages readily available reference images to learn inclusive prompts for each target Sensitive Attribute (tSA), allowing for fair image generation. In this work, we first reveal that this prompt learning-based approach results in degraded sample quality. Our analysis shows that the approach&#39;s training objective -- which aims to align the embedding differences of learned prompts and reference images -- could be sub-optimal, resulting in distortion of the learned prompts and degraded generated images. To further substantiate this claim, as our major contribution, we deep dive into the denoising subnetwork of the T2I model to track down the effect of these learned prompts by analyzing the cross-attention maps. In our analysis, we propose a novel prompt switching analysis: I2H and H2I. Furthermore, we propose new quantitative characterization of cross-attention maps. Our analysis reveals abnormalities in the early denoising steps, perpetuating improper global structure that results in degradation in the generated samples. Building on insights from our analysis, we propose two ideas: (i) Prompt Queuing and (ii) Attention Amplification to address the quality issue. Extensive experimental results on a wide range of tSAs show that our proposed method outperforms SOTA approach&#39;s image generation quality, while achieving competitive fairness. More resources at FairQueue Project site: <a href="https://sutd-visual-computing-group.github.io/FairQueue" rel="external noopener nofollow" class="link-external link-https">this https URL</a>
</details>

<details>
    <summary>Key points</summary>
    * Analysis of cross-attention maps and prompt switching (I2H and H2I)
    * Proposal of Prompt Queuing and Attention Amplification
    * Quantitative characterization of cross-attention abnormalities
</details>
</details>

---


<details>
<summary><b> Diff-Instruct++: Training One-step Text-to-image Generator Model to Align with Human Preferences</b></summary>

* **Authors:** Weijian Luo
* **arXiv ID:** 2410.18881
* **One-liner:** Introduced Diff-Instruct++ (DI++), a fast-converging, image data-free method for aligning one-step T2I generators with human preferences.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2410.18881) | [[PDF]](https://arxiv.org/pdf/2410.18881)

> **Core Innovation**
> Formulated alignment as maximizing human reward with KL divergence, showing CFG for diffusion distillation is equivalent to RLHF.

<details>
    <summary>Abstract</summary>
    One-step text-to-image generator models offer advantages such as swift inference efficiency, flexible architectures, and state-of-the-art generation performance. In this paper, we study the problem of aligning one-step generator models with human preferences for the first time. Inspired by the success of reinforcement learning using human feedback (RLHF), we formulate the alignment problem as maximizing expected human reward functions while adding an Integral Kullback-Leibler divergence term to prevent the generator from diverging. By overcoming technical challenges, we introduce Diff-Instruct++ (DI++), the first, fast-converging and image data-free human preference alignment method for one-step text-to-image generators. We also introduce novel theoretical insights, showing that using CFG for diffusion distillation is secretly doing RLHF with DI++. Such an interesting finding brings understanding and potential contributions to future research involving CFG. In the experiment sections, we align both UNet-based and DiT-based one-step generators using DI++, which use the Stable Diffusion 1.5 and the PixelArt-$\alpha$ as the reference diffusion processes. The resulting DiT-based one-step text-to-image model achieves a strong Aesthetic Score of 6.19 and an Image Reward of 1.24 on the COCO validation prompt dataset. It also achieves a leading Human preference Score (HPSv2.0) of 28.48, outperforming other open-sourced models such as Stable Diffusion XL, DMD2, SD-Turbo, as well as PixelArt-$\alpha$. Both theoretical contributions and empirical evidence indicate that DI++ is a strong human-preference alignment approach for one-step text-to-image models. The homepage of the paper is <a href="https://github.com/pkulwj1994/diff_instruct_pp" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Formulation of alignment problem with human reward and KL divergence
    * Development of Diff-Instruct++ (DI++) method
    * Theoretical insight linking CFG to RLHF
    * Application to UNet-based and DiT-based one-step generators
</details>
</details>

---


<details>
<summary><b> Kandinsky 3: Text-to-Image Synthesis for Multifunctional Generative Framework</b></summary>

* **Authors:** Vladimir Arkhipkin, Viacheslav Vasilev, Andrei Filatov, Igor Pavlov, Julia Agafonova, Nikolai Gerasimenko, Anna Averchenkova, Evelina Mironova, Anton Bukashkin, Konstantin Kulikov, Andrey Kuznetsov, Denis Dimitrov
* **arXiv ID:** 2410.21061
* **One-liner:** Presented Kandinsky 3, a latent diffusion-based T2I model with high quality and adaptability for various generation tasks.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2410.21061) | [[PDF]](https://arxiv.org/pdf/2410.21061)

> **Core Innovation**
> Extended the base model for inpainting, outpainting, image fusion, I2V, T2V, and created a distilled version for faster inference.

<details>
    <summary>Abstract</summary>
    Text-to-image (T2I) diffusion models are popular for introducing image manipulation methods, such as editing, image fusion, inpainting, etc. At the same time, image-to-video (I2V) and text-to-video (T2V) models are also built on top of T2I models. We present Kandinsky 3, a novel T2I model based on latent diffusion, achieving a high level of quality and photorealism. The key feature of the new architecture is the simplicity and efficiency of its adaptation for many types of generation tasks. We extend the base T2I model for various applications and create a multifunctional generation system that includes text-guided inpainting/outpainting, image fusion, text-image fusion, image variations generation, I2V and T2V generation. We also present a distilled version of the T2I model, evaluating inference in 4 steps of the reverse process without reducing image quality and 3 times faster than the base model. We deployed a user-friendly demo system in which all the features can be tested in the public domain. Additionally, we released the source code and checkpoints for the Kandinsky 3 and extended models. Human evaluations show that Kandinsky 3 demonstrates one of the highest quality scores among open source generation systems.
</details>

<details>
    <summary>Key points</summary>
    * Development of Kandinsky 3 latent diffusion model
    * Extension for multiple applications (e.g., inpainting, I2V)
    * Creation of distilled version for efficient inference
    * Deployment of user-friendly demo system
</details>
</details>

---


<details>
<summary><b> Diffusion Beats Autoregressive: An Evaluation of Compositional Generation in Text-to-Image Models</b></summary>

* **Authors:** Arash Marioriyad, Parham Rezaei, Mahdieh Soleymani Baghshah, Mohammad Hossein Rohban
* **arXiv ID:** 2410.22775
* **One-liner:** Evaluated compositional generation capabilities of FLUX and LlamaGen, finding FLUX comparable to DALL-E3 while LlamaGen lags behind diffusion models.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2410.22775) | [[PDF]](https://arxiv.org/pdf/2410.22775)

> **Core Innovation**
> Used T2I-CompBench to assess models, highlighting strengths and weaknesses in handling complex compositions.

<details>
    <summary>Abstract</summary>
    Text-to-image (T2I) generative models, such as Stable Diffusion and DALL-E, have shown remarkable proficiency in producing high-quality, realistic, and natural images from textual descriptions. However, these models sometimes fail to accurately capture all the details specified in the input prompts, particularly concerning entities, attributes, and spatial relationships. This issue becomes more pronounced when the prompt contains novel or complex compositions, leading to what are known as compositional generation failure modes. Recently, a new open-source diffusion-based T2I model, FLUX, has been introduced, demonstrating strong performance in high-quality image generation. Additionally, autoregressive T2I models like LlamaGen have claimed competitive visual quality performance compared to diffusion-based models. In this study, we evaluate the compositional generation capabilities of these newly introduced models against established models using the T2I-CompBench benchmark. Our findings reveal that LlamaGen, as a vanilla autoregressive model, is not yet on par with state-of-the-art diffusion models for compositional generation tasks under the same criteria, such as model size and inference time. On the other hand, the open-source diffusion-based model FLUX exhibits compositional generation capabilities comparable to the state-of-the-art closed-source model DALL-E3.
</details>

<details>
    <summary>Key points</summary>
    * Evaluation using T2I-CompBench benchmark
    * Comparison of FLUX and LlamaGen with established models
    * Findings on FLUX's competitive performance and LlamaGen's limitations
</details>
</details>

---


<details>
<summary><b> Image2Text2Image: A Novel Framework for Label-Free Evaluation of Image-to-Text Generation with Text-to-Image Diffusion Models</b></summary>

* **Authors:** Jia-Hong Huang, Hongyi Zhu, Yixian Shen, Stevan Rudinac, Evangelos Kanoulas
* **arXiv ID:** 2411.05706
* **One-liner:** Proposed Image2Text2Image framework for evaluating image captioning models using diffusion models without human references.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2411.05706) | [[PDF]](https://arxiv.org/pdf/2411.05706)

> **Core Innovation**
> Leverages T2I generation to measure similarity between original and generated images, validated through experiments and human evaluations.

<details>
    <summary>Abstract</summary>
    Evaluating the quality of automatically generated image descriptions is a complex task that requires metrics capturing various dimensions, such as grammaticality, coverage, accuracy, and truthfulness. Although human evaluation provides valuable insights, its cost and time-consuming nature pose limitations. Existing automated metrics like BLEU, ROUGE, METEOR, and CIDEr attempt to fill this gap, but they often exhibit weak correlations with human judgment. To address this challenge, we propose a novel evaluation framework called Image2Text2Image, which leverages diffusion models, such as Stable Diffusion or DALL-E, for text-to-image generation. In the Image2Text2Image framework, an input image is first processed by a selected image captioning model, chosen for evaluation, to generate a textual description. Using this generated description, a diffusion model then creates a new image. By comparing features extracted from the original and generated images, we measure their similarity using a designated similarity metric. A high similarity score suggests that the model has produced a faithful textual description, while a low score highlights discrepancies, revealing potential weaknesses in the model&#39;s performance. Notably, our framework does not rely on human-annotated reference captions, making it a valuable tool for assessing image captioning models. Extensive experiments and human evaluations validate the efficacy of our proposed Image2Text2Image evaluation framework. The code and dataset will be published to support further research in the community.
</details>

<details>
    <summary>Key points</summary>
    * Framework using image captioning and T2I generation for evaluation
    * Similarity measurement between original and generated images
    * Validation through extensive experiments and human evaluations
</details>
</details>

---


<details>
<summary><b> Region-Aware Text-to-Image Generation via Hard Binding and Soft Refinement</b></summary>

* **Authors:** Zhennan Chen, Yajie Li, Haofan Wang, Zhibo Chen, Zhengkai Jiang, Jun Li, Qian Wang, Jian Yang, Ying Tai
* **arXiv ID:** 2411.06558
* **One-liner:** Introduced RAG, a tuning-free method for regional-aware text-to-image generation with precise layout control and repainting capabilities.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2411.06558) | [[PDF]](https://arxiv.org/pdf/2411.06558)

> **Core Innovation**
> Decouples multi-region generation into regional hard binding and soft refinement, enhancing control without additional training.

<details>
    <summary>Abstract</summary>
    Regional prompting, or compositional generation, which enables fine-grained spatial control, has gained increasing attention for its practicality in real-world applications. However, previous methods either introduce additional trainable modules, thus only applicable to specific models, or manipulate on score maps within cross-attention layers using attention masks, resulting in limited control strength when the number of regions increases. To handle these limitations, we present RAG, a Regional-Aware text-to-image Generation method conditioned on regional descriptions for precise layout composition. RAG decouple the multi-region generation into two sub-tasks, the construction of individual region (Regional Hard Binding) that ensures the regional prompt is properly executed, and the overall detail refinement (Regional Soft Refinement) over regions that dismiss the visual boundaries and enhance adjacent interactions. Furthermore, RAG novelly makes repainting feasible, where users can modify specific unsatisfied regions in the last generation while keeping all other regions unchanged, without relying on additional inpainting models. Our approach is tuning-free and applicable to other frameworks as an enhancement to the prompt following property. Quantitative and qualitative experiments demonstrate that RAG achieves superior performance over attribute binding and object relationship than previous tuning-free methods.
</details>

<details>
    <summary>Key points</summary>
    * Regional-Aware Generation (RAG) method
    * Decoupling into Regional Hard Binding and Regional Soft Refinement
    * Feasibility of repainting without inpainting models
    * Applicability to other frameworks as an enhancement
</details>
</details>

---


<details>
<summary><b> JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation</b></summary>

* **Authors:** Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Xingkai yu, Liang Zhao, Yisong Wang, Jiaying Liu, Chong Ruan
* **arXiv ID:** 2411.07975
* **One-liner:** JanusFlow unifies image understanding and generation in a single model.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2411.07975) | [[PDF]](https://arxiv.org/pdf/2411.07975)

> **Core Innovation**
> Integration of autoregressive language models with rectified flow for unified vision-language modeling.

<details>
    <summary>Abstract</summary>
    We present JanusFlow, a powerful framework that unifies image understanding and generation in a single model. JanusFlow introduces a minimalist architecture that integrates autoregressive language models with rectified flow, a state-of-the-art method in generative modeling. Our key finding demonstrates that rectified flow can be straightforwardly trained within the large language model framework, eliminating the need for complex architectural modifications. To further improve the performance of our unified model, we adopt two key strategies: (i) decoupling the understanding and generation encoders, and (ii) aligning their representations during unified training. Extensive experiments show that JanusFlow achieves comparable or superior performance to specialized models in their respective domains, while significantly outperforming existing unified approaches across standard benchmarks. This work represents a step toward more efficient and versatile vision-language models.
</details>

<details>
    <summary>Key points</summary>
    * Introduces a minimalist architecture with rectified flow
    * Decouples understanding and generation encoders
    * Aligns representations during unified training
</details>
</details>

---


<details>
<summary><b> Visual question answering based evaluation metrics for text-to-image generation</b></summary>

* **Authors:** Mizuki Miyamoto, Ryugo Morita, Jinjia Zhou
* **arXiv ID:** 2411.10183
* **One-liner:** Proposes new evaluation metrics for detailed text-image alignment.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2411.10183) | [[PDF]](https://arxiv.org/pdf/2411.10183)

> **Core Innovation**
> Assessment of alignment for every individual object using ChatGPT and VQA.

<details>
    <summary>Abstract</summary>
    Text-to-image generation and text-guided image manipulation have received considerable attention in the field of image generation tasks. However, the mainstream evaluation methods for these tasks have difficulty in evaluating whether all the information from the input text is accurately reflected in the generated images, and they mainly focus on evaluating the overall alignment between the input text and the generated images. This paper proposes new evaluation metrics that assess the alignment between input text and generated images for every individual object. Firstly, according to the input text, chatGPT is utilized to produce questions for the generated images. After that, we use Visual Question Answering(VQA) to measure the relevance of the generated images to the input text, which allows for a more detailed evaluation of the alignment compared to existing methods. In addition, we use Non-Reference Image Quality Assessment(NR-IQA) to evaluate not only the text-image alignment but also the quality of the generated images. Experimental results show that our proposed evaluation approach is the superior metric that can simultaneously assess finer text-image alignment and image quality while allowing for the adjustment of these ratios.
</details>

<details>
    <summary>Key points</summary>
    * Utilizes ChatGPT to generate questions for images
    * Employs VQA for detailed alignment evaluation
    * Incorporates NR-IQA for image quality assessment
</details>
</details>

---


<details>
<summary><b> Safe Text-to-Image Generation: Simply Sanitize the Prompt Embedding</b></summary>

* **Authors:** Huming Qiu, Guanxu Chen, Mi Zhang, Xiaohan Zhang, Xiaoyu You, Min Yang
* **arXiv ID:** 2411.10329
* **One-liner:** Enhances T2I model safety by sanitizing prompt embeddings.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2411.10329) | [[PDF]](https://arxiv.org/pdf/2411.10329)

> **Core Innovation**
> Interpretable framework for identifying and mitigating harmful content in text prompts.

<details>
    <summary>Abstract</summary>
    In recent years, text-to-image (T2I) generation models have made significant progress in generating high-quality images that align with text descriptions. However, these models also face the risk of unsafe generation, potentially producing harmful content that violates usage policies, such as explicit material. Existing safe generation methods typically focus on suppressing inappropriate content by erasing undesired concepts from visual representations, while neglecting to sanitize the textual representation. Although these methods help mitigate the risk of misuse to some extent, their robustness remains insufficient when dealing with adversarial attacks.
<br>Given that semantic consistency between input text and output image is a core requirement of T2I models, we identify that textual representations are likely the primary source of unsafe generation. To this end, we propose Embedding Sanitizer (ES), which enhances the safety of T2I models by sanitizing inappropriate concepts in prompt embeddings. To our knowledge, ES is the first interpretable safe generation framework that assigns a score to each token in the prompt to indicate its potential harmfulness. In addition, ES adopts a plug-and-play modular design, offering compatibility for seamless integration with various T2I models and other safeguards. Evaluations on five prompt benchmarks show that ES outperforms eleven existing safeguard baselines, achieving state-of-the-art robustness while maintaining high-quality image generation.
</details>

<details>
    <summary>Key points</summary>
    * Assigns harmfulness scores to tokens in prompts
    * Adopts plug-and-play modular design
    * Outperforms existing safeguards in robustness
</details>
</details>

---


<details>
<summary><b> High-Resolution Image Synthesis via Next-Token Prediction</b></summary>

* **Authors:** Dengsheng Chen, Jie Hu, Tiezhu Yue, Xiaoming Wei, Enhua Wu
* **arXiv ID:** 2411.14808
* **One-liner:** Achieves state-of-the-art high-resolution image synthesis via next-token prediction.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2411.14808) | [[PDF]](https://arxiv.org/pdf/2411.14808)

> **Core Innovation**
> Autoregressive model for generating photorealistic images at arbitrary resolutions up to 4K.

<details>
    <summary>Abstract</summary>
    Recently, autoregressive models have demonstrated remarkable performance in class-conditional image generation. However, the application of next-token prediction to high-resolution text-to-image generation remains largely unexplored. In this paper, we introduce \textbf{D-JEPA$\cdot$T2I}, an autoregressive model based on continuous tokens that incorporates innovations in both architecture and training strategy to generate high-quality, photorealistic images at arbitrary resolutions, up to 4K. Architecturally, we adopt the denoising joint embedding predictive architecture (D-JEPA) while leveraging a multimodal visual transformer to effectively integrate textual and visual features. Additionally, we introduce flow matching loss alongside the proposed Visual Rotary Positional Embedding (VoPE) to enable continuous resolution learning. In terms of training strategy, we propose a data feedback mechanism that dynamically adjusts the sampling procedure based on statistical analysis and an online learning critic model. This encourages the model to move beyond its comfort zone, reducing redundant training on well-mastered scenarios and compelling it to address more challenging cases with suboptimal generation quality. For the first time, we achieve state-of-the-art high-resolution image synthesis via next-token prediction.
</details>

<details>
    <summary>Key points</summary>
    * Incorporates D-JEPA architecture with multimodal visual transformer
    * Uses flow matching loss and Visual Rotary Positional Embedding
    * Implements data feedback mechanism for adaptive training
</details>
</details>

---


<details>
<summary><b> Automatic Evaluation for Text-to-image Generation: Task-decomposed Framework, Distilled Training, and Meta-evaluation Benchmark</b></summary>

* **Authors:** Rong-Cheng Tu, Zi-Ao Ma, Tian Lan, Yuehao Zhao, Heyan Huang, Xian-Ling Mao
* **arXiv ID:** 2411.15488
* **One-liner:** Distills GPT-4o's evaluation capabilities into a smaller open-source MLLM.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2411.15488) | [[PDF]](https://arxiv.org/pdf/2411.15488)

> **Core Innovation**
> Task decomposition framework for efficient and scalable image quality evaluation.

<details>
    <summary>Abstract</summary>
    Driven by the remarkable progress in diffusion models, text-to-image generation has made significant strides, creating a pressing demand for automatic quality evaluation of generated images. Current state-of-the-art automatic evaluation methods heavily rely on Multi-modal Large Language Models (MLLMs), particularly powerful commercial models like GPT-4o. While these models are highly effective, their substantial costs limit scalability in large-scale evaluations. Adopting open-source MLLMs is an alternative; however, their performance falls short due to significant limitations in processing multi-modal data compared to commercial MLLMs. To tackle these problems, we first propose a task decomposition evaluation framework based on GPT-4o to automatically construct a new training dataset, where the complex evaluation task is decoupled into simpler sub-tasks, effectively reducing the learning complexity. Based on this dataset, we design innovative training strategies to effectively distill GPT-4o&#39;s evaluation capabilities into a 7B open-source MLLM, MiniCPM-V-2.6. Furthermore, to reliably and comprehensively assess prior works and our proposed model, we manually annotate a meta-evaluation benchmark that includes chain-of-thought explanations alongside quality scores for generated images. Experimental results demonstrate that our distilled open-source MLLM significantly outperforms the current state-of-the-art GPT-4o-base baseline, VIEScore, with over 4.6\% improvement in Spearman and Kendall correlations with human judgments.
</details>

<details>
    <summary>Key points</summary>
    * Decouples evaluation into simpler sub-tasks
    * Trains 7B MLLM with innovative strategies
    * Creates meta-evaluation benchmark with human annotations
</details>
</details>

---


<details>
<summary><b> Interactive Visual Assessment for Text-to-Image Generation Models</b></summary>

* **Authors:** Xiaoyue Mi, Fan Tang, Juan Cao, Qiang Sheng, Ziyao Huang, Peng Li, Yang Liu, Tong-Yee Lee
* **arXiv ID:** 2411.15509
* **One-liner:** Facilitates dynamic interactive assessment of T2I models to uncover failures.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2411.15509) | [[PDF]](https://arxiv.org/pdf/2411.15509)

> **Core Innovation**
> LLM-powered framework for collaborative human-model evaluation with adaptive input generation.

<details>
    <summary>Abstract</summary>
    Visual generation models have achieved remarkable progress in computer graphics applications but still face significant challenges in real-world deployment. Current assessment approaches for visual generation tasks typically follow an isolated three-phase framework: test input collection, model output generation, and user assessment. These fashions suffer from fixed coverage, evolving difficulty, and data leakage risks, limiting their effectiveness in comprehensively evaluating increasingly complex generation models. To address these limitations, we propose DyEval, an LLM-powered dynamic interactive visual assessment framework that facilitates collaborative evaluation between humans and generative models for text-to-image systems. DyEval features an intuitive visual interface that enables users to interactively explore and analyze model behaviors, while adaptively generating hierarchical, fine-grained, and diverse textual inputs to continuously probe the capability boundaries of the models based on their feedback. Additionally, to provide interpretable analysis for users to further improve tested models, we develop a contextual reflection module that mines failure triggers of test inputs and reflects model potential failure patterns supporting in-depth analysis using the logical reasoning ability of LLM. Qualitative and quantitative experiments demonstrate that DyEval can effectively help users identify max up to 2.56 times generation failures than conventional methods, and uncover complex and rare failure patterns, such as issues with pronoun generation and specific cultural context generation. Our framework provides valuable insights for improving generative models and has broad implications for advancing the reliability and capabilities of visual generation systems across various domains.
</details>

<details>
    <summary>Key points</summary>
    * Features intuitive visual interface for user interaction
    * Generates hierarchical and diverse textual inputs
    * Includes contextual reflection module for failure analysis
</details>
</details>

---


<details>
<summary><b> ChatGen: Automatic Text-to-Image Generation From FreeStyle Chatting</b></summary>

* **Authors:** Chengyou Jia, Changliang Xia, Zhuohang Dang, Weijia Wu, Hangwei Qian, Minnan Luo
* **arXiv ID:** 2411.17176
* **One-liner:** Automates T2I generation steps to reduce user trial-and-error.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2411.17176) | [[PDF]](https://arxiv.org/pdf/2411.17176)

> **Core Innovation**
> Multi-stage evolution strategy for automating prompt crafting and model configuration.

<details>
    <summary>Abstract</summary>
    Despite the significant advancements in text-to-image (T2I) generative models, users often face a trial-and-error challenge in practical scenarios. This challenge arises from the complexity and uncertainty of tedious steps such as crafting suitable prompts, selecting appropriate models, and configuring specific arguments, making users resort to labor-intensive attempts for desired images. This paper proposes Automatic T2I generation, which aims to automate these tedious steps, allowing users to simply describe their needs in a freestyle chatting way. To systematically study this problem, we first introduce ChatGenBench, a novel benchmark designed for Automatic T2I. It features high-quality paired data with diverse freestyle inputs, enabling comprehensive evaluation of automatic T2I models across all steps. Additionally, recognizing Automatic T2I as a complex multi-step reasoning task, we propose ChatGen-Evo, a multi-stage evolution strategy that progressively equips models with essential automation skills. Through extensive evaluation across step-wise accuracy and image quality, ChatGen-Evo significantly enhances performance over various baselines. Our evaluation also uncovers valuable insights for advancing automatic T2I. All our data, code, and models will be available in \url{<a href="https://chengyou-jia.github.io/ChatGen-Home" rel="external noopener nofollow" class="link-external link-https">this https URL</a>}
</details>

<details>
    <summary>Key points</summary>
    * Introduces ChatGenBench benchmark for evaluation
    * Proposes ChatGen-Evo with progressive skill acquisition
    * Enhances performance across step-wise accuracy and image quality
</details>
</details>

---


<details>
<summary><b> Interleaved Scene Graphs for Interleaved Text-and-Image Generation Assessment</b></summary>

* **Authors:** Dongping Chen, Ruoxi Chen, Shu Pu, Zhaoyi Liu, Yanru Wu, Caixi Chen, Benlin Liu, Yue Huang, Yao Wan, Pan Zhou, Ranjay Krishna
* **arXiv ID:** 2411.17188
* **One-liner:** Presents evaluation framework for interleaved text-and-image generation consistency.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2411.17188) | [[PDF]](https://arxiv.org/pdf/2411.17188)

> **Core Innovation**
> Scene graph-based assessment of multi-modal content coherence and accuracy.

<details>
    <summary>Abstract</summary>
    Many real-world user queries (e.g. &#34;How do to make egg fried rice?&#34;) could benefit from systems capable of generating responses with both textual steps with accompanying images, similar to a cookbook. Models designed to generate interleaved text and images face challenges in ensuring consistency within and across these modalities. To address these challenges, we present ISG, a comprehensive evaluation framework for interleaved text-and-image generation. ISG leverages a scene graph structure to capture relationships between text and image blocks, evaluating responses on four levels of granularity: holistic, structural, block-level, and image-specific. This multi-tiered evaluation allows for a nuanced assessment of consistency, coherence, and accuracy, and provides interpretable question-answer feedback. In conjunction with ISG, we introduce a benchmark, ISG-Bench, encompassing 1,150 samples across 8 categories and 21 subcategories. This benchmark dataset includes complex language-vision dependencies and golden answers to evaluate models effectively on vision-centric tasks such as style transfer, a challenging area for current models. Using ISG-Bench, we demonstrate that recent unified vision-language models perform poorly on generating interleaved content. While compositional approaches that combine separate language and image models show a 111% improvement over unified models at the holistic level, their performance remains suboptimal at both block and image levels. To facilitate future work, we develop ISG-Agent, a baseline agent employing a &#34;plan-execute-refine&#34; pipeline to invoke tools, achieving a 122% performance improvement.
</details>

<details>
    <summary>Key points</summary>
    * Evaluates at holistic, structural, block-level, and image-specific granularities
    * Introduces ISG-Bench benchmark with diverse categories
    * Develops ISG-Agent with plan-execute-refine pipeline
</details>
</details>

---


<details>
<summary><b> Reward Incremental Learning in Text-to-Image Generation</b></summary>

* **Authors:** Maorong Wang, Jiafeng Mao, Xueting Wang, Toshihiko Yamasaki
* **arXiv ID:** 2411.17310
* **One-liner:** Addresses catastrophic forgetting in diffusion model fine-tuning for multiple rewards.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2411.17310) | [[PDF]](https://arxiv.org/pdf/2411.17310)

> **Core Innovation**
> Reward Incremental Learning method for stable adaptation to sequential objectives.

<details>
    <summary>Abstract</summary>
    The recent success of denoising diffusion models has significantly advanced text-to-image generation. While these large-scale pretrained models show excellent performance in general image synthesis, downstream objectives often require fine-tuning to meet specific criteria such as aesthetics or human preference. Reward gradient-based strategies are promising in this context, yet existing methods are limited to single-reward tasks, restricting their applicability in real-world scenarios that demand adapting to multiple objectives introduced incrementally over time. In this paper, we first define this more realistic and unexplored problem, termed Reward Incremental Learning (RIL), where models are desired to adapt to multiple downstream objectives incrementally. Additionally, while the models adapt to the ever-emerging new objectives, we observe a unique form of catastrophic forgetting in diffusion model fine-tuning, affecting both metric-wise and visual structure-wise image quality. To address this catastrophic forgetting challenge, we propose Reward Incremental Distillation (RID), a method that mitigates forgetting with minimal computational overhead, enabling stable performance across sequential reward tasks. The experimental results demonstrate the efficacy of RID in achieving consistent, high-quality generation in RIL scenarios. The source code of our work will be publicly available upon acceptance.
</details>

<details>
    <summary>Key points</summary>
    * Defines Reward Incremental Learning (RIL) problem
    * Proposes Reward Incremental Distillation (RID) to mitigate forgetting
    * Ensures consistent high-quality generation across tasks
</details>
</details>

---


<details>
<summary><b> Type-R: Automatically Retouching Typos for Text-to-Image Generation</b></summary>

* **Authors:** Wataru Shimoda, Naoto Inoue, Daichi Haraguchi, Hayato Mitani, Seiichi Uchida, Kota Yamaguchi
* **arXiv ID:** 2411.18159
* **One-liner:** Improves text rendering accuracy in generated images through post-processing.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2411.18159) | [[PDF]](https://arxiv.org/pdf/2411.18159)

> **Core Innovation**
> Retouching pipeline for correcting typographical errors in T2I outputs.

<details>
    <summary>Abstract</summary>
    While recent text-to-image models can generate photorealistic images from text prompts that reflect detailed instructions, they still face significant challenges in accurately rendering words in the image. In this paper, we propose to retouch erroneous text renderings in the post-processing pipeline. Our approach, called Type-R, identifies typographical errors in the generated image, erases the erroneous text, regenerates text boxes for missing words, and finally corrects typos in the rendered words. Through extensive experiments, we show that Type-R, in combination with the latest text-to-image models such as Stable Diffusion or Flux, achieves the highest text rendering accuracy while maintaining image quality and also outperforms text-focused generation baselines in terms of balancing text accuracy and image quality.
</details>

<details>
    <summary>Key points</summary>
    * Identifies and erases erroneous text
    * Regenerates text boxes for missing words
    * Corrects typos while maintaining image quality
</details>
</details>

---


<details>
<summary><b> Enhancing MMDiT-Based Text-to-Image Models for Similar Subject Generation</b></summary>

* **Authors:** Tianyi Wei, Dongdong Chen, Yifan Zhou, Xingang Pan
* **arXiv ID:** 2411.18301
* **One-liner:** Enhanced MMDiT by mitigating subject neglect in multi-subject prompts.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2411.18301) | [[PDF]](https://arxiv.org/pdf/2411.18301)

> **Core Innovation**
> Proposed test-time optimization with loss functions and strategies to address ambiguities in MMDiT.

<details>
    <summary>Abstract</summary>
    Representing the cutting-edge technique of text-to-image models, the latest Multimodal Diffusion Transformer (MMDiT) largely mitigates many generation issues existing in previous models. However, we discover that it still suffers from subject neglect or mixing when the input text prompt contains multiple subjects of similar semantics or appearance. We identify three possible ambiguities within the MMDiT architecture that cause this problem: Inter-block Ambiguity, Text Encoder Ambiguity, and Semantic Ambiguity. To address these issues, we propose to repair the ambiguous latent on-the-fly by test-time optimization at early denoising steps. In detail, we design three loss functions: Block Alignment Loss, Text Encoder Alignment Loss, and Overlap Loss, each tailored to mitigate these ambiguities. Despite significant improvements, we observe that semantic ambiguity persists when generating multiple similar subjects, as the guidance provided by overlap loss is not explicit enough. Therefore, we further propose Overlap Online Detection and Back-to-Start Sampling Strategy to alleviate the problem. Experimental results on a newly constructed challenging dataset of similar subjects validate the effectiveness of our approach, showing superior generation quality and much higher success rates over existing methods. Our code will be available at <a href="https://github.com/wtybest/EnMMDiT" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Identified Inter-block, Text Encoder, and Semantic Ambiguities
    * Designed Block Alignment Loss, Text Encoder Alignment Loss, and Overlap Loss
    * Introduced Overlap Online Detection and Back-to-Start Sampling Strategy
</details>
</details>

---


<details>
<summary><b> All Seeds Are Not Equal: Enhancing Compositional Text-to-Image Generation with Reliable Random Seeds</b></summary>

* **Authors:** Shuangqi Li, Hieu Le, Jingyi Xu, Mathieu Salzmann
* **arXiv ID:** 2411.18810
* **One-liner:** Improved compositional ability of text-to-image models by leveraging initial noise.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2411.18810) | [[PDF]](https://arxiv.org/pdf/2411.18810)

> **Core Innovation**
> Mined reliable initial noise cases to create a training set for fine-tuning.

<details>
    <summary>Abstract</summary>
    Text-to-image diffusion models have demonstrated remarkable capability in generating realistic images from arbitrary text prompts. However, they often produce inconsistent results for compositional prompts such as &#34;two dogs&#34; or &#34;a penguin on the right of a bowl&#34;. Understanding these inconsistencies is crucial for reliable image generation. In this paper, we highlight the significant role of initial noise in these inconsistencies, where certain noise patterns are more reliable for compositional prompts than others. Our analyses reveal that different initial random seeds tend to guide the model to place objects in distinct image areas, potentially adhering to specific patterns of camera angles and image composition associated with the seed. To improve the model&#39;s compositional ability, we propose a method for mining these reliable cases, resulting in a curated training set of generated images without requiring any manual annotation. By fine-tuning text-to-image models on these generated images, we significantly enhance their compositional capabilities. For numerical composition, we observe relative increases of 29.3% and 19.5% for Stable Diffusion and PixArt-{\alpha}, respectively. Spatial composition sees even larger gains, with 60.7% for Stable Diffusion and 21.1% for PixArt-{\alpha}.
</details>

<details>
    <summary>Key points</summary>
    * Analyzed role of initial noise in object placement inconsistencies
    * Curated training set from generated images without manual annotation
    * Fine-tuned models to enhance numerical and spatial composition
</details>
</details>

---


<details>
<summary><b> Switti: Designing Scale-Wise Transformers for Text-to-Image Synthesis</b></summary>

* **Authors:** Anton Voronov, Denis Kuznedelev, Mikhail Khoroshikh, Valentin Khrulkov, Dmitry Baranchuk
* **arXiv ID:** 2412.01819
* **One-liner:** Developed Switti, a fast and high-quality scale-wise transformer for T2I generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2412.01819) | [[PDF]](https://arxiv.org/pdf/2412.01819)

> **Core Innovation**
> Introduced non-causal architecture and optimized guidance for efficiency and quality.

<details>
    <summary>Abstract</summary>
    This work presents Switti, a scale-wise transformer for text-to-image generation. We start by adapting an existing next-scale prediction autoregressive (AR) architecture to T2I generation, investigating and mitigating training stability issues in the process. Next, we argue that scale-wise transformers do not require causality and propose a non-causal counterpart facilitating ~21% faster sampling and lower memory usage while also achieving slightly better generation quality. Furthermore, we reveal that classifier-free guidance at high-resolution scales is often unnecessary and can even degrade performance. By disabling guidance at these scales, we achieve an additional sampling acceleration of ~32% and improve the generation of fine-grained details. Extensive human preference studies and automated evaluations show that Switti outperforms existing T2I AR models and competes with state-of-the-art T2I diffusion models while being up to 7x faster.
</details>

<details>
    <summary>Key points</summary>
    * Adapted next-scale prediction AR architecture to T2I
    * Proposed non-causal transformer for faster sampling and lower memory
    * Disabled classifier-free guidance at high-res scales to improve details
</details>
</details>

---


<details>
<summary><b> Cross-Attention Head Position Patterns Can Align with Human Visual Concepts in Text-to-Image Generative Models</b></summary>

* **Authors:** Jungwon Park, Jungmin Ko, Dongnam Byun, Jangwon Suh, Wonjong Rhee
* **arXiv ID:** 2412.02237
* **One-liner:** Advanced interpretability of cross-attention layers in diffusion models.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2412.02237) | [[PDF]](https://arxiv.org/pdf/2412.02237)

> **Core Innovation**
> Introduced Head Relevance Vectors for concept alignment and control.

<details>
    <summary>Abstract</summary>
    Recent text-to-image diffusion models leverage cross-attention layers, which have been effectively utilized to enhance a range of visual generative tasks. However, our understanding of cross-attention layers remains somewhat limited. In this study, we introduce a mechanistic interpretability approach for diffusion models by constructing Head Relevance Vectors (HRVs) that align with human-specified visual concepts. An HRV for a given visual concept has a length equal to the total number of cross-attention heads, with each element representing the importance of the corresponding head for the given visual concept. To validate HRVs as interpretable features, we develop an ordered weakening analysis that demonstrates their effectiveness. Furthermore, we propose concept strengthening and concept adjusting methods and apply them to enhance three visual generative tasks. Our results show that HRVs can reduce misinterpretations of polysemous words in image generation, successfully modify five challenging attributes in image editing, and mitigate catastrophic neglect in multi-concept generation. Overall, our work provides an advancement in understanding cross-attention layers and introduces new approaches for fine-controlling these layers at the head level.
</details>

<details>
    <summary>Key points</summary>
    * Constructed HRVs for visual concept importance
    * Developed ordered weakening analysis for validation
    * Applied concept strengthening and adjusting to generative tasks
</details>
</details>

---


<details>
<summary><b> DynamicControl: Adaptive Condition Selection for Improved Text-to-Image Generation</b></summary>

* **Authors:** Qingdong He, Jinlong Peng, Pengcheng Xu, Boyuan Jiang, Xiaobin Hu, Donghao Luo, Yong Liu, Yabiao Wang, Chengjie Wang, Xiangtai Li, Jiangning Zhang
* **arXiv ID:** 2412.03255
* **One-liner:** Proposed DynamicControl for adaptive multi-condition T2I generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2412.03255) | [[PDF]](https://arxiv.org/pdf/2412.03255)

> **Core Innovation**
> Integrated double-cycle controller and MLLM for condition optimization.

<details>
    <summary>Abstract</summary>
    To enhance the controllability of text-to-image diffusion models, current ControlNet-like models have explored various control signals to dictate image attributes. However, existing methods either handle conditions inefficiently or use a fixed number of conditions, which does not fully address the complexity of multiple conditions and their potential conflicts. This underscores the need for innovative approaches to manage multiple conditions effectively for more reliable and detailed image synthesis. To address this issue, we propose a novel framework, DynamicControl, which supports dynamic combinations of diverse control signals, allowing adaptive selection of different numbers and types of conditions. Our approach begins with a double-cycle controller that generates an initial real score sorting for all input conditions by leveraging pre-trained conditional generation models and discriminative models. This controller evaluates the similarity between extracted conditions and input conditions, as well as the pixel-level similarity with the source image. Then, we integrate a Multimodal Large Language Model (MLLM) to build an efficient condition evaluator. This evaluator optimizes the ordering of conditions based on the double-cycle controller&#39;s score ranking. Our method jointly optimizes MLLMs and diffusion models, utilizing MLLMs&#39; reasoning capabilities to facilitate multi-condition text-to-image (T2I) tasks. The final sorted conditions are fed into a parallel multi-control adapter, which learns feature maps from dynamic visual conditions and integrates them to modulate ControlNet, thereby enhancing control over generated images. Through both quantitative and qualitative comparisons, DynamicControl demonstrates its superiority over existing methods in terms of controllability, generation quality and composability under various conditional controls.
</details>

<details>
    <summary>Key points</summary>
    * Used double-cycle controller for initial condition scoring
    * Employed MLLM to refine condition ordering
    * Applied parallel multi-control adapter for feature integration
</details>
</details>

---


<details>
<summary><b> Safeguarding Text-to-Image Generation via Inference-Time Prompt-Noise Optimization</b></summary>

* **Authors:** Jiangweizhi Peng, Zhiwei Tang, Gaowen Liu, Charles Fleming, Mingyi Hong
* **arXiv ID:** 2412.03876
* **One-liner:** Introduced PNO to prevent unsafe image generation without training.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2412.03876) | [[PDF]](https://arxiv.org/pdf/2412.03876)

> **Core Innovation**
> Optimized prompt embedding and noise trajectory for safety and alignment.

<details>
    <summary>Abstract</summary>
    Text-to-Image (T2I) diffusion models are widely recognized for their ability to generate high-quality and diverse images based on text prompts. However, despite recent advances, these models are still prone to generating unsafe images containing sensitive or inappropriate content, which can be harmful to users. Current efforts to prevent inappropriate image generation for diffusion models are easy to bypass and vulnerable to adversarial attacks. How to ensure that T2I models align with specific safety goals remains a significant challenge. In this work, we propose a novel, training-free approach, called Prompt-Noise Optimization (PNO), to mitigate unsafe image generation. Our method introduces a novel optimization framework that leverages both the continuous prompt embedding and the injected noise trajectory in the sampling process to generate safe images. Extensive numerical results demonstrate that our framework achieves state-of-the-art performance in suppressing toxic image generations and demonstrates robustness to adversarial attacks, without needing to tune the model parameters. Furthermore, compared with existing methods, PNO uses comparable generation time while offering the best tradeoff between the conflicting goals of safe generation and prompt-image alignment.
</details>

<details>
    <summary>Key points</summary>
    * Leveraged continuous prompt embedding and noise injection
    * Achieved state-of-the-art performance in toxic image suppression
    * Maintained robustness to adversarial attacks
</details>
</details>

---


<details>
<summary><b> Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis</b></summary>

* **Authors:** Jian Han, Jinlai Liu, Yi Jiang, Bin Yan, Yuqi Zhang, Zehuan Yuan, Bingyue Peng, Xiaobing Liu
* **arXiv ID:** 2412.04431
* **One-liner:** Created Infinity, a fast and high-quality bitwise autoregressive T2I model.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2412.04431) | [[PDF]](https://arxiv.org/pdf/2412.04431)

> **Core Innovation**
> Scaled tokenizer and transformer for improved generation and speed.

<details>
    <summary>Abstract</summary>
    We present Infinity, a Bitwise Visual AutoRegressive Modeling capable of generating high-resolution, photorealistic images following language instruction. Infinity redefines visual autoregressive model under a bitwise token prediction framework with an infinite-vocabulary tokenizer &amp; classifier and bitwise self-correction mechanism, remarkably improving the generation capacity and details. By theoretically scaling the tokenizer vocabulary size to infinity and concurrently scaling the transformer size, our method significantly unleashes powerful scaling capabilities compared to vanilla VAR. Infinity sets a new record for autoregressive text-to-image models, outperforming top-tier diffusion models like SD3-Medium and SDXL. Notably, Infinity surpasses SD3-Medium by improving the GenEval benchmark score from 0.62 to 0.73 and the ImageReward benchmark score from 0.87 to 0.96, achieving a win rate of 66%. Without extra optimization, Infinity generates a high-quality 1024x1024 image in 0.8 seconds, making it 2.6x faster than SD3-Medium and establishing it as the fastest text-to-image model. Models and codes will be released to promote further exploration of Infinity for visual generation and unified tokenizer modeling.
</details>

<details>
    <summary>Key points</summary>
    * Implemented bitwise token prediction with infinite vocabulary
    * Used bitwise self-correction mechanism
    * Achieved record speeds and benchmark scores
</details>
</details>

---


<details>
<summary><b> LayerFusion: Harmonized Multi-Layer Text-to-Image Generation with Generative Priors</b></summary>

* **Authors:** Yusuf Dalva, Yijun Li, Qing Liu, Nanxuan Zhao, Jianming Zhang, Zhe Lin, Pinar Yanardag
* **arXiv ID:** 2412.04460
* **One-liner:** Developed a pipeline for layered content generation with LDMs.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2412.04460) | [[PDF]](https://arxiv.org/pdf/2412.04460)

> **Core Innovation**
> Enabled harmonized generation of foreground and background layers.

<details>
    <summary>Abstract</summary>
    Large-scale diffusion models have achieved remarkable success in generating high-quality images from textual descriptions, gaining popularity across various applications. However, the generation of layered content, such as transparent images with foreground and background layers, remains an under-explored area. Layered content generation is crucial for creative workflows in fields like graphic design, animation, and digital art, where layer-based approaches are fundamental for flexible editing and composition. In this paper, we propose a novel image generation pipeline based on Latent Diffusion Models (LDMs) that generates images with two layers: a foreground layer (RGBA) with transparency information and a background layer (RGB). Unlike existing methods that generate these layers sequentially, our approach introduces a harmonized generation mechanism that enables dynamic interactions between the layers for more coherent outputs. We demonstrate the effectiveness of our method through extensive qualitative and quantitative experiments, showing significant improvements in visual coherence, image quality, and layer consistency compared to baseline methods.
</details>

<details>
    <summary>Key points</summary>
    * Generated RGBA foreground and RGB background layers
    * Introduced dynamic interactions between layers
    * Improved visual coherence and layer consistency
</details>
</details>

---


<details>
<summary><b> Proactive Agents for Multi-Turn Text-to-Image Generation Under Uncertainty</b></summary>

* **Authors:** Meera Hahn, Wenjun Zeng, Nithish Kannen, Rich Galt, Kartikeya Badola, Been Kim, Zi Wang
* **arXiv ID:** 2412.06771
* **One-liner:** Proposed proactive T2I agents to align user intent through clarification.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2412.06771) | [[PDF]](https://arxiv.org/pdf/2412.06771)

> **Core Innovation**
> Used belief graphs and question-asking for better alignment.

<details>
    <summary>Abstract</summary>
    User prompts for generative AI models are often underspecified, leading to a misalignment between the user intent and models&#39; understanding. As a result, users commonly have to painstakingly refine their prompts. We study this alignment problem in text-to-image (T2I) generation and propose a prototype for proactive T2I agents equipped with an interface to (1) actively ask clarification questions when uncertain, and (2) present their uncertainty about user intent as an understandable and editable belief graph. We build simple prototypes for such agents and propose a new scalable and automated evaluation approach using two agents, one with a ground truth intent (an image) while the other tries to ask as few questions as possible to align with the ground truth. We experiment over three image-text datasets: ImageInWords (Garg et al., 2024), COCO (Lin et al., 2014) and DesignBench, a benchmark we curated with strong artistic and design elements. Experiments over the three datasets demonstrate the proposed T2I agents&#39; ability to ask informative questions and elicit crucial information to achieve successful alignment with at least 2 times higher VQAScore (Lin et al., 2024) than the standard T2I generation. Moreover, we conducted human studies and observed that at least 90% of human subjects found these agents and their belief graphs helpful for their T2I workflow, highlighting the effectiveness of our approach. Code and DesignBench can be found at <a href="https://github.com/google-deepmind/proactive_t2i_agents" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Built agents that ask clarification questions
    * Represented uncertainty as editable belief graphs
    * Achieved higher alignment scores in evaluations
</details>
</details>

---


<details>
<summary><b> Boosting Alignment for Post-Unlearning Text-to-Image Generative Models</b></summary>

* **Authors:** Myeongseob Ko, Henry Li, Zhun Wang, Jonathan Patsenker, Jiachen T. Wang, Qinbin Li, Ming Jin, Dawn Song, Ruoxi Jia
* **arXiv ID:** 2412.07808
* **One-liner:** Enhanced machine unlearning for diffusion models to remove harmful content.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2412.07808) | [[PDF]](https://arxiv.org/pdf/2412.07808)

> **Core Innovation**
> Optimized model updates to balance unlearning and alignment.

<details>
    <summary>Abstract</summary>
    Large-scale generative models have shown impressive image-generation capabilities, propelled by massive data. However, this often inadvertently leads to the generation of harmful or inappropriate content and raises copyright concerns. Driven by these concerns, machine unlearning has become crucial to effectively purge undesirable knowledge from models. While existing literature has studied various unlearning techniques, these often suffer from either poor unlearning quality or degradation in text-image alignment after unlearning, due to the competitive nature of these objectives. To address these challenges, we propose a framework that seeks an optimal model update at each unlearning iteration, ensuring monotonic improvement on both objectives. We further derive the characterization of such an update.
<br>In addition, we design procedures to strategically diversify the unlearning and remaining datasets to boost performance improvement. Our evaluation demonstrates that our method effectively removes target classes from recent diffusion-based generative models and concepts from stable diffusion models while maintaining close alignment with the models&#39; original trained states, thus outperforming state-of-the-art baselines. Our code will be made available at <a href="https://github.com/reds-lab/Restricted_gradient_diversity_unlearning.git" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Sought optimal updates for monotonic improvement
    * Diversified unlearning and remaining datasets
    * Maintained text-image alignment after unlearning
</details>
</details>

---


<details>
<summary><b> Fast Prompt Alignment for Text-to-Image Generation</b></summary>

* **Authors:** Khalil Mrini, Hanlin Lu, Linjie Yang, Weilin Huang, Heng Wang
* **arXiv ID:** 2412.08639
* **One-liner:** Introduced Fast Prompt Alignment (FPA) for efficient text-to-image alignment.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2412.08639) | [[PDF]](https://arxiv.org/pdf/2412.08639)

> **Core Innovation**
> Developed a one-pass prompt optimization framework using LLMs for paraphrasing, reducing computational overhead while maintaining alignment fidelity.

<details>
    <summary>Abstract</summary>
    Text-to-image generation has advanced rapidly, yet aligning complex textual prompts with generated visuals remains challenging, especially with intricate object relationships and fine-grained details. This paper introduces Fast Prompt Alignment (FPA), a prompt optimization framework that leverages a one-pass approach, enhancing text-to-image alignment efficiency without the iterative overhead typical of current methods like OPT2I. FPA uses large language models (LLMs) for single-iteration prompt paraphrasing, followed by fine-tuning or in-context learning with optimized prompts to enable real-time inference, reducing computational demands while preserving alignment fidelity. Extensive evaluations on the COCO Captions and PartiPrompts datasets demonstrate that FPA achieves competitive text-image alignment scores at a fraction of the processing time, as validated through both automated metrics (TIFA, VQA) and human evaluation. A human study with expert annotators further reveals a strong correlation between human alignment judgments and automated scores, underscoring the robustness of FPA&#39;s improvements. The proposed method showcases a scalable, efficient alternative to iterative prompt optimization, enabling broader applicability in real-time, high-demand settings. The codebase is provided to facilitate further research: <a href="https://github.com/tiktok/fast_prompt_alignment" rel="external noopener nofollow" class="link-external link-https">this https URL</a>
</details>

<details>
    <summary>Key points</summary>
    * Leverages large language models for single-iteration prompt paraphrasing
    * Uses fine-tuning or in-context learning with optimized prompts
    * Achieves competitive alignment scores with reduced processing time
</details>
</details>

---


<details>
<summary><b> Preference Adaptive and Sequential Text-to-Image Generation</b></summary>

* **Authors:** Ofir Nabati, Guy Tennenholtz, ChihWei Hsu, Moonkyung Ryu, Deepak Ramachandran, Yinlam Chow, Xiang Li, Craig Boutilier
* **arXiv ID:** 2412.10419
* **One-liner:** Designed PASTA, an RL-based agent for interactive text-to-image generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2412.10419) | [[PDF]](https://arxiv.org/pdf/2412.10419)

> **Core Innovation**
> Implemented a reinforcement learning approach with adaptive prompt expansions to improve image sets through multi-turn interactions.

<details>
    <summary>Abstract</summary>
    We address the problem of interactive text-to-image (T2I) generation, designing a reinforcement learning (RL) agent which iteratively improves a set of generated images for a user through a sequence of prompt expansions. Using human raters, we create a novel dataset of sequential preferences, which we leverage, together with large-scale open-source (non-sequential) datasets. We construct user-preference and user-choice models using an EM strategy and identify varying user preference types. We then leverage a large multimodal language model (LMM) and a value-based RL approach to suggest an adaptive and diverse slate of prompt expansions to the user. Our Preference Adaptive and Sequential Text-to-image Agent (PASTA) extends T2I models with adaptive multi-turn capabilities, fostering collaborative co-creation and addressing uncertainty or underspecification in a user&#39;s intent. We evaluate PASTA using human raters, showing significant improvement compared to baseline methods. We also open-source our sequential rater dataset and simulated user-rater interactions to support future research in user-centric multi-turn T2I systems.
</details>

<details>
    <summary>Key points</summary>
    * Uses RL agent for iterative prompt expansions
    * Creates and leverages a sequential preference dataset
    * Integrates LMM and value-based RL for adaptive suggestions
</details>
</details>

---


<details>
<summary><b> AlignGuard: Scalable Safety Alignment for Text-to-Image Generation</b></summary>

* **Authors:** Runtao Liu, I Chieh Chen, Jindong Gu, Jipeng Zhang, Renjie Pi, Qifeng Chen, Philip Torr, Ashkan Khakzar, Fabio Pizzati
* **arXiv ID:** 2412.10493
* **One-liner:** Proposed AlignGuard for safety alignment in text-to-image models.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2412.10493) | [[PDF]](https://arxiv.org/pdf/2412.10493)

> **Core Innovation**
> Applied Direct Preference Optimization with synthetic datasets to remove harmful concepts using LoRA experts.

<details>
    <summary>Abstract</summary>
    Text-to-image (T2I) models are widespread, but their limited safety guardrails expose end users to harmful content and potentially allow for model misuse. Current safety measures are typically limited to text-based filtering or concept removal strategies, able to remove just a few concepts from the model&#39;s generative capabilities. In this work, we introduce AlignGuard, a method for safety alignment of T2I models. We enable the application of Direct Preference Optimization (DPO) for safety purposes in T2I models by synthetically generating a dataset of harmful and safe image-text pairs, which we call CoProV2. Using a custom DPO strategy and this dataset, we train safety experts, in the form of low-rank adaptation (LoRA) matrices, able to guide the generation process away from specific safety-related concepts. Then, we merge the experts into a single LoRA using a novel merging strategy for optimal scaling performance. This expert-based approach enables scalability, allowing us to remove 7x more harmful concepts from T2I models compared to baselines. AlignGuard consistently outperforms the state-of-the-art on many benchmarks and establishes new practices for safety alignment in T2I networks. Code and data will be shared at <a href="https://safetydpo.github.io/" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Generates synthetic dataset of harmful and safe image-text pairs
    * Uses DPO strategy to train safety experts
    * Merges LoRA experts for scalable concept removal
</details>
</details>

---


<details>
<summary><b> Efficient Scaling of Diffusion Transformers for Text-to-Image Generation</b></summary>

* **Authors:** Hao Li, Shamit Lal, Zhiheng Li, Yusheng Xie, Ying Wang, Yang Zou, Orchid Majumder, R. Manmatha, Zhuowen Tu, Stefano Ermon, Stefano Soatto, Ashwin Swaminathan
* **arXiv ID:** 2412.12391
* **One-liner:** Empirically studied scaling properties of Diffusion Transformers for text-to-image generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2412.12391) | [[PDF]](https://arxiv.org/pdf/2412.12391)

> **Core Innovation**
> Identified U-ViT as a scalable DiT model with better performance than variants, and explored data scaling effects.

<details>
    <summary>Abstract</summary>
    We empirically study the scaling properties of various Diffusion Transformers (DiTs) for text-to-image generation by performing extensive and rigorous ablations, including training scaled DiTs ranging from 0.3B upto 8B parameters on datasets up to 600M images. We find that U-ViT, a pure self-attention based DiT model provides a simpler design and scales more effectively in comparison with cross-attention based DiT variants, which allows straightforward expansion for extra conditions and other modalities. We identify a 2.3B U-ViT model can get better performance than SDXL UNet and other DiT variants in controlled setting. On the data scaling side, we investigate how increasing dataset size and enhanced long caption improve the text-image alignment performance and the learning efficiency.
</details>

<details>
    <summary>Key points</summary>
    * Trains scaled DiTs from 0.3B to 8B parameters
    * Finds U-ViT scales more effectively than cross-attention variants
    * Investigates dataset size and caption enhancements for alignment
</details>
</details>

---


<details>
<summary><b> ArtAug: Enhancing Text-to-Image Generation through Synthesis-Understanding Interaction</b></summary>

* **Authors:** Zhongjie Duan, Qianyi Zhao, Cen Chen, Daoyuan Chen, Wenmeng Zhou, Yaliang Li, Yingda Chen
* **arXiv ID:** 2412.12888
* **One-liner:** Introduced ArtAug for enhancing text-to-image models via model interactions.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2412.12888) | [[PDF]](https://arxiv.org/pdf/2412.12888)

> **Core Innovation**
> Leveraged image understanding models to provide aesthetic suggestions and iteratively fuse enhancements into synthesis models.

<details>
    <summary>Abstract</summary>
    The emergence of diffusion models has significantly advanced image synthesis. The recent studies of model interaction and self-corrective reasoning approach in large language models offer new insights for enhancing text-to-image models. Inspired by these studies, we propose a novel method called ArtAug for enhancing text-to-image models in this paper. To the best of our knowledge, ArtAug is the first one that improves image synthesis models via model interactions with understanding models. In the interactions, we leverage human preferences implicitly learned by image understanding models to provide fine-grained suggestions for image synthesis models. The interactions can modify the image content to make it aesthetically pleasing, such as adjusting exposure, changing shooting angles, and adding atmospheric effects. The enhancements brought by the interaction are iteratively fused into the synthesis model itself through an additional enhancement module. This enables the synthesis model to directly produce aesthetically pleasing images without any extra computational cost. In the experiments, we train the ArtAug enhancement module on existing text-to-image models. Various evaluation metrics consistently demonstrate that ArtAug enhances the generative capabilities of text-to-image models without incurring additional computational costs. The source code and models will be released publicly.
</details>

<details>
    <summary>Key points</summary>
    * Uses model interactions with understanding models for fine-grained suggestions
    * Iteratively fuses enhancements through an additional module
    * Enables direct generation of aesthetically pleasing images without extra cost
</details>
</details>

---


<details>
<summary><b> GALOT: Generative Active Learning via Optimizable Zero-shot Text-to-image Generation</b></summary>

* **Authors:** Hanbin Hong, Shenao Yan, Shuya Feng, Yan Yan, Yuan Hong
* **arXiv ID:** 2412.16227
* **One-liner:** Integrated zero-shot text-to-image synthesis with active learning for efficient model training.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2412.16227) | [[PDF]](https://arxiv.org/pdf/2412.16227)

> **Core Innovation**
> Developed a framework using AL criteria to optimize text inputs for generating informative synthetic datasets.

<details>
    <summary>Abstract</summary>
    Active Learning (AL) represents a crucial methodology within machine learning, emphasizing the identification and utilization of the most informative samples for efficient model training. However, a significant challenge of AL is its dependence on the limited labeled data samples and data distribution, resulting in limited performance. To address this limitation, this paper integrates the zero-shot text-to-image (T2I) synthesis and active learning by designing a novel framework that can efficiently train a machine learning (ML) model sorely using the text description. Specifically, we leverage the AL criteria to optimize the text inputs for generating more informative and diverse data samples, annotated by the pseudo-label crafted from text, then served as a synthetic dataset for active learning. This approach reduces the cost of data collection and annotation while increasing the efficiency of model training by providing informative training samples, enabling a novel end-to-end ML task from text description to vision models. Through comprehensive evaluations, our framework demonstrates consistent and significant improvements over traditional AL methods.
</details>

<details>
    <summary>Key points</summary>
    * Leverages AL criteria to optimize text inputs for data generation
    * Uses pseudo-labels from text for annotation
    * Reduces data collection costs and increases training efficiency
</details>
</details>

---


<details>
<summary><b> Self-Corrected Flow Distillation for Consistent One-Step and Few-Step Text-to-Image Generation</b></summary>

* **Authors:** Quan Dao, Hao Phung, Trung Dao, Dimitris Metaxas, Anh Tran
* **arXiv ID:** 2412.16906
* **One-liner:** Introduced a self-corrected flow distillation method for generative models.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2412.16906) | [[PDF]](https://arxiv.org/pdf/2412.16906)

> **Core Innovation**
> Combined consistency models and adversarial training in flow-matching to achieve high-quality generation in few-step and one-step sampling.

<details>
    <summary>Abstract</summary>
    Flow matching has emerged as a promising framework for training generative models, demonstrating impressive empirical performance while offering relative ease of training compared to diffusion-based models. However, this method still requires numerous function evaluations in the sampling process. To address these limitations, we introduce a self-corrected flow distillation method that effectively integrates consistency models and adversarial training within the flow-matching framework. This work is a pioneer in achieving consistent generation quality in both few-step and one-step sampling. Our extensive experiments validate the effectiveness of our method, yielding superior results both quantitatively and qualitatively on CelebA-HQ and zero-shot benchmarks on the COCO dataset. Our implementation is released at <a href="https://github.com/VinAIResearch/SCFlow" rel="external noopener nofollow" class="link-external link-https">this https URL</a>
</details>

<details>
    <summary>Key points</summary>
    * Integrates consistency models and adversarial training in flow-matching
    * Achieves consistent generation quality in few-step and one-step sampling
    * Validated on CelebA-HQ and COCO datasets
</details>
</details>

---


<details>
<summary><b> Hierarchical Vision-Language Alignment for Text-to-Image Generation via Diffusion Models</b></summary>

* **Authors:** Emily Johnson, Noah Wilson
* **arXiv ID:** 2501.00917
* **One-liner:** Proposed VLAD model for improved text-to-image generation with dual-stream strategy.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2501.00917) | [[PDF]](https://arxiv.org/pdf/2501.00917)

> **Core Innovation**
> Utilized semantic alignment and hierarchical diffusion to enhance image quality and text rendering accuracy.

<details>
    <summary>Abstract</summary>
    Text-to-image generation has witnessed significant advancements with the integration of Large Vision-Language Models (LVLMs), yet challenges remain in aligning complex textual descriptions with high-quality, visually coherent images. This paper introduces the Vision-Language Aligned Diffusion (VLAD) model, a generative framework that addresses these challenges through a dual-stream strategy combining semantic alignment and hierarchical diffusion. VLAD utilizes a Contextual Composition Module (CCM) to decompose textual prompts into global and local representations, ensuring precise alignment with visual features. Furthermore, it incorporates a multi-stage diffusion process with hierarchical guidance to generate high-fidelity images. Experiments conducted on MARIO-Eval and INNOVATOR-Eval benchmarks demonstrate that VLAD significantly outperforms state-of-the-art methods in terms of image quality, semantic alignment, and text rendering accuracy. Human evaluations further validate the superior performance of VLAD, making it a promising approach for text-to-image generation in complex scenarios.
</details>

<details>
    <summary>Key points</summary>
    * Employs dual-stream strategy with semantic alignment and hierarchical diffusion
    * Uses Contextual Composition Module for prompt decomposition
    * Outperforms state-of-the-art in benchmarks and human evaluations
</details>
</details>

---


<details>
<summary><b> Evaluating Image Caption via Cycle-consistent Text-to-Image Generation</b></summary>

* **Authors:** Tianyu Cui, Jinbin Bai, Guo-Hua Wang, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, Ye Shi
* **arXiv ID:** 2501.03567
* **One-liner:** Introduced CAMScore, a cyclic reference-free evaluation metric for image captioning.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2501.03567) | [[PDF]](https://arxiv.org/pdf/2501.03567)

> **Core Innovation**
> Developed a framework using text-to-image generation to evaluate captions by comparing generated images with originals, avoiding modality gaps.

<details>
    <summary>Abstract</summary>
    Evaluating image captions typically relies on reference captions, which are costly to obtain and exhibit significant diversity and subjectivity. While reference-free evaluation metrics have been proposed, most focus on cross-modal evaluation between captions and images. Recent research has revealed that the modality gap generally exists in the representation of contrastive learning-based multi-modal systems, undermining the reliability of cross-modality metrics like CLIPScore. In this paper, we propose CAMScore, a cyclic reference-free automatic evaluation metric for image captioning models. To circumvent the aforementioned modality gap, CAMScore utilizes a text-to-image model to generate images from captions and subsequently evaluates these generated images against the original images. Furthermore, to provide fine-grained information for a more comprehensive evaluation, we design a three-level evaluation framework for CAMScore that encompasses pixel-level, semantic-level, and objective-level perspectives. Extensive experiment results across multiple benchmark datasets show that CAMScore achieves a superior correlation with human judgments compared to existing reference-based and reference-free metrics, demonstrating the effectiveness of the framework.
</details>

<details>
    <summary>Key points</summary>
    * Uses text-to-image model to generate images from captions
    * Evaluates generated images against original images
    * Incorporates pixel-level, semantic-level, and objective-level perspectives
</details>
</details>

---


<details>
<summary><b> Boosting Text-To-Image Generation via Multilingual Prompting in Large Multimodal Models</b></summary>

* **Authors:** Yongyu Mu, Hengyu Li, Junxin Wang, Xiaoxuan Zhou, Chenglong Wang, Yingfeng Luo, Qiaozhi He, Tong Xiao, Guocheng Chen, Jingbo Zhu
* **arXiv ID:** 2501.07086
* **One-liner:** Extended multilingual capabilities in text-to-image generation with PMT2I.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2501.07086) | [[PDF]](https://arxiv.org/pdf/2501.07086)

> **Core Innovation**
> Constructed parallel multilingual prompts to enhance comprehension and diversity in image generation using LMMs.

<details>
    <summary>Abstract</summary>
    Previous work on augmenting large multimodal models (LMMs) for text-to-image (T2I) generation has focused on enriching the input space of in-context learning (ICL). This includes providing a few demonstrations and optimizing image descriptions to be more detailed and logical. However, as demand for more complex and flexible image descriptions grows, enhancing comprehension of input text within the ICL paradigm remains a critical yet underexplored area. In this work, we extend this line of research by constructing parallel multilingual prompts aimed at harnessing the multilingual capabilities of LMMs. More specifically, we translate the input text into several languages and provide the models with both the original text and the translations. Experiments on two LMMs across 3 benchmarks show that our method, PMT2I, achieves superior performance in general, compositional, and fine-grained assessments, especially in human preference alignment. Additionally, with its advantage of generating more diverse images, PMT2I significantly outperforms baseline prompts when incorporated with reranking methods. Our code and parallel multilingual data can be found at <a href="https://github.com/takagi97/PMT2I" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Translates input text into multiple languages for prompts
    * Leverages multilingual capabilities of LMMs
    * Improves performance in compositional and fine-grained assessments
</details>
</details>

---


<details>
<summary><b> Democratizing Text-to-Image Masked Generative Models with Compact Text-Aware One-Dimensional Tokens</b></summary>

* **Authors:** Dongwon Kim, Ju He, Qihang Yu, Chenglin Yang, Xiaohui Shen, Suha Kwak, Liang-Chieh Chen
* **arXiv ID:** 2501.07730
* **One-liner:** Introduced TA-TiTok, an efficient image tokenizer that integrates text during decoding for improved performance.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2501.07730) | [[PDF]](https://arxiv.org/pdf/2501.07730)

> **Core Innovation**
> Developed a one-stage training process for image tokenization, eliminating complex distillation and enabling scalability.

<details>
    <summary>Abstract</summary>
    Image tokenizers form the foundation of modern text-to-image generative models but are notoriously difficult to train. Furthermore, most existing text-to-image models rely on large-scale, high-quality private datasets, making them challenging to replicate. In this work, we introduce Text-Aware Transformer-based 1-Dimensional Tokenizer (TA-TiTok), an efficient and powerful image tokenizer that can utilize either discrete or continuous 1-dimensional tokens. TA-TiTok uniquely integrates textual information during the tokenizer decoding stage (i.e., de-tokenization), accelerating convergence and enhancing performance. TA-TiTok also benefits from a simplified, yet effective, one-stage training process, eliminating the need for the complex two-stage distillation used in previous 1-dimensional tokenizers. This design allows for seamless scalability to large datasets. Building on this, we introduce a family of text-to-image Masked Generative Models (MaskGen), trained exclusively on open data while achieving comparable performance to models trained on private data. We aim to release both the efficient, strong TA-TiTok tokenizers and the open-data, open-weight MaskGen models to promote broader access and democratize the field of text-to-image masked generative models.
</details>

<details>
    <summary>Key points</summary>
    * Text-Aware Transformer-based 1-Dimensional Tokenizer (TA-TiTok)
    * Integration of textual information in decoding stage
    * Simplified one-stage training process
    * Masked Generative Models (MaskGen) trained on open data
</details>
</details>

---


<details>
<summary><b> SHYI: Action Support for Contrastive Learning in High-Fidelity Text-to-Image Generation</b></summary>

* **Authors:** Tianxiang Xia, Lin Xiao, Yannick Montorfani, Francesco Pavia, Enis Simsar, Thomas Hofmann
* **arXiv ID:** 2501.09055
* **One-liner:** Improved text-to-image generation fidelity for actions involving multiple objects using enhanced contrastive learning.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2501.09055) | [[PDF]](https://arxiv.org/pdf/2501.09055)

> **Core Innovation**
> Employed semantically hypergraphic contrastive adjacency learning and InteractDiffusion to amend action understanding.

<details>
    <summary>Abstract</summary>
    In this project, we address the issue of infidelity in text-to-image generation, particularly for actions involving multiple objects. For this we build on top of the CONFORM framework which uses Contrastive Learning to improve the accuracy of the generated image for multiple objects. However the depiction of actions which involves multiple different object has still large room for improvement. To improve, we employ semantically hypergraphic contrastive adjacency learning, a comprehension of enhanced contrastive structure and &#34;contrast but link&#34; technique. We further amend Stable Diffusion&#39;s understanding of actions by InteractDiffusion. As evaluation metrics we use image-text similarity CLIP and TIFA. In addition, we conducted a user study.
<br>Our method shows promising results even with verbs that Stable Diffusion understands mediocrely. We then provide future directions by analyzing the results.
<br>Our codebase can be found on polybox under the link: <a href="https://polybox.ethz.ch/index.php/s/dJm3SWyRohUrFxn" rel="external noopener nofollow" class="link-external link-https">this https URL</a>
</details>

<details>
    <summary>Key points</summary>
    * CONFORM framework with Contrastive Learning
    * Semantically hypergraphic contrastive adjacency learning
    * Contrast but link technique
    * InteractDiffusion for action amendment
</details>
</details>

---


<details>
<summary><b> IMAGINE-E: Image Generation Intelligence Evaluation of State-of-the-art Text-to-Image Models</b></summary>

* **Authors:** Jiayi Lei, Renrui Zhang, Xiangfei Hu, Weifeng Lin, Zhen Li, Wenjian Sun, Ruoyi Du, Le Zhuo, Zhongyu Li, Xinyue Li, Shitian Zhao, Ziyu Guo, Yiting Lu, Peng Gao, Hongsheng Li
* **arXiv ID:** 2501.13920
* **One-liner:** Developed IMAGINE-E, a comprehensive evaluation framework for text-to-image models across multiple domains.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2501.13920) | [[PDF]](https://arxiv.org/pdf/2501.13920)

> **Core Innovation**
> Assessed six models in structured output, realism, specific domains, challenging scenarios, and multi-style tasks.

<details>
    <summary>Abstract</summary>
    With the rapid development of diffusion models, text-to-image(T2I) models have made significant progress, showcasing impressive abilities in prompt following and image generation. Recently launched models such as FLUX.1 and Ideogram2.0, along with others like Dall-E3 and Stable Diffusion 3, have demonstrated exceptional performance across various complex tasks, raising questions about whether T2I models are moving towards general-purpose applicability. Beyond traditional image generation, these models exhibit capabilities across a range of fields, including controllable generation, image editing, video, audio, 3D, and motion generation, as well as computer vision tasks like semantic segmentation and depth estimation. However, current evaluation frameworks are insufficient to comprehensively assess these models&#39; performance across expanding domains. To thoroughly evaluate these models, we developed the IMAGINE-E and tested six prominent models: FLUX.1, Ideogram2.0, Midjourney, Dall-E3, Stable Diffusion 3, and Jimeng. Our evaluation is divided into five key domains: structured output generation, realism, and physical consistency, specific domain generation, challenging scenario generation, and multi-style creation tasks. This comprehensive assessment highlights each model&#39;s strengths and limitations, particularly the outstanding performance of FLUX.1 and Ideogram2.0 in structured and specific domain tasks, underscoring the expanding applications and potential of T2I models as foundational AI tools. This study provides valuable insights into the current state and future trajectory of T2I models as they evolve towards general-purpose usability. Evaluation scripts will be released at <a href="https://github.com/jylei16/Imagine-e" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * IMAGINE-E evaluation framework
    * Five key domains: structured output, realism, specific domain, challenging scenario, multi-style
    * Testing of models like FLUX.1 and Ideogram2.0
</details>
</details>

---


<details>
<summary><b> Text-to-Image Generation for Vocabulary Learning Using the Keyword Method</b></summary>

* **Authors:** Nuwan T. Attygalle, Matjaž Kljun, Aaron Quigley, Klen čOpič Pucihar, Jens Grubert, Verena Biener, Luis A. Leiva, Juri Yoneyama, Alice Toniolo, Angela Miguel, Hirokazu Kato, Maheshya Weerasinghe
* **arXiv ID:** 2501.17099
* **One-liner:** Enhanced vocabulary memorization by combining the keyword method with text-to-image generators.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2501.17099) | [[PDF]](https://arxiv.org/pdf/2501.17099)

> **Core Innovation**
> Externalized mental visual links into images, significantly improving memory retention in language learning.

<details>
    <summary>Abstract</summary>
    The &#39;keyword method&#39; is an effective technique for learning vocabulary of a foreign language. It involves creating a memorable visual link between what a word means and what its pronunciation in a foreign language sounds like in the learner&#39;s native language. However, these memorable visual links remain implicit in the people&#39;s mind and are not easy to remember for a large set of words. To enhance the memorisation and recall of the vocabulary, we developed an application that combines the keyword method with text-to-image generators to externalise the memorable visual links into visuals. These visuals represent additional stimuli during the memorisation process. To explore the effectiveness of this approach we first run a pilot study to investigate how difficult it is to externalise the descriptions of mental visualisations of memorable links, by asking participants to write them down. We used these descriptions as prompts for text-to-image generator (DALL-E2) to convert them into images and asked participants to select their favourites. Next, we compared different text-to-image generators (DALL-E2, Midjourney, Stable and Latent Diffusion) to evaluate the perceived quality of the generated images by each. Despite heterogeneous results, participants mostly preferred images generated by DALL-E2, which was used also for the final study. In this study, we investigated whether providing such images enhances the retention of vocabulary being learned, compared to the keyword method only. Our results indicate that people did not encounter difficulties describing their visualisations of memorable links and that providing corresponding images significantly improves memory retention.
</details>

<details>
    <summary>Key points</summary>
    * Keyword method combined with text-to-image generation
    * Pilot study on externalizing visualizations
    * Comparison of generators like DALL-E2
    * Final study on image-enhanced retention
</details>
</details>

---


<details>
<summary><b> TextAtlas5M: A Large-scale Dataset for Dense Text Image Generation</b></summary>

* **Authors:** Alex Jinpeng Wang, Dongxing Mao, Jiawei Zhang, Weiming Han, Zhuobai Dong, Linjie Li, Yiqi Lin, Zhengyuan Yang, Libo Qin, Fuwei Zhang, Lijuan Wang, Min Li
* **arXiv ID:** 2502.07870
* **One-liner:** Introduced TextAtlas5M, a dataset for evaluating long-text rendering in text-conditioned image generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2502.07870) | [[PDF]](https://arxiv.org/pdf/2502.07870)

> **Core Innovation**
> Curated TextAtlasEval benchmark, challenging advanced models and highlighting performance gaps.

<details>
    <summary>Abstract</summary>
    Text-conditioned image generation has gained significant attention in recent years and are processing increasingly longer and comprehensive text prompt. In everyday life, dense and intricate text appears in contexts like advertisements, infographics, and signage, where the integration of both text and visuals is essential for conveying complex information. However, despite these advances, the generation of images containing long-form text remains a persistent challenge, largely due to the limitations of existing datasets, which often focus on shorter and simpler text. To address this gap, we introduce TextAtlas5M, a novel dataset specifically designed to evaluate long-text rendering in text-conditioned image generation. Our dataset consists of 5 million long-text generated and collected images across diverse data types, enabling comprehensive evaluation of large-scale generative models on long-text image generation. We further curate 3000 human-improved test set TextAtlasEval across 3 data domains, establishing one of the most extensive benchmarks for text-conditioned generation. Evaluations suggest that the TextAtlasEval benchmarks present significant challenges even for the most advanced proprietary models (e.g. GPT4o with DallE-3), while their open-source counterparts show an even larger performance gap. These evidences position TextAtlas5M as a valuable dataset for training and evaluating future-generation text-conditioned image generation models.
</details>

<details>
    <summary>Key points</summary>
    * TextAtlas5M dataset with 5 million images
    * TextAtlasEval benchmark across 3 domains
    * Evaluation of long-text generation challenges
</details>
</details>

---


<details>
<summary><b> Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation</b></summary>

* **Authors:** Hoigi Seo, Wongi Jeong, Jae-sun Seo, Se Young Chun
* **arXiv ID:** 2502.08690
* **One-liner:** Proposed Skrr, a pruning strategy for text encoders in T2I models to reduce memory usage without performance loss.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2502.08690) | [[PDF]](https://arxiv.org/pdf/2502.08690)

> **Core Innovation**
> Selectively skips or reuses transformer layers, achieving state-of-the-art memory efficiency.

<details>
    <summary>Abstract</summary>
    Large-scale text encoders in text-to-image (T2I) diffusion models have demonstrated exceptional performance in generating high-quality images from textual prompts. Unlike denoising modules that rely on multiple iterative steps, text encoders require only a single forward pass to produce text embeddings. However, despite their minimal contribution to total inference time and floating-point operations (FLOPs), text encoders demand significantly higher memory usage, up to eight times more than denoising modules. To address this inefficiency, we propose Skip and Re-use layers (Skrr), a simple yet effective pruning strategy specifically designed for text encoders in T2I diffusion models. Skrr exploits the inherent redundancy in transformer blocks by selectively skipping or reusing certain layers in a manner tailored for T2I tasks, thereby reducing memory consumption without compromising performance. Extensive experiments demonstrate that Skrr maintains image quality comparable to the original model even under high sparsity levels, outperforming existing blockwise pruning methods. Furthermore, Skrr achieves state-of-the-art memory efficiency while preserving performance across multiple evaluation metrics, including the FID, CLIP, DreamSim, and GenEval scores.
</details>

<details>
    <summary>Key points</summary>
    * Skip and Re-use layers (Skrr) pruning
    * Exploitation of redundancy in transformer blocks
    * Evaluation with FID, CLIP, DreamSim, GenEval scores
</details>
</details>

---


<details>
<summary><b> FlexControl: Computation-Aware ControlNet with Differentiable Router for Text-to-Image Generation</b></summary>

* **Authors:** Zheng Fang, Lichuan Xiang, Xu Cai, Kaicheng Zhou, Hongkai Wen
* **arXiv ID:** 2502.10451
* **One-liner:** Introduced FlexControl, a framework for dynamic block selection in controlled diffusion models.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2502.10451) | [[PDF]](https://arxiv.org/pdf/2502.10451)

> **Core Innovation**
> Uses trainable gating and computation-aware loss to enhance adaptability and reduce computational overhead.

<details>
    <summary>Abstract</summary>
    ControlNet offers a powerful way to guide diffusion-based generative models, yet most implementations rely on ad-hoc heuristics to choose which network blocks to control-an approach that varies unpredictably with different tasks. To address this gap, we propose FlexControl, a novel framework that copies all diffusion blocks during training and employs a trainable gating mechanism to dynamically select which blocks to activate at each denoising step. With introducing a computation-aware loss, we can encourage control blocks only to activate when it benefit the generation quality. By eliminating manual block selection, FlexControl enhances adaptability across diverse tasks and streamlines the design pipeline, with computation-aware training loss in an end-to-end training manner. Through comprehensive experiments on both UNet (e.g., SD1.5) and DiT (e.g., SD3.0), we show that our method outperforms existing ControlNet variants in certain key aspects of interest. As evidenced by both quantitative and qualitative evaluations, FlexControl preserves or enhances image fidelity while also reducing computational overhead by selectively activating the most relevant blocks. These results underscore the potential of a flexible, data-driven approach for controlled diffusion and open new avenues for efficient generative model design. The code will soon be available at <a href="https://github.com/Anonymousuuser/FlexControl" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * FlexControl with trainable gating mechanism
    * Computation-aware loss for selective activation
    * Experiments on UNet and DiT architectures
</details>
</details>

---


<details>
<summary><b> REAL: Realism Evaluation of Text-to-Image Generation Models for Effective Data Augmentation</b></summary>

* **Authors:** Ran Li, Xiaomeng Jin, Heng ji
* **arXiv ID:** 2502.10663
* **One-liner:** Proposed REAL, an automatic evaluation framework for assessing realism in T2I outputs.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2502.10663) | [[PDF]](https://arxiv.org/pdf/2502.10663)

> **Core Innovation**
> Evaluates fine-grained attributes, unusual relationships, and styles, aligning with human judgement.

<details>
    <summary>Abstract</summary>
    Recent advancements in text-to-image (T2I) generation models have transformed the field. However, challenges persist in generating images that reflect demanding textual descriptions, especially for fine-grained details and unusual relationships. Existing evaluation metrics focus on text-image alignment but overlook the realism of the generated image, which can be crucial for downstream applications like data augmentation in machine learning. To address this gap, we propose REAL, an automatic evaluation framework that assesses realism of T2I outputs along three dimensions: fine-grained visual attributes, unusual visual relationships, and visual styles. REAL achieves a Spearman&#39;s rho score of up to 0.62 in alignment with human judgement and demonstrates utility in ranking and filtering augmented data for tasks like image captioning, classification, and visual relationship detection. Empirical results show that high-scoring images evaluated by our metrics improve F1 scores of image classification by up to 11.3%, while low-scoring ones degrade that by up to 4.95%. We benchmark four major T2I models across the realism dimensions, providing insights for future improvements in T2I output realism.
</details>

<details>
    <summary>Key points</summary>
    * REAL framework with three realism dimensions
    * Spearman's rho score up to 0.62
    * Utility in data augmentation for tasks like classification
</details>
</details>

---


<details>
<summary><b> Learning to Sample Effective and Diverse Prompts for Text-to-Image Generation</b></summary>

* **Authors:** Taeyoung Yun, Dinghuai Zhang, Jinkyoo Park, Ling Pan
* **arXiv ID:** 2502.11477
* **One-liner:** Introduced PAG, using GFlowNets for diverse and effective prompt adaptation in text-to-image generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2502.11477) | [[PDF]](https://arxiv.org/pdf/2502.11477)

> **Core Innovation**
> Frames prompt adaptation as probabilistic inference, addressing mode collapse and neural plasticity loss.

<details>
    <summary>Abstract</summary>
    Recent advances in text-to-image diffusion models have achieved impressive image generation capabilities. However, it remains challenging to control the generation process with desired properties (e.g., aesthetic quality, user intention), which can be expressed as black-box reward functions. In this paper, we focus on prompt adaptation, which refines the original prompt into model-preferred prompts to generate desired images. While prior work uses reinforcement learning (RL) to optimize prompts, we observe that applying RL often results in generating similar postfixes and deterministic behaviors. To this end, we introduce \textbf{P}rompt \textbf{A}daptation with \textbf{G}FlowNets (\textbf{PAG}), a novel approach that frames prompt adaptation as a probabilistic inference problem. Our key insight is that leveraging Generative Flow Networks (GFlowNets) allows us to shift from reward maximization to sampling from an unnormalized density function, enabling both high-quality and diverse prompt generation. However, we identify that a naive application of GFlowNets suffers from mode collapse and uncovers a previously overlooked phenomenon: the progressive loss of neural plasticity in the model, which is compounded by inefficient credit assignment in sequential prompt generation. To address this critical challenge, we develop a systematic approach in PAG with flow reactivation, reward-prioritized sampling, and reward decomposition for prompt adaptation. Extensive experiments validate that PAG successfully learns to sample effective and diverse prompts for text-to-image generation. We also show that PAG exhibits strong robustness across various reward functions and transferability to different text-to-image models.
</details>

<details>
    <summary>Key points</summary>
    * Prompt Adaptation with GFlowNets (PAG)
    * Flow reactivation, reward-prioritized sampling, reward decomposition
    * Robustness across reward functions and models
</details>
</details>

---


<details>
<summary><b> CHATS: Combining Human-Aligned Optimization and Test-Time Sampling for Text-to-Image Generation</b></summary>

* **Authors:** Minghao Fu, Guo-Hua Wang, Liangfu Cao, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang
* **arXiv ID:** 2502.12579
* **One-liner:** Introduced CHATS, a framework combining human-aligned optimization and test-time sampling for T2I models.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2502.12579) | [[PDF]](https://arxiv.org/pdf/2502.12579)

> **Core Innovation**
> Models preferred and dispreferred distributions, achieving high performance with data efficiency.

<details>
    <summary>Abstract</summary>
    Diffusion models have emerged as a dominant approach for text-to-image generation. Key components such as the human preference alignment and classifier-free guidance play a crucial role in ensuring generation quality. However, their independent application in current text-to-image models continues to face significant challenges in achieving strong text-image alignment, high generation quality, and consistency with human aesthetic standards. In this work, we for the first time, explore facilitating the collaboration of human performance alignment and test-time sampling to unlock the potential of text-to-image models. Consequently, we introduce CHATS (Combining Human-Aligned optimization and Test-time Sampling), a novel generative framework that separately models the preferred and dispreferred distributions and employs a proxy-prompt-based sampling strategy to utilize the useful information contained in both distributions. We observe that CHATS exhibits exceptional data efficiency, achieving strong performance with only a small, high-quality funetuning dataset. Extensive experiments demonstrate that CHATS surpasses traditional preference alignment methods, setting new state-of-the-art across various standard benchmarks.
</details>

<details>
    <summary>Key points</summary>
    * CHATS framework with proxy-prompt-based sampling
    * Separate modeling of distributions
    * Data efficiency with small fine-tuning datasets
</details>
</details>

---


<details>
<summary><b> FlipConcept: Tuning-Free Multi-Concept Personalization for Text-to-Image Generation</b></summary>

* **Authors:** Young Beom Woo, Sun Eung Kim, Seong-Whan Lee
* **arXiv ID:** 2502.15203
* **One-liner:** Proposed FlipConcept for seamless multi-concept personalization in T2I without additional tuning.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2502.15203) | [[PDF]](https://arxiv.org/pdf/2502.15203)

> **Core Innovation**
> Introduced guided appearance attention, mask-guided noise mixing, and background dilution to enhance fidelity and prevent concept leakage.

<details>
    <summary>Abstract</summary>
    Integrating multiple personalized concepts into a single image has recently gained attention in text-to-image (T2I) generation. However, existing methods often suffer from performance degradation in complex scenes due to distortions in non-personalized regions and the need for additional fine-tuning, limiting their practicality. To address this issue, we propose FlipConcept, a novel approach that seamlessly integrates multiple personalized concepts into a single image without requiring additional tuning. We introduce guided appearance attention to enhance the visual fidelity of personalized concepts. Additionally, we introduce mask-guided noise mixing to protect non-personalized regions during concept integration. Lastly, we apply background dilution to minimize concept leakage, i.e., the undesired blending of personalized concepts with other objects in the image. In our experiments, we demonstrate that the proposed method, despite not requiring tuning, outperforms existing models in both single and multiple personalized concept inference. These results demonstrate the effectiveness and practicality of our approach for scalable, high-quality multi-concept personalization.
</details>

<details>
    <summary>Key points</summary>
    * Guided appearance attention for visual fidelity
    * Mask-guided noise mixing to protect non-personalized regions
    * Background dilution to minimize concept leakage
</details>
</details>

---


<details>
<summary><b> Multi-Agent Multimodal Models for Multicultural Text to Image Generation</b></summary>

* **Authors:** Parth Bhalerao, Mounika Yalamarty, Brian Trinh, Oana Ignat
* **arXiv ID:** 2502.15972
* **One-liner:** Introduced MosAIG, a multi-agent framework for multicultural image generation using LLMs.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2502.15972) | [[PDF]](https://arxiv.org/pdf/2502.15972)

> **Core Innovation**
> Leveraged distinct cultural personas in LLMs and provided a multicultural dataset, showing multi-agent interactions outperform no-agent models.

<details>
    <summary>Abstract</summary>
    Large Language Models (LLMs) demonstrate impressive performance across various multimodal tasks. However, their effectiveness in cross-cultural contexts remains limited due to the predominantly Western-centric nature of existing data and models. Meanwhile, multi-agent models have shown strong capabilities in solving complex tasks. In this paper, we evaluate the performance of LLMs in a multi-agent interaction setting for the novel task of multicultural image generation. Our key contributions are: (1) We introduce MosAIG, a Multi-Agent framework that enhances multicultural Image Generation by leveraging LLMs with distinct cultural personas; (2) We provide a dataset of 9,000 multicultural images spanning five countries, three age groups, two genders, 25 historical landmarks, and five languages; and (3) We demonstrate that multi-agent interactions outperform simple, no-agent models across multiple evaluation metrics, offering valuable insights for future research. Our dataset and models are available at <a href="https://github.com/OanaIgnat/MosAIG" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Multi-agent framework with cultural personas
    * Dataset of 9,000 multicultural images
    * Demonstrated superiority in evaluation metrics
</details>
</details>

---


<details>
<summary><b> Multimodal Representation Alignment for Image Generation: Text-Image Interleaved Control Is Easier Than You Think</b></summary>

* **Authors:** Liang Chen, Shuai Bai, Wenhao Chai, Weichu Xie, Haozhe Zhao, Leon Vinci, Junyang Lin, Baobao Chang
* **arXiv ID:** 2502.20172
* **One-liner:** Proposed Dream Engine for arbitrary text-image interleaved control in image generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2502.20172) | [[PDF]](https://arxiv.org/pdf/2502.20172)

> **Core Innovation**
> Utilized large multimodal models for shared representation and a two-stage training paradigm for effective control.

<details>
    <summary>Abstract</summary>
    The field of advanced text-to-image generation is witnessing the emergence of unified frameworks that integrate powerful text encoders, such as CLIP and T5, with Diffusion Transformer backbones. Although there have been efforts to control output images with additional conditions, like canny and depth map, a comprehensive framework for arbitrary text-image interleaved control is still lacking. This gap is especially evident when attempting to merge concepts or visual elements from multiple images in the generation process. To mitigate the gap, we conducted preliminary experiments showing that large multimodal models (LMMs) offer an effective shared representation space, where image and text can be well-aligned to serve as a condition for external diffusion models. Based on this discovery, we propose Dream Engine, an efficient and unified framework designed for arbitrary text-image interleaved control in image generation models. Building on powerful text-to-image models like SD3.5, we replace the original text-only encoders by incorporating versatile multimodal information encoders such as QwenVL. Our approach utilizes a two-stage training paradigm, consisting of joint text-image alignment and multimodal interleaved instruction tuning. Our experiments demonstrate that this training method is effective, achieving a 0.69 overall score on the GenEval benchmark, and matching the performance of state-of-the-art text-to-image models like SD3.5 and FLUX.
</details>

<details>
    <summary>Key points</summary>
    * Shared representation space with LMMs
    * Two-stage training: joint alignment and instruction tuning
    * Replaced text-only encoders with multimodal ones
</details>
</details>

---


<details>
<summary><b> Fine-Grained Alignment and Noise Refinement for Compositional Text-to-Image Generation</b></summary>

* **Authors:** Amir Mohammad Izadi, Seyed Mohammad Hadi Hosseini, Soroush Vafaie Tabar, Ali Abdollahi, Armin Saghafian, Mahdieh Soleymani Baghshah
* **arXiv ID:** 2503.06506
* **One-liner:** Developed a training-free method to improve text-to-image compositionality with constraint-based losses.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2503.06506) | [[PDF]](https://arxiv.org/pdf/2503.06506)

> **Core Innovation**
> Integrated entity and attribute constraints as losses and introduced a feedback-driven noise refinement system.

<details>
    <summary>Abstract</summary>
    Text-to-image generative models have made significant advancements in recent years; however, accurately capturing intricate details in textual prompts-such as entity missing, attribute binding errors, and incorrect relationships remains a formidable challenge. In response, we present an innovative, training-free method that directly addresses these challenges by incorporating tailored objectives to account for textual constraints. Unlike layout-based approaches that enforce rigid structures and limit diversity, our proposed approach offers a more flexible arrangement of the scene by imposing just the extracted constraints from the text, without any unnecessary additions. These constraints are formulated as losses-entity missing, entity mixing, attribute binding, and spatial relationships-integrated into a unified loss that is applied in the first generation stage. Furthermore, we introduce a feedback-driven system for fine-grained initial noise refinement. This system integrates a verifier that evaluates the generated image, identifies inconsistencies, and provides corrective feedback. Leveraging this feedback, our refinement method first targets the unmet constraints by refining the faulty attention maps caused by initial noise, through the optimization of selective losses associated with these constraints. Subsequently, our unified loss function is reapplied to proceed the second generation phase. Experimental results demonstrate that our method, relying solely on our proposed objective functions, significantly enhances compositionality, achieving a 24% improvement in human evaluation and a 25% gain in spatial relationships. Furthermore, our fine-grained noise refinement proves effective, boosting performance by up to 5%. Code is available at \href{<a href="https://github.com/hadi-hosseini/noise-refinement" rel="external noopener nofollow" class="link-external link-https">this https URL</a>}{<a href="https://github.com/hadi-hosseini/noise-refinement" rel="external noopener nofollow" class="link-external link-https">this https URL</a>}.
</details>

<details>
    <summary>Key points</summary>
    * Unified loss for textual constraints
    * Feedback-driven noise refinement
    * Verifier for inconsistency identification
</details>
</details>

---


<details>
<summary><b> Unleashing the Potential of Large Language Models for Text-to-Image Generation through Autoregressive Representation Alignment</b></summary>

* **Authors:** Xing Xie, Jiawei Liu, Ziyue Lin, Huijie Fan, Zhi Han, Yandong Tang, Liangqiong Qu
* **arXiv ID:** 2503.07334
* **One-liner:** Introduced ARRA for global-coherent text-to-image generation in autoregressive LLMs without architectural changes.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2503.07334) | [[PDF]](https://arxiv.org/pdf/2503.07334)

> **Core Innovation**
> Aligned LLM hidden states with visual representations using a global visual alignment loss and hybrid token.

<details>
    <summary>Abstract</summary>
    We present Autoregressive Representation Alignment (ARRA), a new training framework that unlocks global-coherent text-to-image generation in autoregressive LLMs without architectural modifications. Different from prior works that require complex architectural redesigns, ARRA aligns LLM&#39;s hidden states with visual representations from external visual foundational models via a global visual alignment loss and a hybrid token, [object Object]. This token enforces dual constraints: local next-token prediction and global semantic distillation, enabling LLMs to implicitly learn spatial and contextual coherence while retaining their original autoregressive paradigm. Extensive experiments validate ARRA&#39;s plug-and-play versatility. When training T2I LLMs from scratch, ARRA reduces FID by 16.6% (ImageNet), 12.0% (LAION-COCO) for autoregressive LLMs like LlamaGen, without modifying original architecture and inference mechanism. For training from text-generation-only LLMs, ARRA reduces FID by 25.5% (MIMIC-CXR), 8.8% (DeepEyeNet) for advanced LLMs like Chameleon. For domain adaptation, ARRA aligns general-purpose LLMs with specialized models (e.g., BioMedCLIP), achieving an 18.6% FID reduction over direct fine-tuning on medical imaging (MIMIC-CXR). These results demonstrate that training objective redesign, rather than architectural modifications, can resolve cross-modal global coherence challenges. ARRA offers a complementary paradigm for advancing autoregressive models. The code is available at <a href="https://github.com/xiexing0916/ARRA" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Global visual alignment loss
    * Hybrid token for dual constraints
    * Plug-and-play versatility for various LLMs
</details>
</details>

---


<details>
<summary><b> SANA-Sprint: One-Step Diffusion with Continuous-Time Consistency Distillation</b></summary>

* **Authors:** Junsong Chen, Shuchen Xue, Yuyang Zhao, Jincheng Yu, Sayak Paul, Junyu Chen, Han Cai, Song Han, Enze Xie
* **arXiv ID:** 2503.09641
* **One-liner:** Presented SANA-Sprint for ultra-fast T2I generation with hybrid distillation and step-adaptive inference.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2503.09641) | [[PDF]](https://arxiv.org/pdf/2503.09641)

> **Core Innovation**
> Combined sCM and LADD for efficient distillation, achieving high-quality generation in 1-4 steps with real-time control.

<details>
    <summary>Abstract</summary>
    This paper presents SANA-Sprint, an efficient diffusion model for ultra-fast text-to-image (T2I) generation. SANA-Sprint is built on a pre-trained foundation model and augmented with hybrid distillation, dramatically reducing inference steps from 20 to 1-4. We introduce three key innovations: (1) We propose a training-free approach that transforms a pre-trained flow-matching model for continuous-time consistency distillation (sCM), eliminating costly training from scratch and achieving high training efficiency. Our hybrid distillation strategy combines sCM with latent adversarial distillation (LADD): sCM ensures alignment with the teacher model, while LADD enhances single-step generation fidelity. (2) SANA-Sprint is a unified step-adaptive model that achieves high-quality generation in 1-4 steps, eliminating step-specific training and improving efficiency. (3) We integrate ControlNet with SANA-Sprint for real-time interactive image generation, enabling instant visual feedback for user interaction. SANA-Sprint establishes a new Pareto frontier in speed-quality tradeoffs, achieving state-of-the-art performance with 7.59 FID and 0.74 GenEval in only 1 step - outperforming FLUX-schnell (7.94 FID / 0.71 GenEval) while being 10x faster (0.1s vs 1.1s on H100). It also achieves 0.1s (T2I) and 0.25s (ControlNet) latency for 1024 x 1024 images on H100, and 0.31s (T2I) on an RTX 4090, showcasing its exceptional efficiency and potential for AI-powered consumer applications (AIPC). Code and pre-trained models will be open-sourced.
</details>

<details>
    <summary>Key points</summary>
    * Hybrid distillation: sCM and LADD
    * Step-adaptive model for 1-4 steps
    * Integration with ControlNet for interactivity
</details>
</details>

---


<details>
<summary><b> ConceptGuard: Continual Personalized Text-to-Image Generation with Forgetting and Confusion Mitigation</b></summary>

* **Authors:** Zirun Guo, Tao Jin
* **arXiv ID:** 2503.10358
* **One-liner:** Proposed ConceptGuard to address catastrophic forgetting in sequential diffusion customization.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2503.10358) | [[PDF]](https://arxiv.org/pdf/2503.10358)

> **Core Innovation**
> Combined shift embedding, concept-binding prompts, memory preservation, and a priority queue for dynamic concept management.

<details>
    <summary>Abstract</summary>
    Diffusion customization methods have achieved impressive results with only a minimal number of user-provided images. However, existing approaches customize concepts collectively, whereas real-world applications often require sequential concept integration. This sequential nature can lead to catastrophic forgetting, where previously learned concepts are lost. In this paper, we investigate concept forgetting and concept confusion in the continual customization. To tackle these challenges, we present ConceptGuard, a comprehensive approach that combines shift embedding, concept-binding prompts and memory preservation regularization, supplemented by a priority queue which can adaptively update the importance and occurrence order of different concepts. These strategies can dynamically update, unbind and learn the relationship of the previous concepts, thus alleviating concept forgetting and confusion. Through comprehensive experiments, we show that our approach outperforms all the baseline methods consistently and significantly in both quantitative and qualitative analyses.
</details>

<details>
    <summary>Key points</summary>
    * Shift embedding and concept-binding prompts
    * Memory preservation regularization
    * Priority queue for concept importance
</details>
</details>

---


<details>
<summary><b> DiT-Air: Revisiting the Efficiency of Diffusion Model Architecture Design in Text to Image Generation</b></summary>

* **Authors:** Chen Chen, Rui Qian, Wenze Hu, Tsu-Jui Fu, Jialing Tong, Xinze Wang, Lezhi Li, Bowen Zhang, Alex Schwing, Wei Liu, Yinfei Yang
* **arXiv ID:** 2503.10618
* **One-liner:** Empirically studied DiTs and introduced DiT-Air for efficient and high-performance text-to-image generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2503.10618) | [[PDF]](https://arxiv.org/pdf/2503.10618)

> **Core Innovation**
> Found standard DiT comparable to specialized models, with layer-wise sharing reducing model size and achieving SOTA performance.

<details>
    <summary>Abstract</summary>
    In this work, we empirically study Diffusion Transformers (DiTs) for text-to-image generation, focusing on architectural choices, text-conditioning strategies, and training protocols. We evaluate a range of DiT-based architectures--including PixArt-style and MMDiT variants--and compare them with a standard DiT variant which directly processes concatenated text and noise inputs. Surprisingly, our findings reveal that the performance of standard DiT is comparable with those specialized models, while demonstrating superior parameter-efficiency, especially when scaled up. Leveraging the layer-wise parameter sharing strategy, we achieve a further reduction of 66% in model size compared to an MMDiT architecture, with minimal performance impact. Building on an in-depth analysis of critical components such as text encoders and Variational Auto-Encoders (VAEs), we introduce DiT-Air and DiT-Air-Lite. With supervised and reward fine-tuning, DiT-Air achieves state-of-the-art performance on GenEval and T2I CompBench, while DiT-Air-Lite remains highly competitive, surpassing most existing models despite its compact size.
</details>

<details>
    <summary>Key points</summary>
    * Architectural evaluation of DiTs
    * Layer-wise parameter sharing
    * Supervised and reward fine-tuning
</details>
</details>

---


<details>
<summary><b> TF-TI2I: Training-Free Text-and-Image-to-Image Generation via Multi-Modal Implicit-Context Learning in Text-to-Image Models</b></summary>

* **Authors:** Teng-Fang Hsiao, Bo-Kai Ruan, Yi-Lun Wu, Tzu-Ling Lin, Hong-Han Shuai
* **arXiv ID:** 2503.15283
* **One-liner:** Introduced TF-TI2I for training-free text-and-image-to-image generation with enhanced multimodal interactions.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2503.15283) | [[PDF]](https://arxiv.org/pdf/2503.15283)

> **Core Innovation**
> Used MM-DiT architecture with Reference Contextual Masking and Winner-Takes-All module to handle complex instructions.

<details>
    <summary>Abstract</summary>
    Text-and-Image-To-Image (TI2I), an extension of Text-To-Image (T2I), integrates image inputs with textual instructions to enhance image generation. Existing methods often partially utilize image inputs, focusing on specific elements like objects or styles, or they experience a decline in generation quality with complex, multi-image instructions. To overcome these challenges, we introduce Training-Free Text-and-Image-to-Image (TF-TI2I), which adapts cutting-edge T2I models such as SD3 without the need for additional training. Our method capitalizes on the MM-DiT architecture, in which we point out that textual tokens can implicitly learn visual information from vision tokens. We enhance this interaction by extracting a condensed visual representation from reference images, facilitating selective information sharing through Reference Contextual Masking -- this technique confines the usage of contextual tokens to instruction-relevant visual information. Additionally, our Winner-Takes-All module mitigates distribution shifts by prioritizing the most pertinent references for each vision token. Addressing the gap in TI2I evaluation, we also introduce the FG-TI2I Bench, a comprehensive benchmark tailored for TI2I and compatible with existing T2I methods. Our approach shows robust performance across various benchmarks, confirming its effectiveness in handling complex image-generation tasks.
</details>

<details>
    <summary>Key points</summary>
    * Reference Contextual Masking
    * Winner-Takes-All module
    * FG-TI2I Bench for evaluation
</details>
</details>

---


<details>
<summary><b> Zero-Shot Styled Text Image Generation, but Make It Autoregressive</b></summary>

* **Authors:** Vittorio Pippi, Fabio Quattrini, Silvia Cascianelli, Alessio Tonioni, Rita Cucchiara
* **arXiv ID:** 2503.17074
* **One-liner:** Proposed Emuru, an autoregressive model for styled handwritten text generation with zero-shot generalization.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2503.17074) | [[PDF]](https://arxiv.org/pdf/2503.17074)

> **Core Innovation**
> Leveraged a VAE and Transformer trained on synthetic data to generate styled text images without background artifacts.

<details>
    <summary>Abstract</summary>
    Styled Handwritten Text Generation (HTG) has recently received attention from the computer vision and document analysis communities, which have developed several solutions, either GAN- or diffusion-based, that achieved promising results. Nonetheless, these strategies fail to generalize to novel styles and have technical constraints, particularly in terms of maximum output length and training efficiency. To overcome these limitations, in this work, we propose a novel framework for text image generation, dubbed Emuru. Our approach leverages a powerful text image representation model (a variational autoencoder) combined with an autoregressive Transformer. Our approach enables the generation of styled text images conditioned on textual content and style examples, such as specific fonts or handwriting styles. We train our model solely on a diverse, synthetic dataset of English text rendered in over 100,000 typewritten and calligraphy fonts, which gives it the capability to reproduce unseen styles (both fonts and users&#39; handwriting) in zero-shot. To the best of our knowledge, Emuru is the first autoregressive model for HTG, and the first designed specifically for generalization to novel styles. Moreover, our model generates images without background artifacts, which are easier to use for downstream applications. Extensive evaluation on both typewritten and handwritten, any-length text image generation scenarios demonstrates the effectiveness of our approach.
</details>

<details>
    <summary>Key points</summary>
    * Autoregressive Transformer with VAE
    * Training on diverse synthetic dataset
    * Zero-shot generalization to novel styles
</details>
</details>

---


<details>
<summary><b> Progressive Prompt Detailing for Improved Alignment in Text-to-Image Generative Models</b></summary>

* **Authors:** Ketan Suhaas Saichandran, Xavier Thomas, Prakhar Kaushik, Deepti Ghadiyaram
* **arXiv ID:** 2503.17794
* **One-liner:** Proposed SCoPE, a training-free method to improve text-to-image alignment for complex scenes.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2503.17794) | [[PDF]](https://arxiv.org/pdf/2503.17794)

> **Core Innovation**
> Progressively refines input prompts from coarse to fine details during inference.

<details>
    <summary>Abstract</summary>
    Text-to-image generative models often struggle with long prompts detailing complex scenes, diverse objects with distinct visual characteristics and spatial relationships. In this work, we propose SCoPE (Scheduled interpolation of Coarse-to-fine Prompt Embeddings), a training-free method to improve text-to-image alignment by progressively refining the input prompt in a coarse-to-fine-grained manner. Given a detailed input prompt, we first decompose it into multiple sub-prompts which evolve from describing broad scene layout to highly intricate details. During inference, we interpolate between these sub-prompts and thus progressively introduce finer-grained details into the generated image. Our training-free plug-and-play approach significantly enhances prompt alignment, achieves an average improvement of more than +8 in Visual Question Answering (VQA) scores over the Stable Diffusion baselines on 83% of the prompts from the GenAI-Bench dataset.
</details>

<details>
    <summary>Key points</summary>
    * Decompose prompts into sub-prompts evolving from broad layout to intricate details
    * Interpolate between sub-prompts to introduce finer-grained details
    * Achieves +8 VQA score improvement on GenAI-Bench dataset
</details>
</details>

---


<details>
<summary><b> Plug-and-Play Interpretable Responsible Text-to-Image Generation via Dual-Space Multi-facet Concept Control</b></summary>

* **Authors:** Basim Azam, Naveed Akhtar
* **arXiv ID:** 2503.18324
* **One-liner:** Introduced a scalable technique for responsible T2I generation addressing fairness and safety.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2503.18324) | [[PDF]](https://arxiv.org/pdf/2503.18324)

> **Core Innovation**
> Uses an interpretable composite responsible space via knowledge distillation and concept whitening.

<details>
    <summary>Abstract</summary>
    Ethical issues around text-to-image (T2I) models demand a comprehensive control over the generative content. Existing techniques addressing these issues for responsible T2I models aim for the generated content to be fair and safe (non-violent/explicit). However, these methods remain bounded to handling the facets of responsibility concepts individually, while also lacking in interpretability. Moreover, they often require alteration to the original model, which compromises the model performance. In this work, we propose a unique technique to enable responsible T2I generation by simultaneously accounting for an extensive range of concepts for fair and safe content generation in a scalable manner. The key idea is to distill the target T2I pipeline with an external plug-and-play mechanism that learns an interpretable composite responsible space for the desired concepts, conditioned on the target T2I pipeline. We use knowledge distillation and concept whitening to enable this. At inference, the learned space is utilized to modulate the generative content. A typical T2I pipeline presents two plug-in points for our approach, namely; the text embedding space and the diffusion model latent space. We develop modules for both points and show the effectiveness of our approach with a range of strong results.
</details>

<details>
    <summary>Key points</summary>
    * Distill T2I pipeline with plug-and-play mechanism
    * Learn interpretable composite responsible space
    * Modulate generative content at text embedding and latent spaces
</details>
</details>

---


<details>
<summary><b> Beyond Words: Advancing Long-Text Image Generation via Multimodal Autoregressive Models</b></summary>

* **Authors:** Alex Jinpeng Wang, Linjie Li, Zhengyuan Yang, Lijuan Wang, Min Li
* **arXiv ID:** 2503.20198
* **One-liner:** Developed a model for generating high-quality long-text images with unprecedented fidelity.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2503.20198) | [[PDF]](https://arxiv.org/pdf/2503.20198)

> **Core Innovation**
> Addresses the bottleneck in text generation quality via a novel binary tokenizer.

<details>
    <summary>Abstract</summary>
    Recent advancements in autoregressive and diffusion models have led to strong performance in image generation with short scene text words. However, generating coherent, long-form text in images, such as paragraphs in slides or documents, remains a major challenge for current generative models. We present the first work specifically focused on long text image generation, addressing a critical gap in existing text-to-image systems that typically handle only brief phrases or single sentences. Through comprehensive analysis of state-of-the-art autoregressive generation models, we identify the image tokenizer as a critical bottleneck in text generating quality. To address this, we introduce a novel text-focused, binary tokenizer optimized for capturing detailed scene text features. Leveraging our tokenizer, we develop \ModelName, a multimodal autoregressive model that excels in generating high-quality long-text images with unprecedented fidelity. Our model offers robust controllability, enabling customization of text properties such as font style, size, color, and alignment. Extensive experiments demonstrate that \ModelName~significantly outperforms SD3.5 Large~\cite{sd3} and GPT4o~\cite{gpt4o} with DALL-E 3~\cite{dalle3} in generating long text accurately, consistently, and flexibly. Beyond its technical achievements, \ModelName~opens up exciting opportunities for innovative applications like interleaved document and PowerPoint generation, establishing a new frontier in long-text image generating.
</details>

<details>
    <summary>Key points</summary>
    * Introduce text-focused binary tokenizer for detailed scene text
    * Build multimodal autoregressive model for long-text generation
    * Enable customization of text properties like font and alignment
</details>
</details>

---


<details>
<summary><b> Lumina-Image 2.0: A Unified and Efficient Image Generative Framework</b></summary>

* **Authors:** Qi Qin, Le Zhuo, Yi Xin, Ruoyi Du, Zhen Li, Bin Fu, Yiting Lu, Jiakang Yuan, Xinyue Li, Dongyang Liu, Xiangyang Zhu, Manyuan Zhang, Will Beddow, Erwann Millon, Victor Perez, Wenhai Wang, Conghui He, Bo Zhang, Xiaohong Liu, Hongsheng Li, Yu Qiao, Chang Xu, Peng Gao
* **arXiv ID:** 2503.21758
* **One-liner:** Advanced text-to-image generation with Lumina-Image 2.0, emphasizing unification and efficiency.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2503.21758) | [[PDF]](https://arxiv.org/pdf/2503.21758)

> **Core Innovation**
> Adopts unified architecture and captioning system for better cross-modal interactions.

<details>
    <summary>Abstract</summary>
    We introduce Lumina-Image 2.0, an advanced text-to-image generation framework that achieves significant progress compared to previous work, Lumina-Next. Lumina-Image 2.0 is built upon two key principles: (1) Unification - it adopts a unified architecture (Unified Next-DiT) that treats text and image tokens as a joint sequence, enabling natural cross-modal interactions and allowing seamless task expansion. Besides, since high-quality captioners can provide semantically well-aligned text-image training pairs, we introduce a unified captioning system, Unified Captioner (UniCap), specifically designed for T2I generation tasks. UniCap excels at generating comprehensive and accurate captions, accelerating convergence and enhancing prompt adherence. (2) Efficiency - to improve the efficiency of our proposed model, we develop multi-stage progressive training strategies and introduce inference acceleration techniques without compromising image quality. Extensive evaluations on academic benchmarks and public text-to-image arenas show that Lumina-Image 2.0 delivers strong performances even with only 2.6B parameters, highlighting its scalability and design efficiency. We have released our training details, code, and models at <a href="https://github.com/Alpha-VLLM/Lumina-Image-2.0" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Use Unified Next-DiT for joint text-image token sequences
    * Introduce Unified Captioner for accurate captions
    * Apply multi-stage training and inference acceleration techniques
</details>
</details>

---


<details>
<summary><b> Geometrical Properties of Text Token Embeddings for Strong Semantic Binding in Text-to-Image Generation</b></summary>

* **Authors:** Hoigi Seo, Junseo Bang, Haechang Lee, Joohoon Lee, Byung Hyun Lee, Se Young Chun
* **arXiv ID:** 2503.23011
* **One-liner:** Proposed TokeBi, a training-free framework for strong semantic binding in T2I models.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2503.23011) | [[PDF]](https://arxiv.org/pdf/2503.23011)

> **Core Innovation**
> Leverages geometrical properties of token embeddings to improve cross-attention maps.

<details>
    <summary>Abstract</summary>
    Text-to-image (T2I) models often suffer from text-image misalignment in complex scenes involving multiple objects and attributes. Semantic binding has attempted to associate the generated attributes and objects with their corresponding noun phrases (NPs) by text or latent optimizations with the modulation of cross-attention (CA) maps; yet, the factors that influence semantic binding remain underexplored. Here, we investigate the geometrical properties of text token embeddings and their CA maps. We found that the geometrical properties of token embeddings, specifically angular distances and norms, are crucial factors in the differentiation of the CA map. These theoretical findings led to our proposed training-free text-embedding-aware T2I framework, dubbed \textbf{TokeBi}, for strong semantic binding. TokeBi consists of Causality-Aware Projection-Out (CAPO) for distinguishing inter-NP CA maps and Adaptive Token Mixing (ATM) for enhancing inter-NP separation while maintaining intra-NP cohesion in CA maps. Extensive experiments confirm that TokeBi outperforms prior arts across diverse baselines and datasets.
</details>

<details>
    <summary>Key points</summary>
    * Investigate angular distances and norms of token embeddings
    * Implement Causality-Aware Projection-Out for inter-NP distinction
    * Use Adaptive Token Mixing for intra-NP cohesion
</details>
</details>

---


<details>
<summary><b> LayerCraft: Enhancing Text-to-Image Generation with CoT Reasoning and Layered Object Integration</b></summary>

* **Authors:** Yuyao Zhang, Jinghao Li, Yu-Wing Tai
* **arXiv ID:** 2504.00010
* **One-liner:** Introduced LayerCraft, a modular framework for structured image generation and editing using LLMs.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2504.00010) | [[PDF]](https://arxiv.org/pdf/2504.00010)

> **Core Innovation**
> Enables controllable scene decomposition and object integration without retraining.

<details>
    <summary>Abstract</summary>
    Text-to-image (T2I) generation has made remarkable progress, yet existing systems still lack intuitive control over spatial composition, object consistency, and multi-step editing. We present $\textbf{LayerCraft}$, a modular framework that uses large language models (LLMs) as autonomous agents to orchestrate structured, layered image generation and editing. LayerCraft supports two key capabilities: (1) $\textit{structured generation}$ from simple prompts via chain-of-thought (CoT) reasoning, enabling it to decompose scenes, reason about object placement, and guide composition in a controllable, interpretable manner; and (2) $\textit{layered object integration}$, allowing users to insert and customize objects -- such as characters or props -- across diverse images or scenes while preserving identity, context, and style. The system comprises a coordinator agent, the $\textbf{ChainArchitect}$ for CoT-driven layout planning, and the $\textbf{Object Integration Network (OIN)}$ for seamless image editing using off-the-shelf T2I models without retraining. Through applications like batch collage editing and narrative scene generation, LayerCraft empowers non-experts to iteratively design, customize, and refine visual content with minimal manual effort. Code will be released at <a href="https://github.com/PeterYYZhang/LayerCraft" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Use LLMs as autonomous agents for chain-of-thought reasoning
    * Implement ChainArchitect for layout planning
    * Develop Object Integration Network for seamless editing
</details>
</details>

---


<details>
<summary><b> Compass Control: Multi Object Orientation Control for Text-to-Image Generation</b></summary>

* **Authors:** Rishubh Parihar, Vaibhav Agrawal, Sachidanand VS, R. Venkatesh Babu
* **arXiv ID:** 2504.06752
* **One-liner:** Enabled precise multi-object orientation control in T2I diffusion models.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2504.06752) | [[PDF]](https://arxiv.org/pdf/2504.06752)

> **Core Innovation**
> Conditions model with orientation-aware compass tokens and constrains cross-attention maps.

<details>
    <summary>Abstract</summary>
    Existing approaches for controlling text-to-image diffusion models, while powerful, do not allow for explicit 3D object-centric control, such as precise control of object orientation. In this work, we address the problem of multi-object orientation control in text-to-image diffusion models. This enables the generation of diverse multi-object scenes with precise orientation control for each object. The key idea is to condition the diffusion model with a set of orientation-aware \textbf{compass} tokens, one for each object, along with text tokens. A light-weight encoder network predicts these compass tokens taking object orientation as the input. The model is trained on a synthetic dataset of procedurally generated scenes, each containing one or two 3D assets on a plain background. However, direct training this framework results in poor orientation control as well as leads to entanglement among objects. To mitigate this, we intervene in the generation process and constrain the cross-attention maps of each compass token to its corresponding object regions. The trained model is able to achieve precise orientation control for a) complex objects not seen during training and b) multi-object scenes with more than two objects, indicating strong generalization capabilities. Further, when combined with personalization methods, our method precisely controls the orientation of the new object in diverse contexts. Our method achieves state-of-the-art orientation control and text alignment, quantified with extensive evaluations and a user study.
</details>

<details>
    <summary>Key points</summary>
    * Predict compass tokens with light-weight encoder
    * Constrain cross-attention maps to object regions
    * Achieve generalization to unseen objects and multi-object scenes
</details>
</details>

---


<details>
<summary><b> Towards NSFW-Free Text-to-Image Generation via Safety-Constraint Direct Preference Optimization</b></summary>

* **Authors:** Shouwei Ruan, Zhenyu Wu, Yao Huang, Ruochen Zhang, Yitong Sun, Caixin Kang, Xingxing Wei
* **arXiv ID:** 2504.14290
* **One-liner:** Proposed SC-DPO for safety alignment in T2I models, balancing safety and quality.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2504.14290) | [[PDF]](https://arxiv.org/pdf/2504.14290)

> **Core Innovation**
> Integrates safety constraints into preference optimization with a safety cost model.

<details>
    <summary>Abstract</summary>
    Ensuring the safety of generated content remains a fundamental challenge for Text-to-Image (T2I) generation. Existing studies either fail to guarantee complete safety under potentially harmful concepts or struggle to balance safety with generation quality. To address these issues, we propose Safety-Constrained Direct Preference Optimization (SC-DPO), a novel framework for safety alignment in T2I models. SC-DPO integrates safety constraints into the general human preference calibration, aiming to maximize the likelihood of generating human-preferred samples while minimizing the safety cost of the generated outputs. In SC-DPO, we introduce a safety cost model to accurately quantify harmful levels for images, and train it effectively using the proposed contrastive learning and cost anchoring objectives. To apply SC-DPO for effective T2I safety alignment, we constructed SCP-10K, a safety-constrained preference dataset containing rich harmful concepts, which blends safety-constrained preference pairs under both harmful and clean instructions, further mitigating the trade-off between safety and sample quality. Additionally, we propose a Dynamic Focusing Mechanism (DFM) for SC-DPO, promoting the model&#39;s learning of difficult preference pair samples. Extensive experiments demonstrate that SC-DPO outperforms existing methods, effectively defending against various NSFW content while maintaining optimal sample quality and human preference alignment. Additionally, SC-DPO exhibits resilience against adversarial prompts designed to generate harmful content.
</details>

<details>
    <summary>Key points</summary>
    * Introduce safety cost model for harmful level quantification
    * Use contrastive learning and cost anchoring objectives
    * Construct SCP-10K dataset and apply Dynamic Focusing Mechanism
</details>
</details>

---


<details>
<summary><b> LLM-Enabled Style and Content Regularization for Personalized Text-to-Image Generation</b></summary>

* **Authors:** Anran Yu, Wei Feng, Yaochen Zhang, Xiang Li, Lei Meng, Lei Wu, Xiangxu Meng
* **arXiv ID:** 2504.15309
* **One-liner:** Enhanced personalized T2I generation with style refinement and content preservation strategies.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2504.15309) | [[PDF]](https://arxiv.org/pdf/2504.15309)

> **Core Innovation**
> Optimizes style embeddings and preserves model generalization for better controllability.

<details>
    <summary>Abstract</summary>
    The personalized text-to-image generation has rapidly advanced with the emergence of Stable Diffusion. Existing methods, which typically fine-tune models using embedded identifiers, often struggle with insufficient stylization and inaccurate image content due to reduced textual controllability. In this paper, we propose style refinement and content preservation strategies. The style refinement strategy leverages the semantic information of visual reasoning prompts and reference images to optimize style embeddings, allowing a more precise and consistent representation of style information. The content preservation strategy addresses the content bias problem by preserving the model&#39;s generalization capabilities, ensuring enhanced textual controllability without compromising stylization. Experimental results verify that our approach achieves superior performance in generating consistent and personalized text-to-image outputs.
</details>

<details>
    <summary>Key points</summary>
    * Leverage semantic information for style embedding optimization
    * Address content bias to maintain textual controllability
    * Achieve consistent and personalized outputs
</details>
</details>

---


<details>
<summary><b> RefVNLI: Towards Scalable Evaluation of Subject-driven Text-to-image Generation</b></summary>

* **Authors:** Aviv Slobodkin, Hagai Taitelbaum, Yonatan Bitton, Brian Gordon, Michal Sokolik, Nitzan Bitton Guetta, Almog Gueta, Royi Rassin, Dani Lischinski, Idan Szpektor
* **arXiv ID:** 2504.17502
* **One-liner:** Introduced RefVNLI, a cost-effective metric for evaluating subject-driven T2I generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2504.17502) | [[PDF]](https://arxiv.org/pdf/2504.17502)

> **Core Innovation**
> Evaluates both textual alignment and subject preservation in a single run.

<details>
    <summary>Abstract</summary>
    Subject-driven text-to-image (T2I) generation aims to produce images that align with a given textual description, while preserving the visual identity from a referenced subject image. Despite its broad downstream applicability - ranging from enhanced personalization in image generation to consistent character representation in video rendering - progress in this field is limited by the lack of reliable automatic evaluation. Existing methods either assess only one aspect of the task (i.e., textual alignment or subject preservation), misalign with human judgments, or rely on costly API-based evaluation. To address this gap, we introduce RefVNLI, a cost-effective metric that evaluates both textual alignment and subject preservation in a single run. Trained on a large-scale dataset derived from video-reasoning benchmarks and image perturbations, RefVNLI outperforms or statistically matches existing baselines across multiple benchmarks and subject categories (e.g., \emph{Animal}, \emph{Object}), achieving up to 6.4-point gains in textual alignment and 5.9-point gains in subject preservation.
</details>

<details>
    <summary>Key points</summary>
    * Train on large-scale dataset from video-reasoning benchmarks
    * Outperform baselines in textual alignment and subject preservation
    * Provide up to 6.4-point gains in alignment and 5.9-point gains in preservation
</details>
</details>

---


<details>
<summary><b> TextTIGER: Text-based Intelligent Generation with Entity Prompt Refinement for Text-to-Image Generation</b></summary>

* **Authors:** Shintaro Ozaki, Kazuki Hayashi, Yusuke Sakai, Jingun Kwon, Hidetaka Kamigaito, Katsuhiko Hayashi, Manabu Okumura, Taro Watanabe
* **arXiv ID:** 2504.18269
* **One-liner:** Improved image generation by refining prompts with augmented entity knowledge.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2504.18269) | [[PDF]](https://arxiv.org/pdf/2504.18269)

> **Core Innovation**
> Proposed TextTIGER, which uses LLMs to summarize augmented entity descriptions for better image generation.

<details>
    <summary>Abstract</summary>
    Generating images from prompts containing specific entities requires models to retain as much entity-specific knowledge as possible. However, fully memorizing such knowledge is impractical due to the vast number of entities and their continuous emergence. To address this, we propose Text-based Intelligent Generation with Entity prompt Refinement (TextTIGER), which augments knowledge on entities included in the prompts and then summarizes the augmented descriptions using Large Language Models (LLMs) to mitigate performance degradation from longer inputs. To evaluate our method, we introduce WiT-Cub (WiT with Captions and Uncomplicated Background-explanations), a dataset comprising captions, images, and an entity list. Experiments on four image generation models and five LLMs show that TextTIGER improves image generation performance in standard metrics (IS, FID, and CLIPScore) compared to caption-only prompts. Additionally, multiple annotators&#39; evaluation confirms that the summarized descriptions are more informative, validating LLMs&#39; ability to generate concise yet rich descriptions. These findings demonstrate that refining prompts with augmented and summarized entity-related descriptions enhances image generation capabilities. The code and dataset will be available upon acceptance.
</details>

<details>
    <summary>Key points</summary>
    * Augment knowledge on entities in prompts
    * Summarize augmented descriptions using LLMs
    * Evaluate with WiT-Cub dataset and standard metrics
</details>
</details>

---


<details>
<summary><b> T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level and Token-level CoT</b></summary>

* **Authors:** Dongzhi Jiang, Ziyu Guo, Renrui Zhang, Zhuofan Zong, Hao Li, Le Zhuo, Shilin Yan, Pheng-Ann Heng, Hongsheng Li
* **arXiv ID:** 2505.00703
* **One-liner:** Enhanced text-to-image generation with bi-level chain-of-thought reasoning.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2505.00703) | [[PDF]](https://arxiv.org/pdf/2505.00703)

> **Core Innovation**
> Introduced T2I-R1, a model using RL and bi-level CoT for semantic and token-level planning.

<details>
    <summary>Abstract</summary>
    Recent advancements in large language models have demonstrated how chain-of-thought (CoT) and reinforcement learning (RL) can improve performance. However, applying such reasoning strategies to the visual generation domain remains largely unexplored. In this paper, we present T2I-R1, a novel reasoning-enhanced text-to-image generation model, powered by RL with a bi-level CoT reasoning process. Specifically, we identify two levels of CoT that can be utilized to enhance different stages of generation: (1) the semantic-level CoT for high-level planning of the prompt and (2) the token-level CoT for low-level pixel processing during patch-by-patch generation. To better coordinate these two levels of CoT, we introduce BiCoT-GRPO with an ensemble of generation rewards, which seamlessly optimizes both generation CoTs within the same training step. By applying our reasoning strategies to the baseline model, Janus-Pro, we achieve superior performance with 13% improvement on T2I-CompBench and 19% improvement on the WISE benchmark, even surpassing the state-of-the-art model FLUX.1. Code is available at: <a href="https://github.com/CaraJ7/T2I-R1" rel="external noopener nofollow" class="link-external link-https">this https URL</a>
</details>

<details>
    <summary>Key points</summary>
    * Semantic-level CoT for high-level prompt planning
    * Token-level CoT for low-level pixel processing
    * Use BiCoT-GRPO with ensemble rewards for optimization
</details>
</details>

---


<details>
<summary><b> Deconstructing Bias: A Multifaceted Framework for Diagnosing Cultural and Compositional Inequities in Text-to-Image Generative Models</b></summary>

* **Authors:** Muna Numan Said, Aarib Zaidi, Rabia Usman, Sonia Okon, Praneeth Medepalli, Kevin Zhu, Vasu Sharma, Sean O&#39;Brien
* **arXiv ID:** 2505.01430
* **One-liner:** Benchmarked cultural biases in text-to-image models using CIS metric.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2505.01430) | [[PDF]](https://arxiv.org/pdf/2505.01430)

> **Core Innovation**
> Developed CIS to evaluate cultural fidelity and identify biases in image generation.

<details>
    <summary>Abstract</summary>
    The transformative potential of text-to-image (T2I) models hinges on their ability to synthesize culturally diverse, photorealistic images from textual prompts. However, these models often perpetuate cultural biases embedded within their training data, leading to systemic misrepresentations. This paper benchmarks the Component Inclusion Score (CIS), a metric designed to evaluate the fidelity of image generation across cultural contexts. Through extensive analysis involving 2,400 images, we quantify biases in terms of compositional fragility and contextual misalignment, revealing significant performance gaps between Western and non-Western cultural prompts. Our findings underscore the impact of data imbalance, attention entropy, and embedding superposition on model fairness. By benchmarking models like Stable Diffusion with CIS, we provide insights into architectural and data-centric interventions for enhancing cultural inclusivity in AI-generated imagery. This work advances the field by offering a comprehensive tool for diagnosing and mitigating biases in T2I generation, advocating for more equitable AI systems.
</details>

<details>
    <summary>Key points</summary>
    * Design Component Inclusion Score (CIS) metric
    * Analyze biases with 2,400 images
    * Provide insights for architectural and data-centric interventions
</details>
</details>

---


<details>
<summary><b> Improving Physical Object State Representation in Text-to-Image Generative Systems</b></summary>

* **Authors:** Tianle Chen, Chaitanya Chakka, Deepti Ghadiyaram
* **arXiv ID:** 2505.02236
* **One-liner:** Improved generation of object states in text-to-image models.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2505.02236) | [[PDF]](https://arxiv.org/pdf/2505.02236)

> **Core Innovation**
> Created a pipeline for synthetic data and fine-tuned models to better represent object states.

<details>
    <summary>Abstract</summary>
    Current text-to-image generative models struggle to accurately represent object states (e.g., &#34;a table without a bottle,&#34; &#34;an empty tumbler&#34;). In this work, we first design a fully-automatic pipeline to generate high-quality synthetic data that accurately captures objects in varied states. Next, we fine-tune several open-source text-to-image models on this synthetic data. We evaluate the performance of the fine-tuned models by quantifying the alignment of the generated images to their prompts using GPT4o-mini, and achieve an average absolute improvement of 8+% across four models on the public GenAI-Bench dataset. We also curate a collection of 200 prompts with a specific focus on common objects in various physical states. We demonstrate a significant improvement of an average of 24+% over the baseline on this dataset. We release all evaluation prompts and code.
</details>

<details>
    <summary>Key points</summary>
    * Design automatic pipeline for synthetic data generation
    * Fine-tune models on synthetic data
    * Evaluate with GPT4o-mini and custom datasets
</details>
</details>

---


<details>
<summary><b> MCCD: Multi-Agent Collaboration-based Compositional Diffusion for Complex Text-to-Image Generation</b></summary>

* **Authors:** Mingcheng Li, Xiaolu Hou, Ziyang Liu, Dingkang Yang, Ziyun Qian, Jiawei Chen, Jinjie Wei, Yue Jiang, Qingyao Xu, Lihua Zhang
* **arXiv ID:** 2505.02648
* **One-liner:** Enhanced complex scene generation with multi-agent collaboration.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2505.02648) | [[PDF]](https://arxiv.org/pdf/2505.02648)

> **Core Innovation**
> Proposed MCCD using multi-agent scene parsing and hierarchical compositional diffusion.

<details>
    <summary>Abstract</summary>
    Diffusion models have shown excellent performance in text-to-image generation. Nevertheless, existing methods often suffer from performance bottlenecks when handling complex prompts that involve multiple objects, characteristics, and relations. Therefore, we propose a Multi-agent Collaboration-based Compositional Diffusion (MCCD) for text-to-image generation for complex scenes. Specifically, we design a multi-agent collaboration-based scene parsing module that generates an agent system comprising multiple agents with distinct tasks, utilizing MLLMs to extract various scene elements effectively. In addition, Hierarchical Compositional diffusion utilizes a Gaussian mask and filtering to refine bounding box regions and enhance objects through region enhancement, resulting in the accurate and high-fidelity generation of complex scenes. Comprehensive experiments demonstrate that our MCCD significantly improves the performance of the baseline models in a training-free manner, providing a substantial advantage in complex scene generation.
</details>

<details>
    <summary>Key points</summary>
    * Multi-agent collaboration for scene parsing
    * Hierarchical compositional diffusion with Gaussian mask
    * Training-free improvement for complex scenes
</details>
</details>

---


<details>
<summary><b> HCMA: Hierarchical Cross-model Alignment for Grounded Text-to-Image Generation</b></summary>

* **Authors:** Hang Wang, Zhi-Qi Cheng, Chenhao Lin, Chao Shen, Lei Zhang
* **arXiv ID:** 2505.06512
* **One-liner:** Achieved better spatial control and semantic fidelity in text-to-image generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2505.06512) | [[PDF]](https://arxiv.org/pdf/2505.06512)

> **Core Innovation**
> Introduced HCMA framework with global and local alignment modules for grounded generation.

<details>
    <summary>Abstract</summary>
    Text-to-image synthesis has progressed to the point where models can generate visually compelling images from natural language prompts. Yet, existing methods often fail to reconcile high-level semantic fidelity with explicit spatial control, particularly in scenes involving multiple objects, nuanced relations, or complex layouts. To bridge this gap, we propose a Hierarchical Cross-Modal Alignment (HCMA) framework for grounded text-to-image generation. HCMA integrates two alignment modules into each diffusion sampling step: a global module that continuously aligns latent representations with textual descriptions to ensure scene-level coherence, and a local module that employs bounding-box layouts to anchor objects at specified locations, enabling fine-grained spatial control. Extensive experiments on the MS-COCO 2014 validation set show that HCMA surpasses state-of-the-art baselines, achieving a 0.69 improvement in Frechet Inception Distance (FID) and a 0.0295 gain in CLIP Score. These results demonstrate HCMA&#39;s effectiveness in faithfully capturing intricate textual semantics while adhering to user-defined spatial constraints, offering a robust solution for semantically grounded image generation. Our code is available at <a href="https://github.com/hwang-cs-ime/HCMA" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Global alignment for scene-level coherence
    * Local alignment with bounding-box layouts
    * Integrate modules into diffusion sampling steps
</details>
</details>

---


<details>
<summary><b> IMAGE-ALCHEMY: Advancing subject fidelity in personalised text-to-image generation</b></summary>

* **Authors:** Amritanshu Tiwari, Cherish Puniani, Kaustubh Sharma, Ojasva Nema
* **arXiv ID:** 2505.10743
* **One-liner:** Improved personalization in text-to-image models with reduced forgetting.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2505.10743) | [[PDF]](https://arxiv.org/pdf/2505.10743)

> **Core Innovation**
> Developed a two-stage pipeline using LoRA fine-tuning and segmentation for subject integration.

<details>
    <summary>Abstract</summary>
    Recent advances in text-to-image diffusion models, particularly Stable Diffusion, have enabled the generation of highly detailed and semantically rich images. However, personalizing these models to represent novel subjects based on a few reference images remains challenging. This often leads to catastrophic forgetting, overfitting, or large computational <a href="http://overhead.We" rel="external noopener nofollow" class="link-external link-http">this http URL</a> propose a two-stage pipeline that addresses these limitations by leveraging LoRA-based fine-tuning on the attention weights within the U-Net of the Stable Diffusion XL (SDXL) model. First, we use the unmodified SDXL to generate a generic scene by replacing the subject with its class label. Then, we selectively insert the personalized subject through a segmentation-driven image-to-image (Img2Img) pipeline that uses the trained LoRA <a href="http://weights.This" rel="external noopener nofollow" class="link-external link-http">this http URL</a> framework isolates the subject encoding from the overall composition, thus preserving SDXL&#39;s broader generative capabilities while integrating the new subject in a high-fidelity manner. Our method achieves a DINO similarity score of 0.789 on SDXL, outperforming existing personalized text-to-image approaches.
</details>

<details>
    <summary>Key points</summary>
    * LoRA-based fine-tuning on attention weights
    * Segmentation-driven image-to-image pipeline
    * Isolate subject encoding to preserve generative capabilities
</details>
</details>

---


<details>
<summary><b> CompAlign: Improving Compositional Text-to-Image Generation with a Complex Benchmark and Fine-Grained Feedback</b></summary>

* **Authors:** Yixin Wan, Kai-Wei Chang
* **arXiv ID:** 2505.11178
* **One-liner:** Advanced evaluation and improvement of compositional image generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2505.11178) | [[PDF]](https://arxiv.org/pdf/2505.11178)

> **Core Innovation**
> Introduced CompAlign benchmark and CompQuest framework for fine-grained assessment and alignment.

<details>
    <summary>Abstract</summary>
    State-of-the-art T2I models are capable of generating high-resolution images given textual prompts. However, they still struggle with accurately depicting compositional scenes that specify multiple objects, attributes, and spatial relations. We present CompAlign, a challenging benchmark with an emphasis on assessing the depiction of 3D-spatial relationships, for evaluating and improving models on compositional image generation. CompAlign consists of 900 complex multi-subject image generation prompts that combine numerical and 3D-spatial relationships with varied attribute bindings. Our benchmark is remarkably challenging, incorporating generation tasks with 3+ generation subjects with complex 3D-spatial relationships. Additionally, we propose CompQuest, an interpretable and accurate evaluation framework that decomposes complex prompts into atomic sub-questions, then utilizes a MLLM to provide fine-grained binary feedback on the correctness of each aspect of generation elements in model-generated images. This enables precise quantification of alignment between generated images and compositional prompts. Furthermore, we propose an alignment framework that uses CompQuest&#39;s feedback as preference signals to improve diffusion models&#39; compositional image generation abilities. Using adjustable per-image preferences, our method is easily scalable and flexible for different tasks. Evaluation of 9 T2I models reveals that: (1) models remarkable struggle more with compositional tasks with more complex 3D-spatial configurations, and (2) a noticeable performance gap exists between open-source accessible models and closed-source commercial models. Further empirical study on using CompAlign for model alignment yield promising results: post-alignment diffusion models achieve remarkable improvements in compositional accuracy, especially on complex generation tasks, outperforming previous approaches.
</details>

<details>
    <summary>Key points</summary>
    * Create CompAlign benchmark with complex prompts
    * Develop CompQuest for atomic sub-question evaluation
    * Use feedback for model alignment and improvement
</details>
</details>

---


<details>
<summary><b> Diff-MM: Exploring Pre-trained Text-to-Image Generation Model for Unified Multi-modal Object Tracking</b></summary>

* **Authors:** Shiyu Xuan, Zechao Li, Jinhui Tang
* **arXiv ID:** 2505.12606
* **One-liner:** Enhanced multi-modal object tracking using text-to-image model features.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2505.12606) | [[PDF]](https://arxiv.org/pdf/2505.12606)

> **Core Innovation**
> Proposed Diff-MM, leveraging Stable Diffusion UNet for unified multi-modal tracking.

<details>
    <summary>Abstract</summary>
    Multi-modal object tracking integrates auxiliary modalities such as depth, thermal infrared, event flow, and language to provide additional information beyond RGB images, showing great potential in improving tracking stabilization in complex scenarios. Existing methods typically start from an RGB-based tracker and learn to understand auxiliary modalities only from training data. Constrained by the limited multi-modal training data, the performance of these methods is unsatisfactory. To alleviate this limitation, this work proposes a unified multi-modal tracker Diff-MM by exploiting the multi-modal understanding capability of the pre-trained text-to-image generation model. Diff-MM leverages the UNet of pre-trained Stable Diffusion as a tracking feature extractor through the proposed parallel feature extraction pipeline, which enables pairwise image inputs for object tracking. We further introduce a multi-modal sub-module tuning method that learns to gain complementary information between different modalities. By harnessing the extensive prior knowledge in the generation model, we achieve a unified tracker with uniform parameters for RGB-N/D/T/E tracking. Experimental results demonstrate the promising performance of our method compared with recently proposed trackers, e.g., its AUC outperforms OneTracker by 8.3% on TNL2K.
</details>

<details>
    <summary>Key points</summary>
    * Use UNet from Stable Diffusion as feature extractor
    * Parallel feature extraction pipeline
    * Multi-modal sub-module tuning for complementary information
</details>
</details>

---


<details>
<summary><b> Emerging Properties in Unified Multimodal Pretraining</b></summary>

* **Authors:** Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, Guang Shi, Haoqi Fan
* **arXiv ID:** 2505.14683
* **One-liner:** Developed an open-source unified multimodal model with advanced reasoning.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2505.14683) | [[PDF]](https://arxiv.org/pdf/2505.14683)

> **Core Innovation**
> Introduced BAGEL, a decoder-only model pretrained on diverse multimodal data for understanding and generation.

<details>
    <summary>Abstract</summary>
    Unifying multimodal understanding and generation has shown impressive capabilities in cutting-edge proprietary systems. In this work, we introduce BAGEL, an open-source foundational model that natively supports multimodal understanding and generation. BAGEL is a unified, decoder-only model pretrained on trillions of tokens curated from large-scale interleaved text, image, video, and web data. When scaled with such diverse multimodal interleaved data, BAGEL exhibits emerging capabilities in complex multimodal reasoning. As a result, it significantly outperforms open-source unified models in both multimodal generation and understanding across standard benchmarks, while exhibiting advanced multimodal reasoning abilities such as free-form image manipulation, future frame prediction, 3D manipulation, and world navigation. In the hope of facilitating further opportunities for multimodal research, we share the key findings, pretraining details, data creation protocal, and release our code and checkpoints to the community. The project page is at <a href="https://bagel-ai.org/" rel="external noopener nofollow" class="link-external link-https">this https URL</a>
</details>

<details>
    <summary>Key points</summary>
    * Pretrain on trillions of tokens from interleaved data
    * Support multimodal understanding and generation
    * Exhibit emerging capabilities in complex reasoning
</details>
</details>

---


<details>
<summary><b> Harnessing Caption Detailness for Data-Efficient Text-to-Image Generation</b></summary>

* **Authors:** Xinran Wang, Muxi Diao, Yuanzhi Liu, Chunyu Wang, Kongming Liang, Zhanyu Ma, Jun Guo
* **arXiv ID:** 2505.15172
* **One-liner:** Proposed a new metric for caption detailness in T2I training, improving model performance with efficient data selection.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2505.15172) | [[PDF]](https://arxiv.org/pdf/2505.15172)

> **Core Innovation**
> Introduced ICR and AOD metrics to evaluate caption detailness, enabling superior T2I model training on high-detail captions.

<details>
    <summary>Abstract</summary>
    Training text-to-image (T2I) models with detailed captions can significantly improve their generation quality. Existing methods often rely on simplistic metrics like caption length to represent the detailness of the caption in the T2I training set. In this paper, we propose a new metric to estimate caption detailness based on two aspects: image coverage rate (ICR), which evaluates whether the caption covers all regions/objects in the image, and average object detailness (AOD), which quantifies the detailness of each object&#39;s description. Through experiments on the COCO dataset using ShareGPT4V captions, we demonstrate that T2I models trained on high-ICR and -AOD captions achieve superior performance on DPG and other benchmarks. Notably, our metric enables more effective data selection-training on only 20% of full data surpasses both full-dataset training and length-based selection method, improving alignment and reconstruction ability. These findings highlight the critical role of detail-aware metrics over length-based heuristics in caption selection for T2I tasks.
</details>

<details>
    <summary>Key points</summary>
    * Defined image coverage rate (ICR) to assess caption coverage of image regions.
    * Quantified average object detailness (AOD) for object description detail.
    * Demonstrated training on 20% high-detail data outperforms full-dataset and length-based methods.
</details>
</details>

---


<details>
<summary><b> IA-T2I: Internet-Augmented Text-to-Image Generation</b></summary>

* **Authors:** Chuanhao Li, Jianwen Sun, Yukang Feng, Mingliang Zhai, Yifan Chang, Kaipeng Zhang
* **arXiv ID:** 2505.15779
* **One-liner:** Developed an Internet-Augmented T2I framework to handle uncertain knowledge in prompts using reference images.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2505.15779) | [[PDF]](https://arxiv.org/pdf/2505.15779)

> **Core Innovation**
> Integrated active retrieval, hierarchical image selection, and self-reflection to enhance T2I generation for uncertain scenarios.

<details>
    <summary>Abstract</summary>
    Current text-to-image (T2I) generation models achieve promising results, but they fail on the scenarios where the knowledge implied in the text prompt is uncertain. For example, a T2I model released in February would struggle to generate a suitable poster for a movie premiering in April, because the character designs and styles are uncertain to the model. To solve this problem, we propose an Internet-Augmented text-to-image generation (IA-T2I) framework to compel T2I models clear about such uncertain knowledge by providing them with reference images. Specifically, an active retrieval module is designed to determine whether a reference image is needed based on the given text prompt; a hierarchical image selection module is introduced to find the most suitable image returned by an image search engine to enhance the T2I model; a self-reflection mechanism is presented to continuously evaluate and refine the generated image to ensure faithful alignment with the text prompt. To evaluate the proposed framework;s performance, we collect a dataset named Img-Ref-T2I, where text prompts include three types of uncertain knowledge: (1) known but rare. (2) unknown. (3) ambiguous. Moreover, we carefully craft a complex prompt to guide GPT-4o in making preference evaluation, which has been shown to have an evaluation accuracy similar to that of human preference evaluation. Experimental results demonstrate the effectiveness of our framework, outperforming GPT-4o by about 30% in human evaluation.
</details>

<details>
    <summary>Key points</summary>
    * Designed active retrieval module to determine need for reference images.
    * Introduced hierarchical image selection for suitable image retrieval.
    * Implemented self-reflection mechanism for continuous image refinement.
</details>
</details>

---


<details>
<summary><b> Self-Rewarding Large Vision-Language Models for Optimizing Prompts in Text-to-Image Generation</b></summary>

* **Authors:** Hongji Yang, Yucheng Zhou, Wencheng Han, Jianbing Shen
* **arXiv ID:** 2505.16763
* **One-liner:** Created a prompt optimization framework using LVLMs for AI feedback, reducing reliance on human annotations.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2505.16763) | [[PDF]](https://arxiv.org/pdf/2505.16763)

> **Core Innovation**
> Employed LVLMs as both solver and reward model in reinforcement learning for self-improving prompt rewriting.

<details>
    <summary>Abstract</summary>
    Text-to-image models are powerful for producing high-quality images based on given text prompts, but crafting these prompts often requires specialized vocabulary. To address this, existing methods train rewriting models with supervision from large amounts of manually annotated data and trained aesthetic assessment models. To alleviate the dependence on data scale for model training and the biases introduced by trained models, we propose a novel prompt optimization framework, designed to rephrase a simple user prompt into a sophisticated prompt to a text-to-image model. Specifically, we employ the large vision language models (LVLMs) as the solver to rewrite the user prompt, and concurrently, employ LVLMs as a reward model to score the aesthetics and alignment of the images generated by the optimized prompt. Instead of laborious human feedback, we exploit the prior knowledge of the LVLM to provide rewards, i.e., AI feedback. Simultaneously, the solver and the reward model are unified into one model and iterated in reinforcement learning to achieve self-improvement by giving a solution and judging itself. Results on two popular datasets demonstrate that our method outperforms other strong competitors.
</details>

<details>
    <summary>Key points</summary>
    * Used LVLMs to rewrite user prompts into sophisticated versions.
    * Applied LVLMs as reward models to score image aesthetics and alignment.
    * Integrated solver and reward model in RL for iterative self-improvement.
</details>
</details>

---


<details>
<summary><b> RePrompt: Reasoning-Augmented Reprompting for Text-to-Image Generation via Reinforcement Learning</b></summary>

* **Authors:** Mingrui Wu, Lu Wang, Pu Zhao, Fangkai Yang, Jianjin Zhang, Jianfeng Liu, Yuefeng Zhan, Weihao Han, Hao Sun, Jiayi Ji, Xiaoshuai Sun, Qingwei Lin, Weiwei Deng, Dongmei Zhang, Feng Sun, Qi Zhang, Rongrong Ji
* **arXiv ID:** 2505.17540
* **One-liner:** Introduced RePrompt, a reinforcement learning-based reprompting framework for better T2I alignment and composition.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2505.17540) | [[PDF]](https://arxiv.org/pdf/2505.17540)

> **Core Innovation**
> Trained a language model to generate structured, self-reflective prompts optimized for image outcomes.

<details>
    <summary>Abstract</summary>
    Despite recent progress in text-to-image (T2I) generation, existing models often struggle to faithfully capture user intentions from short and under-specified prompts. While prior work has attempted to enhance prompts using large language models (LLMs), these methods frequently generate stylistic or unrealistic content due to insufficient grounding in visual semantics and real-world composition. Inspired by recent advances in reasoning for language model, we propose RePrompt, a novel reprompting framework that introduces explicit reasoning into the prompt enhancement process via reinforcement learning. Instead of relying on handcrafted rules or stylistic rewrites, our method trains a language model to generate structured, self-reflective prompts by optimizing for image-level outcomes. The tailored reward models assesse the generated images in terms of human preference, semantic alignment, and visual composition, providing indirect supervision to refine prompt generation. Our approach enables end-to-end training without human-annotated data. Experiments on GenEval and T2I-Compbench show that RePrompt significantly boosts spatial layout fidelity and compositional generalization across diverse T2I backbones, establishing new state-of-the-art results.
</details>

<details>
    <summary>Key points</summary>
    * Used reinforcement learning to train prompt generation without human data.
    * Developed reward models for human preference, semantic alignment, and visual composition.
    * Enhanced spatial layout fidelity and compositional generalization in T2I models.
</details>
</details>

---


<details>
<summary><b> Align Beyond Prompts: Evaluating World Knowledge Alignment in Text-to-Image Generation</b></summary>

* **Authors:** Wenchao Zhang, Jiahe Tian, Runze He, Jizhong Han, Jiao Dai, Miaomiao Feng, Wei Mi, Xiaodan Zhang
* **arXiv ID:** 2505.18730
* **One-liner:** Established the ABP benchmark to evaluate T2I model alignment with real-world knowledge beyond prompts.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2505.18730) | [[PDF]](https://arxiv.org/pdf/2505.18730)

> **Core Innovation**
> Proposed ABPScore metric and ITKI strategy for improving knowledge integration in T2I generation.

<details>
    <summary>Abstract</summary>
    Recent text-to-image (T2I) generation models have advanced significantly, enabling the creation of high-fidelity images from textual prompts. However, existing evaluation benchmarks primarily focus on the explicit alignment between generated images and prompts, neglecting the alignment with real-world knowledge beyond prompts. To address this gap, we introduce Align Beyond Prompts (ABP), a comprehensive benchmark designed to measure the alignment of generated images with real-world knowledge that extends beyond the explicit user prompts. ABP comprises over 2,000 meticulously crafted prompts, covering real-world knowledge across six distinct scenarios. We further introduce ABPScore, a metric that utilizes existing Multimodal Large Language Models (MLLMs) to assess the alignment between generated images and world knowledge beyond prompts, which demonstrates strong correlations with human judgments. Through a comprehensive evaluation of 8 popular T2I models using ABP, we find that even state-of-the-art models, such as GPT-4o, face limitations in integrating simple real-world knowledge into generated images. To mitigate this issue, we introduce a training-free strategy within ABP, named Inference-Time Knowledge Injection (ITKI). By applying this strategy to optimize 200 challenging samples, we achieved an improvement of approximately 43% in ABPScore. The dataset and code are available in <a href="https://github.com/smile365317/ABP" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Created ABP benchmark with over 2,000 prompts across six scenarios.
    * Introduced ABPScore using MLLMs for alignment assessment.
    * Developed Inference-Time Knowledge Injection (ITKI) for training-free improvement.
</details>
</details>

---


<details>
<summary><b> Training-free Stylized Text-to-Image Generation with Fast Inference</b></summary>

* **Authors:** Xin Ma, Yaohui Wang, Xinyuan Chen, Tien-Tsin Wong, Cunjian Chen
* **arXiv ID:** 2505.19063
* **One-liner:** Proposed OmniPainter for stylized image generation without fine-tuning, using pre-trained diffusion models.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2505.19063) | [[PDF]](https://arxiv.org/pdf/2505.19063)

> **Core Innovation**
> Leveraged self-consistency of latent consistency models and norm mixture of self-attention for style transfer.

<details>
    <summary>Abstract</summary>
    Although diffusion models exhibit impressive generative capabilities, existing methods for stylized image generation based on these models often require textual inversion or fine-tuning with style images, which is time-consuming and limits the practical applicability of large-scale diffusion models. To address these challenges, we propose a novel stylized image generation method leveraging a pre-trained large-scale diffusion model without requiring fine-tuning or any additional optimization, termed as OmniPainter. Specifically, we exploit the self-consistency property of latent consistency models to extract the representative style statistics from reference style images to guide the stylization process. Additionally, we then introduce the norm mixture of self-attention, which enables the model to query the most relevant style patterns from these statistics for the intermediate output content features. This mechanism also ensures that the stylized results align closely with the distribution of the reference style images. Our qualitative and quantitative experimental results demonstrate that the proposed method outperforms state-of-the-art approaches.
</details>

<details>
    <summary>Key points</summary>
    * Extracted style statistics from reference images using self-consistency property.
    * Introduced norm mixture of self-attention for querying relevant style patterns.
    * Ensured stylized results align with reference style distribution.
</details>
</details>

---


<details>
<summary><b> Alchemist: Turning Public Text-to-Image Data into Generative Gold</b></summary>

* **Authors:** Valerii Startsev, Alexander Ustyuzhanin, Alexey Kirillov, Dmitry Baranchuk, Sergey Kastryulin
* **arXiv ID:** 2505.19297
* **One-liner:** Introduced a methodology for creating high-impact SFT datasets using pre-trained models, releasing Alchemist dataset.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2505.19297) | [[PDF]](https://arxiv.org/pdf/2505.19297)

> **Core Innovation**
> Leveraged generative models to estimate effective training samples for T2I model fine-tuning.

<details>
    <summary>Abstract</summary>
    Pre-training equips text-to-image (T2I) models with broad world knowledge, but this alone is often insufficient to achieve high aesthetic quality and alignment. Consequently, supervised fine-tuning (SFT) is crucial for further refinement. However, its effectiveness highly depends on the quality of the fine-tuning dataset. Existing public SFT datasets frequently target narrow domains (e.g., anime or specific art styles), and the creation of high-quality, general-purpose SFT datasets remains a significant challenge. Current curation methods are often costly and struggle to identify truly impactful samples. This challenge is further complicated by the scarcity of public general-purpose datasets, as leading models often rely on large, proprietary, and poorly documented internal data, hindering broader research progress. This paper introduces a novel methodology for creating general-purpose SFT datasets by leveraging a pre-trained generative model as an estimator of high-impact training samples. We apply this methodology to construct and release Alchemist, a compact (3,350 samples) yet highly effective SFT dataset. Experiments demonstrate that Alchemist substantially improves the generative quality of five public T2I models while preserving diversity and style. Additionally, we release the fine-tuned models&#39; weights to the public.
</details>

<details>
    <summary>Key points</summary>
    * Used pre-trained generative model to identify high-impact SFT samples.
    * Constructed Alchemist dataset with 3,350 samples.
    * Demonstrated improved generative quality and diversity in public T2I models.
</details>
</details>

---


<details>
<summary><b> StyleAR: Customizing Multimodal Autoregressive Model for Style-Aligned Text-to-Image Generation</b></summary>

* **Authors:** Yi Wu, Lingting Zhu, Shengju Qian, Lei Liu, Wandi Qiao, Lequan Yu, Bin Li
* **arXiv ID:** 2505.19874
* **One-liner:** Developed StyleAR for style-aligned T2I generation using binary data and enhanced AR models.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2505.19874) | [[PDF]](https://arxiv.org/pdf/2505.19874)

> **Core Innovation**
> Combined data curation with AR models to utilize text-image binary data for style consistency.

<details>
    <summary>Abstract</summary>
    In the current research landscape, multimodal autoregressive (AR) models have shown exceptional capabilities across various domains, including visual understanding and generation. However, complex tasks such as style-aligned text-to-image generation present significant challenges, particularly in data acquisition. In analogy to instruction-following tuning for image editing of AR models, style-aligned generation requires a reference style image and prompt, resulting in a text-image-to-image triplet where the output shares the style and semantics of the input. However, acquiring large volumes of such triplet data with specific styles is considerably more challenging than obtaining conventional text-to-image data used for training generative models. To address this issue, we propose StyleAR, an innovative approach that combines a specially designed data curation method with our proposed AR models to effectively utilize text-to-image binary data for style-aligned text-to-image generation. Our method synthesizes target stylized data using a reference style image and prompt, but only incorporates the target stylized image as the image modality to create high-quality binary data. To facilitate binary data training, we introduce a CLIP image encoder with a perceiver resampler that translates the image input into style tokens aligned with multimodal tokens in AR models and implement a style-enhanced token technique to prevent content leakage which is a common issue in previous work. Furthermore, we mix raw images drawn from large-scale text-image datasets with stylized images to enhance StyleAR&#39;s ability to extract richer stylistic features and ensure style consistency. Extensive qualitative and quantitative experiments demonstrate our superior performance.
</details>

<details>
    <summary>Key points</summary>
    * Designed data curation method to synthesize stylized binary data.
    * Introduced CLIP encoder with perceiver resampler for style token alignment.
    * Used style-enhanced tokens to prevent content leakage and mix raw images for richer features.
</details>
</details>

---


<details>
<summary><b> Identity-Preserving Text-to-Image Generation via Dual-Level Feature Decoupling and Expert-Guided Fusion</b></summary>

* **Authors:** Kewen Chen, Xiaobin Hu, Wenqi Ren
* **arXiv ID:** 2505.22360
* **One-liner:** Proposed a framework for subject-driven T2I generation with improved identity feature disentanglement and fusion.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2505.22360) | [[PDF]](https://arxiv.org/pdf/2505.22360)

> **Core Innovation**
> Introduced IEDM for implicit-explicit decoupling and FFM with MoE for feature integration.

<details>
    <summary>Abstract</summary>
    Recent advances in large-scale text-to-image generation models have led to a surge in subject-driven text-to-image generation, which aims to produce customized images that align with textual descriptions while preserving the identity of specific subjects. Despite significant progress, current methods struggle to disentangle identity-relevant information from identity-irrelevant details in the input images, resulting in overfitting or failure to maintain subject identity. In this work, we propose a novel framework that improves the separation of identity-related and identity-unrelated features and introduces an innovative feature fusion mechanism to improve the quality and text alignment of generated images. Our framework consists of two key components: an Implicit-Explicit foreground-background Decoupling Module (IEDM) and a Feature Fusion Module (FFM) based on a Mixture of Experts (MoE). IEDM combines learnable adapters for implicit decoupling at the feature level with inpainting techniques for explicit foreground-background separation at the image level. FFM dynamically integrates identity-irrelevant features with identity-related features, enabling refined feature representations even in cases of incomplete decoupling. In addition, we introduce three complementary loss functions to guide the decoupling process. Extensive experiments demonstrate the effectiveness of our proposed method in enhancing image generation quality, improving flexibility in scene adaptation, and increasing the diversity of generated outputs across various textual descriptions.
</details>

<details>
    <summary>Key points</summary>
    * Developed Implicit-Explicit Decoupling Module (IEDM) for foreground-background separation.
    * Created Feature Fusion Module (FFM) using Mixture of Experts (MoE).
    * Applied complementary loss functions to guide decoupling and enhance generation quality.
</details>
</details>

---


<details>
<summary><b> HiDream-I1: A High-Efficient Image Generative Foundation Model with Sparse Diffusion Transformer</b></summary>

* **Authors:** Qi Cai, Jingwen Chen, Yang Chen, Yehao Li, Fuchen Long, Yingwei Pan, Zhaofan Qiu, Yiheng Zhang, Fengbin Gao, Peihan Xu, Yimeng Wang, Kai Yu, Wenxuan Chen, Ziwei Feng, Zijian Gong, Jianzhuang Pan, Yi Peng, Rui Tian, Siyu Wang, Bo Zhao, Ting Yao, Tao Mei
* **arXiv ID:** 2505.22705
* **One-liner:** Introduced HiDream-I1, a sparse DiT-based model for fast, high-quality image generation and editing.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2505.22705) | [[PDF]](https://arxiv.org/pdf/2505.22705)

> **Core Innovation**
> Built a dual-stream and single-stream sparse DiT with MoE for efficient multimodal interaction.

<details>
    <summary>Abstract</summary>
    Recent advancements in image generative foundation models have prioritized quality improvements but often at the cost of increased computational complexity and inference latency. To address this critical trade-off, we introduce HiDream-I1, a new open-source image generative foundation model with 17B parameters that achieves state-of-the-art image generation quality within seconds. HiDream-I1 is constructed with a new sparse Diffusion Transformer (DiT) structure. Specifically, it starts with a dual-stream decoupled design of sparse DiT with dynamic Mixture-of-Experts (MoE) architecture, in which two separate encoders are first involved to independently process image and text tokens. Then, a single-stream sparse DiT structure with dynamic MoE architecture is adopted to trigger multi-model interaction for image generation in a cost-efficient manner. To support flexiable accessibility with varied model capabilities, we provide HiDream-I1 in three variants: HiDream-I1-Full, HiDream-I1-Dev, and HiDream-I1-Fast.
<br>Furthermore, we go beyond the typical text-to-image generation and remould HiDream-I1 with additional image conditions to perform precise, instruction-based editing on given images, yielding a new instruction-based image editing model namely HiDream-E1. Ultimately, by integrating text-to-image generation and instruction-based image editing, HiDream-I1 evolves to form a comprehensive image agent (HiDream-A1) capable of fully interactive image creation and refinement. To accelerate multi-modal AIGC research, we have open-sourced all the codes and model weights of HiDream-I1-Full, HiDream-I1-Dev, HiDream-I1-Fast, HiDream-E1 through our project websites: <a href="https://github.com/HiDream-ai/HiDream-I1" rel="external noopener nofollow" class="link-external link-https">this https URL</a> and <a href="https://github.com/HiDream-ai/HiDream-E1" rel="external noopener nofollow" class="link-external link-https">this https URL</a>. All features can be directly experienced via <a href="https://vivago.ai/studio" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Designed sparse Diffusion Transformer (DiT) with dynamic MoE architecture.
    * Provided variants (Full, Dev, Fast) for flexible model capabilities.
    * Extended to instruction-based image editing (HiDream-E1) and interactive agent (HiDream-A1).
</details>
</details>

---


<details>
<summary><b> Rhetorical Text-to-Image Generation via Two-layer Diffusion Policy Optimization</b></summary>

* **Authors:** Yuxi Zhang, Yueting Li, Xinyu Du, Sibo Wang
* **arXiv ID:** 2505.22792
* **One-liner:** Proposed Rhet2Pix, a framework that improves rhetorical text-to-image generation by formulating it as a multi-step policy optimization problem.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2505.22792) | [[PDF]](https://arxiv.org/pdf/2505.22792)

> **Core Innovation**
> Rhet2Pix addresses the challenge of generating images from rhetorical language by using a two-layer MDP diffusion module to elaborate sub-sentences and optimize actions, outperforming SOTA models.

<details>
    <summary>Abstract</summary>
    Generating images from rhetorical languages remains a critical challenge for text-to-image models. Even state-of-the-art (SOTA) multimodal large language models (MLLM) fail to generate images based on the hidden meaning inherent in rhetorical language--despite such content being readily mappable to visual representations by humans. A key limitation is that current models emphasize object-level word embedding alignment, causing metaphorical expressions to steer image generation towards their literal visuals and overlook the intended semantic meaning. To address this, we propose Rhet2Pix, a framework that formulates rhetorical text-to-image generation as a multi-step policy optimization problem, incorporating a two-layer MDP diffusion module. In the outer layer, Rhet2Pix converts the input prompt into incrementally elaborated sub-sentences and executes corresponding image-generation actions, constructing semantically richer visuals. In the inner layer, Rhet2Pix mitigates reward sparsity during image generation by discounting the final reward and optimizing every adjacent action pair along the diffusion denoising trajectory. Extensive experiments demonstrate the effectiveness of Rhet2Pix in rhetorical text-to-image generation. Our model outperforms SOTA MLLMs such as GPT-4o, Grok-3 and leading academic baselines across both qualitative and quantitative evaluations. The code and dataset used in this work are publicly available.
</details>

<details>
    <summary>Key points</summary>
    * Formulates rhetorical text-to-image generation as a multi-step policy optimization problem
    * Incorporates a two-layer MDP diffusion module with outer and inner layers
    * Converts input prompts into incrementally elaborated sub-sentences
    * Mitigates reward sparsity by discounting final reward and optimizing adjacent action pairs
</details>
</details>

---


<details>
<summary><b> Muddit: Liberating Generation Beyond Text-to-Image with a Unified Discrete Diffusion Model</b></summary>

* **Authors:** Qingyu Shi, Jinbin Bai, Zhuoran Zhao, Wenhao Chai, Kaidong Yu, Jianzong Wu, Shuangyong Song, Yunhai Tong, Xiangtai Li, Xuelong Li, Shuicheng Yan
* **arXiv ID:** 2505.23606
* **One-liner:** Introduced Muddit, a unified discrete diffusion transformer for fast and parallel generation across text and image modalities.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2505.23606) | [[PDF]](https://arxiv.org/pdf/2505.23606)

> **Core Innovation**
> Muddit integrates strong visual priors from a pretrained text-to-image backbone with a lightweight text decoder, achieving competitive performance in quality and efficiency.

<details>
    <summary>Abstract</summary>
    Unified generation models aim to handle diverse tasks across modalities -- such as text generation, image generation, and vision-language reasoning -- within a single architecture and decoding paradigm. Autoregressive unified models suffer from slow inference due to sequential decoding, and non-autoregressive unified models suffer from weak generalization due to limited pretrained backbones. We introduce Muddit, a unified discrete diffusion transformer that enables fast and parallel generation across both text and image modalities. Unlike prior unified diffusion models trained from scratch, Muddit integrates strong visual priors from a pretrained text-to-image backbone with a lightweight text decoder, enabling flexible and high-quality multimodal generation under a unified architecture. Empirical results show that Muddit achieves competitive or superior performance compared to significantly larger autoregressive models in both quality and efficiency. The work highlights the potential of purely discrete diffusion, when equipped with strong visual priors, as a scalable and effective backbone for unified generation.
</details>

<details>
    <summary>Key points</summary>
    * Uses a unified discrete diffusion transformer for multimodal generation
    * Integrates strong visual priors from a pretrained backbone
    * Enables fast and parallel generation across text and image modalities
    * Highlights the potential of discrete diffusion with visual priors
</details>
</details>

---


<details>
<summary><b> OSPO: Object-centric Self-improving Preference Optimization for Text-to-Image Generation</b></summary>

* **Authors:** Yoonjin Oh, Yongjin Kim, Hyomin Kim, Donghwan Chi, Sungwoong Kim
* **arXiv ID:** 2506.02015
* **One-liner:** Developed OSPO, an object-centric self-improving framework to enhance fine-grained text-image alignment in text-to-image generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2506.02015) | [[PDF]](https://arxiv.org/pdf/2506.02015)

> **Core Innovation**
> OSPO addresses object hallucination by constructing hard negative data and using object-centric preference optimization, surpassing prior methods in fine-grained alignment.

<details>
    <summary>Abstract</summary>
    Recent advances in Multimodal Large Language Models (MLLMs) have enabled models to perform both understanding and generation of multimodal data in a unified manner. However, achieving a fine-grained alignment between input prompts and generated images remains a major challenge especially in text-to-image generation. Therefore, recent works have introduced self-improving mechanisms based on self-generated data and self-feedback to efficiently mitigate this challenge without relying on external large-scale data or models. However, existing self-improving approaches have not focused on fine-grained visual details especially at the object level in generating training data or providing a feedback, and thus they still struggle to resolve the object hallucination problem in text-to-image generation. To tackle this problem, we propose an Object-centric Self-improving Preference Optimization (OSPO), a self-improving framework for enhancing object-level text-image alignment. OSPO is designed to explicitly address the need for constructing and leveraging object-level hard negative data and an object-centric optimization in improving object-specific fidelity. In specific, OSPO consists of: (1) Initial Prompt Generation (2) Hard Preference Pair Generation (3) Filtering and Selection (4) Object-centric Preference Optimization with Conditional Preference Loss. Extensive experiments on compositional image generation benchmarks demonstrate that OSPO significantly improves fine-grained alignment in text-to-image generation, surpassing not only prior self-improving methods but also diffusion-based specialized image generation models.
</details>

<details>
    <summary>Key points</summary>
    * Introduces Object-centric Self-improving Preference Optimization (OSPO)
    * Generates hard preference pairs and filters them
    * Uses object-centric preference optimization with conditional preference loss
    * Focuses on improving object-level text-image alignment
</details>
</details>

---


<details>
<summary><b> DIMCIM: A Quantitative Evaluation Framework for Default-mode Diversity and Generalization in Text-to-Image Generative Models</b></summary>

* **Authors:** Revant Teotia, Candace Ross, Karen Ullrich, Sumit Chopra, Adriana Romero-Soriano, Melissa Hall, Matthew J. Muckley
* **arXiv ID:** 2506.05108
* **One-liner:** Introduced DIM-CIM, a reference-free framework for measuring diversity and generalization in text-to-image models.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2506.05108) | [[PDF]](https://arxiv.org/pdf/2506.05108)

> **Core Innovation**
> DIM-CIM evaluates default-mode diversity and generalization capacity using the COCO-DIMCIM benchmark, revealing trade-offs and failure cases in model scaling.

<details>
    <summary>Abstract</summary>
    Recent advances in text-to-image (T2I) models have achieved impressive quality and consistency. However, this has come at the cost of representation diversity. While automatic evaluation methods exist for benchmarking model diversity, they either require reference image datasets or lack specificity about the kind of diversity measured, limiting their adaptability and interpretability. To address this gap, we introduce the Does-it/Can-it framework, DIM-CIM, a reference-free measurement of default-mode diversity (&#34;Does&#34; the model generate images with expected attributes?) and generalization capacity (&#34;Can&#34; the model generate diverse attributes for a particular concept?). We construct the COCO-DIMCIM benchmark, which is seeded with COCO concepts and captions and augmented by a large language model. With COCO-DIMCIM, we find that widely-used models improve in generalization at the cost of default-mode diversity when scaling from 1.5B to 8.1B parameters. DIMCIM also identifies fine-grained failure cases, such as attributes that are generated with generic prompts but are rarely generated when explicitly requested. Finally, we use DIMCIM to evaluate the training data of a T2I model and observe a correlation of 0.85 between diversity in training images and default-mode diversity. Our work provides a flexible and interpretable framework for assessing T2I model diversity and generalization, enabling a more comprehensive understanding of model performance.
</details>

<details>
    <summary>Key points</summary>
    * Proposes the Does-it/Can-it framework (DIM-CIM) for diversity measurement
    * Constructs COCO-DIMCIM benchmark with COCO concepts and LLM augmentation
    * Measures default-mode diversity and generalization capacity
    * Identifies fine-grained failure cases and correlations with training data
</details>
</details>

---


<details>
<summary><b> FocusDiff: Advancing Fine-Grained Text-Image Alignment for Autoregressive Visual Generation through RL</b></summary>

* **Authors:** Kaihang Pan, Wendong Bu, Yuruo Wu, Yang Wu, Kai Shen, Yunfei Li, Hang Zhao, Juncheng Li, Siliang Tang, Yueting Zhuang
* **arXiv ID:** 2506.05501
* **One-liner:** Proposed FocusDiff, a method to enhance fine-grained text-image alignment by focusing on subtle semantic differences.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2506.05501) | [[PDF]](https://arxiv.org/pdf/2506.05501)

> **Core Innovation**
> FocusDiff uses a novel reinforcement learning algorithm and a dataset of paired texts to achieve SOTA performance on benchmarks like PairComp.

<details>
    <summary>Abstract</summary>
    Recent studies extend the autoregression paradigm to text-to-image generation, achieving performance comparable to diffusion models. However, our new PairComp benchmark -- featuring test cases of paired prompts with similar syntax but different fine-grained semantics -- reveals that existing models struggle with fine-grained text-image alignment thus failing to realize precise control over visual tokens. To address this, we propose FocusDiff, which enhances fine-grained text-image semantic alignment by focusing on subtle differences between similar text-image pairs. We construct a new dataset of paired texts and images with similar overall expressions but distinct local semantics, further introducing a novel reinforcement learning algorithm to emphasize such fine-grained semantic differences for desired image generation. Our approach achieves state-of-the-art performance on existing text-to-image benchmarks and significantly outperforms prior methods on PairComp.
</details>

<details>
    <summary>Key points</summary>
    * Enhances fine-grained text-image semantic alignment
    * Constructs a dataset of paired texts with distinct local semantics
    * Introduces a reinforcement learning algorithm to emphasize semantic differences
    * Achieves state-of-the-art performance on text-to-image benchmarks
</details>
</details>

---


<details>
<summary><b> A Comprehensive Study of Decoder-Only LLMs for Text-to-Image Generation</b></summary>

* **Authors:** Andrew Z. Wang, Songwei Ge, Tero Karras, Ming-Yu Liu, Yogesh Balaji
* **arXiv ID:** 2506.08210
* **One-liner:** Investigated using modern LLMs as text encoders for text-to-image diffusion models, improving alignment with complex prompts.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2506.08210) | [[PDF]](https://arxiv.org/pdf/2506.08210)

> **Core Innovation**
> By exploring embeddings from various layers and using layer-normalized averaging, the study shows that LLMs outperform T5 in text-to-image generation.

<details>
    <summary>Abstract</summary>
    Both text-to-image generation and large language models (LLMs) have made significant advancements. However, many text-to-image models still employ the somewhat outdated T5 and CLIP as their text encoders. In this work, we investigate the effectiveness of using modern decoder-only LLMs as text encoders for text-to-image diffusion models. We build a standardized training and evaluation pipeline that allows us to isolate and evaluate the effect of different text embeddings. We train a total of 27 text-to-image models with 12 different text encoders to analyze the critical aspects of LLMs that could impact text-to-image generation, including the approaches to extract embeddings, different LLMs variants, and model sizes. Our experiments reveal that the de facto way of using last-layer embeddings as conditioning leads to inferior performance. Instead, we explore embeddings from various layers and find that using layer-normalized averaging across all layers significantly improves alignment with complex prompts. Most LLMs with this conditioning outperform the baseline T5 model, showing enhanced performance in advanced visio-linguistic reasoning skills.
</details>

<details>
    <summary>Key points</summary>
    * Evaluates modern decoder-only LLMs as text encoders for diffusion models
    * Trains 27 models with 12 text encoders to analyze embedding effects
    * Finds that layer-normalized averaging across layers improves alignment
    * Shows LLMs enhance performance in visio-linguistic reasoning
</details>
</details>

---


<details>
<summary><b> A High-Quality Dataset and Reliable Evaluation for Interleaved Image-Text Generation</b></summary>

* **Authors:** Yukang Feng, Jianwen Sun, Chuanhao Li, Zizhen Li, Jiaxin Ai, Fanrui Zhang, Yifan Chang, Sizhuo Zhou, Shenglin Zhang, Yu Dai, Kaipeng Zhang
* **arXiv ID:** 2506.09427
* **One-liner:** Introduced InterSyn, a large-scale multimodal dataset, and SynJudge, an automatic evaluation model for interleaved image-text outputs.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2506.09427) | [[PDF]](https://arxiv.org/pdf/2506.09427)

> **Core Innovation**
> InterSyn is constructed using the SEIR method, providing rich dialogues for training LMMs, while SynJudge assesses multimodal outputs across multiple dimensions.

<details>
    <summary>Abstract</summary>
    Recent advancements in Large Multimodal Models (LMMs) have significantly improved multimodal understanding and generation. However, these models still struggle to generate tightly interleaved image-text outputs, primarily due to the limited scale, quality and instructional richness of current training datasets. To address this, we introduce InterSyn, a large-scale multimodal dataset constructed using our Self-Evaluation with Iterative Refinement (SEIR) method. InterSyn features multi-turn, instruction-driven dialogues with tightly interleaved imagetext responses, providing rich object diversity and rigorous automated quality refinement, making it well-suited for training next-generation instruction-following LMMs. Furthermore, to address the lack of reliable evaluation tools capable of assessing interleaved multimodal outputs, we introduce SynJudge, an automatic evaluation model designed to quantitatively assess multimodal outputs along four dimensions: text content, image content, image quality, and image-text synergy.
<br>Experimental studies show that the SEIR method leads to substantially higher dataset quality compared to an otherwise identical process without refinement.
<br>Moreover, LMMs trained on InterSyn achieve uniform performance gains across all evaluation metrics, confirming InterSyn&#39;s utility for advancing multimodal systems.
</details>

<details>
    <summary>Key points</summary>
    * Constructs InterSyn dataset with Self-Evaluation with Iterative Refinement (SEIR) method
    * Features multi-turn, instruction-driven dialogues with interleaved image-text responses
    * Introduces SynJudge for automatic evaluation of multimodal outputs
    * Improves LMM performance across evaluation metrics
</details>
</details>

---


<details>
<summary><b> ELBO-T2IAlign: A Generic ELBO-Based Method for Calibrating Pixel-level Text-Image Alignment in Diffusion Models</b></summary>

* **Authors:** Qin Zhou, Zhiyang Zhang, Jinglong Wang, Xiaobin Li, Jing Zhang, Qian Yu, Lu Sheng, Dong Xu
* **arXiv ID:** 2506.09740
* **One-liner:** Proposed ELBO-T2IAlign, a training-free method to calibrate pixel-text alignment in diffusion models using zero-shot referring image segmentation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2506.09740) | [[PDF]](https://arxiv.org/pdf/2506.09740)

> **Core Innovation**
> ELBO-T2IAlign addresses misalignment issues by leveraging the evidence lower bound, working across various diffusion architectures without identifying specific causes.

<details>
    <summary>Abstract</summary>
    Diffusion models excel at image generation. Recent studies have shown that these models not only generate high-quality images but also encode text-image alignment information through attention maps or loss functions. This information is valuable for various downstream tasks, including segmentation, text-guided image editing, and compositional image generation. However, current methods heavily rely on the assumption of perfect text-image alignment in diffusion models, which is not the case. In this paper, we propose using zero-shot referring image segmentation as a proxy task to evaluate the pixel-level image and class-level text alignment of popular diffusion models. We conduct an in-depth analysis of pixel-text misalignment in diffusion models from the perspective of training data bias. We find that misalignment occurs in images with small sized, occluded, or rare object classes. Therefore, we propose ELBO-T2IAlign, a simple yet effective method to calibrate pixel-text alignment in diffusion models based on the evidence lower bound (ELBO) of likelihood. Our method is training-free and generic, eliminating the need to identify the specific cause of misalignment and works well across various diffusion model architectures. Extensive experiments on commonly used benchmark datasets on image segmentation and generation have verified the effectiveness of our proposed calibration approach.
</details>

<details>
    <summary>Key points</summary>
    * Uses zero-shot referring image segmentation as a proxy task for alignment evaluation
    * Analyzes pixel-text misalignment from training data bias perspective
    * Proposes ELBO-T2IAlign based on evidence lower bound for calibration
    * Training-free and generic method applicable to various diffusion models
</details>
</details>

---


<details>
<summary><b> Fair Generation without Unfair Distortions: Debiasing Text-to-Image Generation with Entanglement-Free Attention</b></summary>

* **Authors:** Jeonghoon Park, Juyoung Lee, Chaeyeon Chung, Jaeseong Lee, Jaegul Choo, Jindong Gu
* **arXiv ID:** 2506.13298
* **One-liner:** Introduced Entanglement-Free Attention (EFA) to mitigate societal biases in text-to-image models while preserving non-target attributes.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2506.13298) | [[PDF]](https://arxiv.org/pdf/2506.13298)

> **Core Innovation**
> EFA adjusts cross-attention layers to incorporate target attributes with equal probability, outperforming existing methods in bias mitigation without distribution shifts.

<details>
    <summary>Abstract</summary>
    Recent advancements in diffusion-based text-to-image (T2I) models have enabled the generation of high-quality and photorealistic images from text. However, they often exhibit societal biases related to gender, race, and socioeconomic status, thereby potentially reinforcing harmful stereotypes and shaping public perception in unintended ways. While existing bias mitigation methods demonstrate effectiveness, they often encounter attribute entanglement, where adjustments to attributes relevant to the bias (i.e., target attributes) unintentionally alter attributes unassociated with the bias (i.e., non-target attributes), causing undesirable distribution shifts. To address this challenge, we introduce Entanglement-Free Attention (EFA), a method that accurately incorporates target attributes (e.g., White, Black, and Asian) while preserving non-target attributes (e.g., background) during bias mitigation. At inference time, EFA randomly samples a target attribute with equal probability and adjusts the cross-attention in selected layers to incorporate the sampled attribute, achieving a fair distribution of target attributes. Extensive experiments demonstrate that EFA outperforms existing methods in mitigating bias while preserving non-target attributes, thereby maintaining the original model&#39;s output distribution and generative capacity.
</details>

<details>
    <summary>Key points</summary>
    * Proposes Entanglement-Free Attention (EFA) for bias mitigation
    * Randomly samples target attributes and adjusts cross-attention in selected layers
    * Preserves non-target attributes during bias adjustment
    * Achieves fair distribution of target attributes and maintains generative capacity
</details>
</details>

---


<details>
<summary><b> Discrete JEPA: Learning Discrete Token Representations without Reconstruction</b></summary>

* **Authors:** Junyeob Baek, Hosung Lee, Christopher Hoang, Mengye Ren, Sungjin Ahn
* **arXiv ID:** 2506.14373
* **One-liner:** Proposed Discrete-JEPA, a method for robust tokenization to enhance symbolic reasoning in AI systems.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2506.14373) | [[PDF]](https://arxiv.org/pdf/2506.14373)

> **Core Innovation**
> Discrete-JEPA extends latent predictive coding with semantic tokenization and complementary objectives, outperforming baselines in visual symbolic prediction tasks.

<details>
    <summary>Abstract</summary>
    The cornerstone of cognitive intelligence lies in extracting hidden patterns from observations and leveraging these principles to systematically predict future outcomes. However, current image tokenization methods demonstrate significant limitations in tasks requiring symbolic abstraction and logical reasoning capabilities essential for systematic inference. To address this challenge, we propose Discrete-JEPA, extending the latent predictive coding framework with semantic tokenization and novel complementary objectives to create robust tokenization for symbolic reasoning tasks. Discrete-JEPA dramatically outperforms baselines on visual symbolic prediction tasks, while striking visual evidence reveals the spontaneous emergence of deliberate systematic patterns within the learned semantic token space. Though an initial model, our approach promises a significant impact for advancing Symbolic world modeling and planning capabilities in artificial intelligence systems.
</details>

<details>
    <summary>Key points</summary>
    * Extends latent predictive coding framework with semantic tokenization
    * Introduces novel complementary objectives for robust tokenization
    * Outperforms baselines on visual symbolic prediction tasks
    * Shows emergence of systematic patterns in semantic token space
</details>
</details>

---


<details>
<summary><b> Cost-Aware Routing for Efficient Text-To-Image Generation</b></summary>

* **Authors:** Qinchan Li, Kenneth Chen, Changyue Su, Wittawat Jitkrittum, Qi Sun, Patsorn Sangkloy
* **arXiv ID:** 2506.14753
* **One-liner:** Developed a framework to optimize the trade-off between image quality and computational cost in text-to-image generation by routing prompts to appropriate models.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2506.14753) | [[PDF]](https://arxiv.org/pdf/2506.14753)

> **Core Innovation**
> Proposed a framework that automatically routes prompts to different text-to-image models or denoising steps based on complexity, achieving higher average quality than any single model.

<details>
    <summary>Abstract</summary>
    Diffusion models are well known for their ability to generate a high-fidelity image for an input prompt through an iterative denoising process. Unfortunately, the high fidelity also comes at a high computational cost due the inherently sequential generative process. In this work, we seek to optimally balance quality and computational cost, and propose a framework to allow the amount of computation to vary for each prompt, depending on its complexity. Each prompt is automatically routed to the most appropriate text-to-image generation function, which may correspond to a distinct number of denoising steps of a diffusion model, or a disparate, independent text-to-image model. Unlike uniform cost reduction techniques (e.g., distillation, model quantization), our approach achieves the optimal trade-off by learning to reserve expensive choices (e.g., 100+ denoising steps) only for a few complex prompts, and employ more economical choices (e.g., small distilled model) for less sophisticated prompts. We empirically demonstrate on COCO and DiffusionDB that by learning to route to nine already-trained text-to-image models, our approach is able to deliver an average quality that is higher than that achievable by any of these models alone.
</details>

<details>
    <summary>Key points</summary>
    * Automatic routing of prompts to the most appropriate generation function
    * Learning to use expensive models for complex prompts and economical ones for simpler prompts
    * Empirical validation on COCO and DiffusionDB datasets
</details>
</details>

---


<details>
<summary><b> NSFW-Classifier Guided Prompt Sanitization for Safe Text-to-Image Generation</b></summary>

* **Authors:** Yu Xie, Chengjie Zeng, Lingyun Zhang, Yanwei Fu
* **arXiv ID:** 2506.18325
* **One-liner:** Introduced PromptSan to detoxify harmful prompts in text-to-image models without altering model architecture.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2506.18325) | [[PDF]](https://arxiv.org/pdf/2506.18325)

> **Core Innovation**
> Proposed NSFW-Classifier Guided Prompt Sanitization with two variants to reduce harmful content generation while maintaining usability.

<details>
    <summary>Abstract</summary>
    The rapid advancement of text-to-image (T2I) models, such as Stable Diffusion, has enhanced their capability to synthesize images from textual prompts. However, this progress also raises significant risks of misuse, including the generation of harmful content (e.g., pornography, violence, discrimination), which contradicts the ethical goals of T2I technology and hinders its sustainable development. Inspired by &#34;jailbreak&#34; attacks in large language models, which bypass restrictions through subtle prompt modifications, this paper proposes NSFW-Classifier Guided Prompt Sanitization (PromptSan), a novel approach to detoxify harmful prompts without altering model architecture or degrading generation capability. PromptSan includes two variants: PromptSan-Modify, which iteratively identifies and replaces harmful tokens in input prompts using text NSFW classifiers during inference, and PromptSan-Suffix, which trains an optimized suffix token sequence to neutralize harmful intent while passing both text and image NSFW classifier checks. Extensive experiments demonstrate that PromptSan achieves state-of-the-art performance in reducing harmful content generation across multiple metrics, effectively balancing safety and usability.
</details>

<details>
    <summary>Key points</summary>
    * PromptSan-Modify: Iterative token replacement using text NSFW classifiers
    * PromptSan-Suffix: Training optimized suffix tokens to neutralize harmful intent
    * Extensive experiments showing state-of-the-art performance in safety metrics
</details>
</details>

---


<details>
<summary><b> Ovis-U1 Technical Report</b></summary>

* **Authors:** Guo-Hua Wang, Shanshan Zhao, Xinjie Zhang, Liangfu Cao, Pengxin Zhan, Lunhao Duan, Shiyin Lu, Minghao Fu, Xiaohao Chen, Jianshan Zhao, Yang Li, Qing-Guo Chen
* **arXiv ID:** 2506.23044
* **One-liner:** Introduced Ovis-U1, a unified model integrating multimodal understanding, text-to-image generation, and image editing.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2506.23044) | [[PDF]](https://arxiv.org/pdf/2506.23044)

> **Core Innovation**
> Developed a 3-billion-parameter model with unified training, achieving high scores on multiple benchmarks for understanding and generation tasks.

<details>
    <summary>Abstract</summary>
    In this report, we introduce Ovis-U1, a 3-billion-parameter unified model that integrates multimodal understanding, text-to-image generation, and image editing capabilities. Building on the foundation of the Ovis series, Ovis-U1 incorporates a diffusion-based visual decoder paired with a bidirectional token refiner, enabling image generation tasks comparable to leading models like GPT-4o. Unlike some previous models that use a frozen MLLM for generation tasks, Ovis-U1 utilizes a new unified training approach starting from a language model. Compared to training solely on understanding or generation tasks, unified training yields better performance, demonstrating the enhancement achieved by integrating these two tasks. Ovis-U1 achieves a score of 69.6 on the OpenCompass Multi-modal Academic Benchmark, surpassing recent state-of-the-art models such as Ristretto-3B and SAIL-VL-1.5-2B. In text-to-image generation, it excels with scores of 83.72 and 0.89 on the DPG-Bench and GenEval benchmarks, respectively. For image editing, it achieves 4.00 and 6.42 on the ImgEdit-Bench and GEdit-Bench-EN, respectively. As the initial version of the Ovis unified model series, Ovis-U1 pushes the boundaries of multimodal understanding, generation, and editing.
</details>

<details>
    <summary>Key points</summary>
    * Unified training approach starting from a language model
    * Diffusion-based visual decoder with bidirectional token refiner
    * Superior performance on OpenCompass, DPG-Bench, GenEval, ImgEdit-Bench, and GEdit-Bench-EN
</details>
</details>

---


<details>
<summary><b> RichControl: Structure- and Appearance-Rich Training-Free Spatial Control for Text-to-Image Generation</b></summary>

* **Authors:** Liheng Zhang, Lexi Pang, Hang Ye, Xiaoxuan Ma, Yizhou Wang
* **arXiv ID:** 2507.02792
* **One-liner:** Proposed a training-free framework for feature injection in text-to-image models to improve structure guidance and visual quality.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2507.02792) | [[PDF]](https://arxiv.org/pdf/2507.02792)

> **Core Innovation**
> Decoupled the sampling schedule of condition features from denoising, introducing a simple schedule and restart refinement for better alignment.

<details>
    <summary>Abstract</summary>
    Text-to-image (T2I) diffusion models have shown remarkable success in generating high-quality images from text prompts. Recent efforts extend these models to incorporate conditional images (e.g., canny edge) for fine-grained spatial control. Among them, feature injection methods have emerged as a training-free alternative to traditional fine-tuning-based approaches. However, they often suffer from structural misalignment, condition leakage, and visual artifacts, especially when the condition image diverges significantly from natural RGB distributions. Through an empirical analysis of existing methods, we identify a key limitation: the sampling schedule of condition features, previously unexplored, fails to account for the evolving interplay between structure preservation and domain alignment throughout diffusion steps. Inspired by this observation, we propose a flexible training-free framework that decouples the sampling schedule of condition features from the denoising process, and systematically investigate the spectrum of feature injection schedules for a higher-quality structure guidance in the feature space. Specifically, we find that condition features sampled from a single timestep are sufficient, yielding a simple yet efficient schedule that balances structure alignment and appearance quality. We further enhance the sampling process by introducing a restart refinement schedule, and improve the visual quality with an appearance-rich prompting strategy. Together, these designs enable training-free generation that is both structure-rich and appearance-rich. Extensive experiments show that our approach achieves state-of-the-art results across diverse zero-shot conditioning scenarios.
</details>

<details>
    <summary>Key points</summary>
    * Decoupling condition feature sampling from denoising process
    * Single timestep sampling for condition features
    * Restart refinement schedule and appearance-rich prompting strategy
</details>
</details>

---


<details>
<summary><b> Subject-Consistent and Pose-Diverse Text-to-Image Generation</b></summary>

* **Authors:** Zhanxin Gao, Beier Zhu, Liang Yao, Jian Yang, Ying Tai
* **arXiv ID:** 2507.08396
* **One-liner:** Developed CoDi for subject-consistent generation with diverse poses and layouts in text-to-image models.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2507.08396) | [[PDF]](https://arxiv.org/pdf/2507.08396)

> **Core Innovation**
> Introduced a two-stage strategy using identity transport and refinement to maintain subject consistency while enabling pose diversity.

<details>
    <summary>Abstract</summary>
    Subject-consistent generation (SCG)-aiming to maintain a consistent subject identity across diverse scenes-remains a challenge for text-to-image (T2I) models. Existing training-free SCG methods often achieve consistency at the cost of layout and pose diversity, hindering expressive visual storytelling. To address the limitation, we propose subject-Consistent and pose-Diverse T2I framework, dubbed as CoDi, that enables consistent subject generation with diverse pose and layout. Motivated by the progressive nature of diffusion, where coarse structures emerge early and fine details are refined later, CoDi adopts a two-stage strategy: Identity Transport (IT) and Identity Refinement (IR). IT operates in the early denoising steps, using optimal transport to transfer identity features to each target image in a pose-aware manner. This promotes subject consistency while preserving pose diversity. IR is applied in the later denoising steps, selecting the most salient identity features to further refine subject details. Extensive qualitative and quantitative results on subject consistency, pose diversity, and prompt fidelity demonstrate that CoDi achieves both better visual perception and stronger performance across all metrics. The code is provided in <a href="https://github.com/NJU-PCALab/CoDi" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Identity Transport (IT) in early denoising steps using optimal transport
    * Identity Refinement (IR) in later steps for detail refinement
    * Evaluation showing improvements in consistency, diversity, and fidelity
</details>
</details>

---


<details>
<summary><b> Visual Semantic Description Generation with MLLMs for Image-Text Matching</b></summary>

* **Authors:** Junyu Chen, Yihua Gao, Mingyong Li
* **arXiv ID:** 2507.08590
* **One-liner:** Proposed a framework for image-text matching using multimodal large language models to bridge the modality gap.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2507.08590) | [[PDF]](https://arxiv.org/pdf/2507.08590)

> **Core Innovation**
> Leveraged MLLMs to generate Visual Semantic Descriptions for instance-level and prototype-level alignment in ITM tasks.

<details>
    <summary>Abstract</summary>
    Image-text matching (ITM) aims to address the fundamental challenge of aligning visual and textual modalities, which inherently differ in their representations, continuous, high-dimensional image features vs. discrete, structured text. We propose a novel framework that bridges the modality gap by leveraging multimodal large language models (MLLMs) as visual semantic parsers. By generating rich Visual Semantic Descriptions (VSD), MLLMs provide semantic anchor that facilitate cross-modal alignment. Our approach combines: (1) Instance-level alignment by fusing visual features with VSD to enhance the linguistic expressiveness of image representations, and (2) Prototype-level alignment through VSD clustering to ensure category-level consistency. These modules can be seamlessly integrated into existing ITM models. Extensive experiments on Flickr30K and MSCOCO demonstrate substantial performance improvements. The approach also exhibits remarkable zero-shot generalization to cross-domain tasks, including news and remote sensing ITM. The code and model checkpoints are available at <a href="https://github.com/Image-Text-Matching/VSD" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Instance-level alignment by fusing visual features with VSD
    * Prototype-level alignment through VSD clustering
    * Extensive experiments on Flickr30K and MSCOCO with zero-shot generalization
</details>
</details>

---


<details>
<summary><b> RaDL: Relation-aware Disentangled Learning for Multi-Instance Text-to-Image Generation</b></summary>

* **Authors:** Geon Park, Seon Bin Kim, Gunho Jung, Seong-Whan Lee
* **arXiv ID:** 2507.11947
* **One-liner:** Introduced RaDL for multi-instance image generation with improved relationship and attribute handling.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2507.11947) | [[PDF]](https://arxiv.org/pdf/2507.11947)

> **Core Innovation**
> Proposed relation-aware disentangled learning to enhance instance-specific attributes and generate relation-aware features.

<details>
    <summary>Abstract</summary>
    With recent advancements in text-to-image (T2I) models, effectively generating multiple instances within a single image prompt has become a crucial challenge. Existing methods, while successful in generating positions of individual instances, often struggle to account for relationship discrepancy and multiple attributes leakage. To address these limitations, this paper proposes the relation-aware disentangled learning (RaDL) framework. RaDL enhances instance-specific attributes through learnable parameters and generates relation-aware image features via Relation Attention, utilizing action verbs extracted from the global prompt. Through extensive evaluations on benchmarks such as COCO-Position, COCO-MIG, and DrawBench, we demonstrate that RaDL outperforms existing methods, showing significant improvements in positional accuracy, multiple attributes consideration, and the relationships between instances. Our results present RaDL as the solution for generating images that consider both the relationships and multiple attributes of each instance within the multi-instance image.
</details>

<details>
    <summary>Key points</summary>
    * Learnable parameters for instance-specific attributes
    * Relation Attention using action verbs from prompts
    * Evaluation on COCO-Position, COCO-MIG, and DrawBench benchmarks
</details>
</details>

---


<details>
<summary><b> ID-EA: Identity-driven Text Enhancement and Adaptation with Textual Inversion for Personalized Text-to-Image Generation</b></summary>

* **Authors:** Hyun-Jun Jin, Young-Eun Kim, Seong-Whan Lee
* **arXiv ID:** 2507.11990
* **One-liner:** Developed ID-EA to improve identity preservation in personalized portrait generation using text-to-image models.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2507.11990) | [[PDF]](https://arxiv.org/pdf/2507.11990)

> **Core Innovation**
> Introduced ID-Enhancer and ID-Adapter to align text embeddings with visual identity embeddings for better consistency.

<details>
    <summary>Abstract</summary>
    Recently, personalized portrait generation with a text-to-image diffusion model has significantly advanced with Textual Inversion, emerging as a promising approach for creating high-fidelity personalized images. Despite its potential, current Textual Inversion methods struggle to maintain consistent facial identity due to semantic misalignments between textual and visual embedding spaces regarding identity. We introduce ID-EA, a novel framework that guides text embeddings to align with visual identity embeddings, thereby improving identity preservation in a personalized generation. ID-EA comprises two key components: the ID-driven Enhancer (ID-Enhancer) and the ID-conditioned Adapter (ID-Adapter). First, the ID-Enhancer integrates identity embeddings with a textual ID anchor, refining visual identity embeddings derived from a face recognition model using representative text embeddings. Then, the ID-Adapter leverages the identity-enhanced embedding to adapt the text condition, ensuring identity preservation by adjusting the cross-attention module in the pre-trained UNet model. This process encourages the text features to find the most related visual clues across the foreground snippets. Extensive quantitative and qualitative evaluations demonstrate that ID-EA substantially outperforms state-of-the-art methods in identity preservation metrics while achieving remarkable computational efficiency, generating personalized portraits approximately 15 times faster than existing approaches.
</details>

<details>
    <summary>Key points</summary>
    * ID-Enhancer refining visual identity embeddings with textual ID anchor
    * ID-Adapter adjusting cross-attention for identity preservation
    * Quantitative and qualitative evaluations showing superior performance and efficiency
</details>
</details>

---


<details>
<summary><b> Local Representative Token Guided Merging for Text-to-Image Generation</b></summary>

* **Authors:** Min-Jeong Lee, Hee-Dong Kim, Seong-Whan Lee
* **arXiv ID:** 2507.12771
* **One-liner:** Proposed ReToM, a token merging strategy to reduce computational cost in stable diffusion models while maintaining quality.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2507.12771) | [[PDF]](https://arxiv.org/pdf/2507.12771)

> **Core Innovation**
> Introduced local representative token guided merging to preserve salient features and improve efficiency in attention operations.

<details>
    <summary>Abstract</summary>
    Stable diffusion is an outstanding image generation model for text-to-image, but its time-consuming generation process remains a challenge due to the quadratic complexity of attention operations. Recent token merging methods improve efficiency by reducing the number of tokens during attention operations, but often overlook the characteristics of attention-based image generation models, limiting their effectiveness. In this paper, we propose local representative token guided merging (ReToM), a novel token merging strategy applicable to any attention mechanism in image generation. To merge tokens based on various contextual information, ReToM defines local boundaries as windows within attention inputs and adjusts window sizes. Furthermore, we introduce a representative token, which represents the most representative token per window by computing similarity at a specific timestep and selecting the token with the highest average similarity. This approach preserves the most salient local features while minimizing computational overhead. Experimental results show that ReToM achieves a 6.2% improvement in FID and higher CLIP scores compared to the baseline, while maintaining comparable inference time. We empirically demonstrate that ReToM is effective in balancing visual quality and computational efficiency.
</details>

<details>
    <summary>Key points</summary>
    * Defining local boundaries as windows for token merging
    * Selecting representative tokens based on similarity
    * Experimental results showing improved FID and CLIP scores with comparable inference time
</details>
</details>

---


<details>
<summary><b> LSSGen: Leveraging Latent Space Scaling in Flow and Diffusion for Efficient Text to Image Generation</b></summary>

* **Authors:** Jyun-Ze Tang, Chih-Fan Hsu, Jeng-Lin Li, Ming-Ching Chang, Wei-Chao Chen
* **arXiv ID:** 2507.16154
* **One-liner:** Introduced LSSGen for efficient and high-quality multi-resolution image generation in latent space.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2507.16154) | [[PDF]](https://arxiv.org/pdf/2507.16154)

> **Core Innovation**
> Proposed latent space scaling with a lightweight upsampler to avoid artifacts from traditional pixel scaling methods.

<details>
    <summary>Abstract</summary>
    Flow matching and diffusion models have shown impressive results in text-to-image generation, producing photorealistic images through an iterative denoising process. A common strategy to speed up synthesis is to perform early denoising at lower resolutions. However, traditional methods that downscale and upscale in pixel space often introduce artifacts and distortions. These issues arise when the upscaled images are re-encoded into the latent space, leading to degraded final image quality. To address this, we propose {\bf Latent Space Scaling Generation (LSSGen)}, a framework that performs resolution scaling directly in the latent space using a lightweight latent upsampler. Without altering the Transformer or U-Net architecture, LSSGen improves both efficiency and visual quality while supporting flexible multi-resolution generation. Our comprehensive evaluation covering text-image alignment and perceptual quality shows that LSSGen significantly outperforms conventional scaling approaches. When generating $1024^2$ images at similar speeds, it achieves up to 246\% TOPIQ score improvement.
</details>

<details>
    <summary>Key points</summary>
    * Performing resolution scaling directly in latent space
    * Using a lightweight latent upsampler without altering core architectures
    * Evaluation showing significant improvements in TOPIQ score and visual quality
</details>
</details>

---


<details>
<summary><b> Lumina-mGPT 2.0: Stand-Alone AutoRegressive Image Modeling</b></summary>

* **Authors:** Yi Xin, Juncheng Yan, Qi Qin, Zhen Li, Dongyang Liu, Shicheng Li, Victor Shea-Jay Huang, Yupeng Zhou, Renrui Zhang, Le Zhuo, Tiancheng Han, Xiaoqing Sun, Siqi Luo, Mengmeng Wang, Bin Fu, Yuewen Cao, Hongsheng Li, Guangtao Zhai, Xiaohong Liu, Yu Qiao, Peng Gao
* **arXiv ID:** 2507.17801
* **One-liner:** Introduced Lumina-mGPT 2.0, a high-quality autoregressive image generation model trained from scratch, matching state-of-the-art diffusion models.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2507.17801) | [[PDF]](https://arxiv.org/pdf/2507.17801)

> **Core Innovation**
> Revitalized autoregressive modeling for unified multimodal generation with a single framework, achieving flexibility and compositionality.

<details>
    <summary>Abstract</summary>
    We present Lumina-mGPT 2.0, a stand-alone, decoder-only autoregressive model that revisits and revitalizes the autoregressive paradigm for high-quality image generation and beyond. Unlike existing approaches that rely on pretrained components or hybrid architectures, Lumina-mGPT 2.0 is trained entirely from scratch, enabling unrestricted architectural design and licensing freedom. It achieves generation quality on par with state-of-the-art diffusion models such as DALL-E 3 and SANA, while preserving the inherent flexibility and compositionality of autoregressive modeling. Our unified tokenization scheme allows the model to seamlessly handle a wide spectrum of tasks-including subject-driven generation, image editing, controllable synthesis, and dense prediction-within a single generative framework. To further boost usability, we incorporate efficient decoding strategies like inference-time scaling and speculative Jacobi sampling to improve quality and speed, respectively. Extensive evaluations on standard text-to-image benchmarks (e.g., GenEval, DPG) demonstrate that Lumina-mGPT 2.0 not only matches but in some cases surpasses diffusion-based models. Moreover, we confirm its multi-task capabilities on the Graph200K benchmark, with the native Lumina-mGPT 2.0 performing exceptionally well. These results position Lumina-mGPT 2.0 as a strong, flexible foundation model for unified multimodal generation. We have released our training details, code, and models at <a href="https://github.com/Alpha-VLLM/Lumina-mGPT-2.0" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Trained entirely from scratch for unrestricted architectural design and licensing freedom
    * Unified tokenization scheme for handling multiple tasks like subject-driven generation and image editing
    * Incorporated efficient decoding strategies like inference-time scaling and speculative Jacobi sampling
</details>
</details>

---


<details>
<summary><b> T2I-Copilot: A Training-Free Multi-Agent Text-to-Image System for Enhanced Prompt Interpretation and Interactive Generation</b></summary>

* **Authors:** Chieh-Yun Chen, Min Shi, Gong Zhang, Humphrey Shi
* **arXiv ID:** 2507.20536
* **One-liner:** Developed T2I-Copilot, a training-free multi-agent system that automates prompt engineering and model selection for T2I generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2507.20536) | [[PDF]](https://arxiv.org/pdf/2507.20536)

> **Core Innovation**
> Enhanced generation quality and text-image alignment through collaborative agents, reducing the need for manual prompt refinement.

<details>
    <summary>Abstract</summary>
    Text-to-Image (T2I) generative models have revolutionized content creation but remain highly sensitive to prompt phrasing, often requiring users to repeatedly refine prompts multiple times without clear feedback. While techniques such as automatic prompt engineering, controlled text embeddings, denoising, and multi-turn generation mitigate these issues, they offer limited controllability, or often necessitate additional training, restricting the generalization abilities. Thus, we introduce T2I-Copilot, a training-free multi-agent system that leverages collaboration between (Multimodal) Large Language Models to automate prompt phrasing, model selection, and iterative refinement. This approach significantly simplifies prompt engineering while enhancing generation quality and text-image alignment compared to direct generation. Specifically, T2I-Copilot consists of three agents: (1) Input Interpreter, which parses the input prompt, resolves ambiguities, and generates a standardized report; (2) Generation Engine, which selects the appropriate model from different types of T2I models and organizes visual and textual prompts to initiate generation; and (3) Quality Evaluator, which assesses aesthetic quality and text-image alignment, providing scores and feedback for potential regeneration. T2I-Copilot can operate fully autonomously while also supporting human-in-the-loop intervention for fine-grained control. On GenAI-Bench, using open-source generation models, T2I-Copilot achieves a VQA score comparable to commercial models RecraftV3 and Imagen 3, surpasses FLUX1.1-pro by 6.17% at only 16.59% of its cost, and outperforms FLUX.1-dev and SD 3.5 Large by 9.11% and 6.36%. Code will be released at: <a href="https://github.com/SHI-Labs/T2I-Copilot" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Input Interpreter parses and standardizes prompts
    * Generation Engine selects appropriate T2I models and organizes prompts
    * Quality Evaluator assesses aesthetic quality and alignment for iterative refinement
</details>
</details>

---


<details>
<summary><b> Multimodal LLMs as Customized Reward Models for Text-to-Image Generation</b></summary>

* **Authors:** Shijie Zhou, Ruiyi Zhang, Huaisheng Zhu, Branislav Kveton, Yufan Zhou, Jiuxiang Gu, Jian Chen, Changyou Chen
* **arXiv ID:** 2507.21391
* **One-liner:** Proposed LLaVA-Reward, an efficient reward model for automatic evaluation of T2I generations using MLLMs.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2507.21391) | [[PDF]](https://arxiv.org/pdf/2507.21391)

> **Core Innovation**
> Improved text-image correlation reasoning with a Skip-connection Cross Attention module, enabling human-aligned scoring.

<details>
    <summary>Abstract</summary>
    We introduce LLaVA-Reward, an efficient reward model designed to automatically evaluate text-to-image (T2I) generations across multiple perspectives, leveraging pretrained multimodal large language models (MLLMs). Existing MLLM-based approaches require instruction-following data for supervised fine-tuning and evaluate generation quality on analyzing text response, which is time-consuming and difficult to train. To address this problem, we propose LLaVA-Reward, which directly utilizes the hidden states of MLLMs given text-image pairs. To enhance the bidirectional interaction between visual and textual representations in decoder-only MLLMs, we further propose adding a Skip-connection Cross Attention (SkipCA) module. This design enhances text-image correlation reasoning by connecting early-layer visual features with later-layer hidden representations. In addition, LLaVA-Reward supports different types of preference data for efficient fine-tuning, including paired preference data and unpaired data. We train LLaVA-Reward on four evaluation perspectives: text-image alignment, fidelity/artifact, safety, and overall ranking. Empirical results demonstrate that LLaVA-Reward outperforms conventional and MLLM-based methods in generating human-aligned scores for automatic evaluations and inference-time scaling in text-to-image generations.
</details>

<details>
    <summary>Key points</summary>
    * Utilizes hidden states of MLLMs for direct evaluation without fine-tuning
    * SkipCA module enhances bidirectional interaction between visual and textual features
    * Supports fine-tuning with paired and unpaired preference data for multiple evaluation perspectives
</details>
</details>

---


<details>
<summary><b> Qwen-Image Technical Report</b></summary>

* **Authors:** Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, Deqing Li, Hang Zhang, Hao Meng, Hu Wei, Jingyuan Ni, Kai Chen, Kuan Cao, Liang Peng, Lin Qu, Minggang Wu, Peng Wang, Shuting Yu, Tingkun Wen, Wensen Feng, Xiaoxiao Xu, Yi Wang, Yichang Zhang, Yongqiang Zhu, Yujia Wu, Yuxuan Cai, Zenan Liu
* **arXiv ID:** 2508.02324
* **One-liner:** Achieved state-of-the-art performance in complex text rendering and precise image editing with Qwen-Image.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2508.02324) | [[PDF]](https://arxiv.org/pdf/2508.02324)

> **Core Innovation**
> Enhanced text rendering capabilities through progressive training and improved multi-task alignment for consistency.

<details>
    <summary>Abstract</summary>
    We present Qwen-Image, an image generation foundation model in the Qwen series that achieves significant advances in complex text rendering and precise image editing. To address the challenges of complex text rendering, we design a comprehensive data pipeline that includes large-scale data collection, filtering, annotation, synthesis, and balancing. Moreover, we adopt a progressive training strategy that starts with non-text-to-text rendering, evolves from simple to complex textual inputs, and gradually scales up to paragraph-level descriptions. This curriculum learning approach substantially enhances the model&#39;s native text rendering capabilities. As a result, Qwen-Image not only performs exceptionally well in alphabetic languages such as English, but also achieves remarkable progress on more challenging logographic languages like Chinese. To enhance image editing consistency, we introduce an improved multi-task training paradigm that incorporates not only traditional text-to-image (T2I) and text-image-to-image (TI2I) tasks but also image-to-image (I2I) reconstruction, effectively aligning the latent representations between Qwen2.5-VL and MMDiT. Furthermore, we separately feed the original image into Qwen2.5-VL and the VAE encoder to obtain semantic and reconstructive representations, respectively. This dual-encoding mechanism enables the editing module to strike a balance between preserving semantic consistency and maintaining visual fidelity. Qwen-Image achieves state-of-the-art performance, demonstrating its strong capabilities in both image generation and editing across multiple benchmarks.
</details>

<details>
    <summary>Key points</summary>
    * Comprehensive data pipeline for text rendering including synthesis and balancing
    * Progressive training strategy from non-text to paragraph-level descriptions
    * Dual-encoding mechanism for semantic and reconstructive representations in editing
</details>
</details>

---


<details>
<summary><b> Documenting Patterns of Exoticism of Marginalized Populations within Text-to-Image Generators</b></summary>

* **Authors:** Sourojit Ghosh, Sanjana Gautam, Pranav Venkit, Avijit Ghosh
* **arXiv ID:** 2508.02937
* **One-liner:** Identified and analyzed exoticism in GAI tools, highlighting biases against non-Western and marginalized communities.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2508.02937) | [[PDF]](https://arxiv.org/pdf/2508.02937)

> **Core Innovation**
> Documented patterns of exoticism in attire depiction across Global South countries, with implications for harm-aware design.

<details>
    <summary>Abstract</summary>
    A significant majority of AI fairness research studying the harmful outcomes of GAI tools have overlooked non-Western communities and contexts, necessitating a stronger coverage in this vein. We extend our previous work on exoticism (Ghosh et al., 2024) of &#39;Global South&#39; countries from across the world, as depicted by GAI tools. We analyze generated images of individuals from 13 countries -- India, Bangladesh, Papua New Guinea, Egypt, Ethiopia, Tunisia, Sudan, Libya, Venezuela, Colombia, Indonesia, Honduras, and Mexico -- performing everyday activities (such as being at home, going to work, getting groceries, etc.), as opposed to images for the same activities being performed by persons from 3 &#39;Global North&#39; countries -- USA, UK, Australia. While outputs for &#39;Global North&#39; demonstrate a difference across images and people clad in activity-appropriate attire, individuals from &#39;Global South&#39; countries are depicted in similar attire irrespective of the performed activity, indicative of a pattern of exoticism where attire or other cultural features are overamplified at the cost of accuracy. We further show qualitatively-analyzed case studies that demonstrate how exoticism is not simply performed upon &#39;Global South&#39; countries but also upon marginalized populations even in Western contexts, as we observe a similar exoticization of Indigenous populations in the &#39;Global North&#39;, and doubly upon marginalized populations within &#39;Global South&#39; countries. We document implications for harm-aware usage patterns of such tools, and steps towards designing better GAI tools through community-centered endeavors.
</details>

<details>
    <summary>Key points</summary>
    * Analyzed generated images from 13 Global South and 3 Global North countries
    * Qualitative case studies showing exoticism in marginalized populations
    * Proposed community-centered steps for better GAI tool design
</details>
</details>

---


<details>
<summary><b> Draw Your Mind: Personalized Generation via Condition-Level Modeling in Text-to-Image Diffusion Models</b></summary>

* **Authors:** Hyungjin Kim, Seokho Ahn, Young-Duk Seo
* **arXiv ID:** 2508.03481
* **One-liner:** Proposed DrUM, a method for personalized T2I generation using user profiling and transformer-based adapters.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2508.03481) | [[PDF]](https://arxiv.org/pdf/2508.03481)

> **Core Innovation**
> Enabled condition-level modeling in latent space for accurate personalization without fine-tuning.

<details>
    <summary>Abstract</summary>
    Personalized generation in T2I diffusion models aims to naturally incorporate individual user preferences into the generation process with minimal user intervention. However, existing studies primarily rely on prompt-level modeling with large-scale models, often leading to inaccurate personalization due to the limited input token capacity of T2I diffusion models. To address these limitations, we propose DrUM, a novel method that integrates user profiling with a transformer-based adapter to enable personalized generation through condition-level modeling in the latent space. DrUM demonstrates strong performance on large-scale datasets and seamlessly integrates with open-source text encoders, making it compatible with widely used foundation T2I models without requiring additional fine-tuning.
</details>

<details>
    <summary>Key points</summary>
    * Integrates user profiling with transformer-based adapter
    * Compatible with open-source text encoders and foundation T2I models
    * Demonstrates strong performance on large-scale datasets
</details>
</details>

---


<details>
<summary><b> LumiGen: An LVLM-Enhanced Iterative Framework for Fine-Grained Text-to-Image Generation</b></summary>

* **Authors:** Xiaoqi Dong, Xiangyu Zhou, Nicholas Evans, Yujia Lin
* **arXiv ID:** 2508.04732
* **One-liner:** Introduced LumiGen, an LVLM-enhanced iterative framework for fine-grained control in T2I generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2508.04732) | [[PDF]](https://arxiv.org/pdf/2508.04732)

> **Core Innovation**
> Elevated T2I performance through closed-loop feedback, improving text rendering and pose expression.

<details>
    <summary>Abstract</summary>
    Text-to-Image (T2I) generation has made significant advancements with diffusion models, yet challenges persist in handling complex instructions, ensuring fine-grained content control, and maintaining deep semantic consistency. Existing T2I models often struggle with tasks like accurate text rendering, precise pose generation, or intricate compositional coherence. Concurrently, Vision-Language Models (LVLMs) have demonstrated powerful capabilities in cross-modal understanding and instruction following. We propose LumiGen, a novel LVLM-enhanced iterative framework designed to elevate T2I model performance, particularly in areas requiring fine-grained control, through a closed-loop, LVLM-driven feedback mechanism. LumiGen comprises an Intelligent Prompt Parsing &amp; Augmentation (IPPA) module for proactive prompt enhancement and an Iterative Visual Feedback &amp; Refinement (IVFR) module, which acts as a &#34;visual critic&#34; to iteratively correct and optimize generated images. Evaluated on the challenging LongBench-T2I Benchmark, LumiGen achieves a superior average score of 3.08, outperforming state-of-the-art baselines. Notably, our framework demonstrates significant improvements in critical dimensions such as text rendering and pose expression, validating the effectiveness of LVLM integration for more controllable and higher-quality image generation.
</details>

<details>
    <summary>Key points</summary>
    * Intelligent Prompt Parsing & Augmentation module for proactive enhancement
    * Iterative Visual Feedback & Refinement module as a visual critic
    * Achieved superior scores on LongBench-T2I Benchmark
</details>
</details>

---


<details>
<summary><b> UNCAGE: Contrastive Attention Guidance for Masked Generative Transformers in Text-to-Image Generation</b></summary>

* **Authors:** Wonjun Kang, Byeongkeun Ahn, Minjae Lee, Kevin Galim, Seunghyuk Oh, Hyung Il Koo, Nam Ik Cho
* **arXiv ID:** 2508.05399
* **One-liner:** Proposed UNCAGE, a training-free method to improve compositional fidelity in Masked Generative Transformers.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2508.05399) | [[PDF]](https://arxiv.org/pdf/2508.05399)

> **Core Innovation**
> Enhanced text-image alignment by prioritizing unmasking of object tokens using attention maps.

<details>
    <summary>Abstract</summary>
    Text-to-image (T2I) generation has been actively studied using Diffusion Models and Autoregressive Models. Recently, Masked Generative Transformers have gained attention as an alternative to Autoregressive Models to overcome the inherent limitations of causal attention and autoregressive decoding through bidirectional attention and parallel decoding, enabling efficient and high-quality image generation. However, compositional T2I generation remains challenging, as even state-of-the-art Diffusion Models often fail to accurately bind attributes and achieve proper text-image alignment. While Diffusion Models have been extensively studied for this issue, Masked Generative Transformers exhibit similar limitations but have not been explored in this context. To address this, we propose Unmasking with Contrastive Attention Guidance (UNCAGE), a novel training-free method that improves compositional fidelity by leveraging attention maps to prioritize the unmasking of tokens that clearly represent individual objects. UNCAGE consistently improves performance in both quantitative and qualitative evaluations across multiple benchmarks and metrics, with negligible inference overhead. Our code is available at <a href="https://github.com/furiosa-ai/uncage" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Leverages attention maps for token unmasking
    * Consistently improves performance across benchmarks with negligible overhead
    * Addresses limitations in compositional T2I generation
</details>
</details>

---


<details>
<summary><b> CoAR: Concept Injection into Autoregressive Models for Personalized Text-to-Image Generation</b></summary>

* **Authors:** Fangtai Wu, Mushui Liu, Weijie He, Wanggui He, Hao Jiang, Zhao Wang, Yunlong Yu
* **arXiv ID:** 2508.07341
* **One-liner:** Developed CoAR, a framework for customized image generation in unified AR models with minimal parameter tuning.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2508.07341) | [[PDF]](https://arxiv.org/pdf/2508.07341)

> **Core Innovation**
> Enabled subject and style customization while keeping pre-trained parameters frozen, improving efficiency.

<details>
    <summary>Abstract</summary>
    The unified autoregressive (AR) model excels at multimodal understanding and generation, but its potential for customized image generation remains underexplored. Existing customized generation methods rely on full fine-tuning or adapters, making them costly and prone to overfitting or catastrophic forgetting. In this paper, we propose \textbf{CoAR}, a novel framework for injecting subject concepts into the unified AR models while keeping all pre-trained parameters completely frozen. CoAR learns effective, specific subject representations with only a minimal number of parameters using a Layerwise Multimodal Context Learning strategy. To address overfitting and language drift, we further introduce regularization that preserves the pre-trained distribution and anchors context tokens to improve subject fidelity and re-contextualization. Additionally, CoAR supports training-free subject customization in a user-provided style. Experiments demonstrate that CoAR achieves superior performance on both subject-driven personalization and style personalization, while delivering significant gains in computational and memory efficiency. Notably, CoAR tunes less than \textbf{0.05\%} of the parameters while achieving competitive performance compared to recent Proxy-Tuning. Code: <a href="https://github.com/KZF-kzf/CoAR" rel="external noopener nofollow" class="link-external link-https">this https URL</a>
</details>

<details>
    <summary>Key points</summary>
    * Layerwise Multimodal Context Learning for subject representations
    * Regularization to prevent overfitting and language drift
    * Tunes less than 0.05% of parameters for competitive performance
</details>
</details>

---


<details>
<summary><b> Dual Recursive Feedback on Generation and Appearance Latents for Pose-Robust Text-to-Image Diffusion</b></summary>

* **Authors:** Jiwon Kim, Pureum Kim, SeonHwa Kim, Soobin Park, Eunju Cha, Kyong Hwan Jin
* **arXiv ID:** 2508.09575
* **One-liner:** Proposed DRF, a training-free system for fine-grained control in controllable T2I diffusion models.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2508.09575) | [[PDF]](https://arxiv.org/pdf/2508.09575)

> **Core Innovation**
> Recursively refines latents to better reflect appearance and structural conditions, ensuring consistency.

<details>
    <summary>Abstract</summary>
    Recent advancements in controllable text-to-image (T2I) diffusion models, such as Ctrl-X and FreeControl, have demonstrated robust spatial and appearance control without requiring auxiliary module training. However, these models often struggle to accurately preserve spatial structures and fail to capture fine-grained conditions related to object poses and scene layouts. To address these challenges, we propose a training-free Dual Recursive Feedback (DRF) system that properly reflects control conditions in controllable T2I models. The proposed DRF consists of appearance feedback and generation feedback that recursively refines the intermediate latents to better reflect the given appearance information and the user&#39;s intent. This dual-update mechanism guides latent representations toward reliable manifolds, effectively integrating structural and appearance attributes. Our approach enables fine-grained generation even between class-invariant structure-appearance fusion, such as transferring human motion onto a tiger&#39;s form. Extensive experiments demonstrate the efficacy of our method in producing high-quality, semantically coherent, and structurally consistent image generations. Our source code is available at <a href="https://github.com/jwonkm/DRF" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Dual Recursive Feedback with appearance and generation feedback
    * Enables class-invariant structure-appearance fusion
    * Produces high-quality, semantically coherent images
</details>
</details>

---


<details>
<summary><b> High Fidelity Text to Image Generation with Contrastive Alignment and Structural Guidance</b></summary>

* **Authors:** Danyi Gao
* **arXiv ID:** 2508.10280
* **One-liner:** Proposed a high-fidelity image generation method integrating text-image contrastive constraints with structural guidance mechanisms.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2508.10280) | [[PDF]](https://arxiv.org/pdf/2508.10280)

> **Core Innovation**
> Enhanced semantic alignment and structural consistency in text-driven image generation without increasing computational complexity.

<details>
    <summary>Abstract</summary>
    This paper addresses the performance bottlenecks of existing text-driven image generation methods in terms of semantic alignment accuracy and structural consistency. A high-fidelity image generation method is proposed by integrating text-image contrastive constraints with structural guidance mechanisms. The approach introduces a contrastive learning module that builds strong cross-modal alignment constraints to improve semantic matching between text and image. At the same time, structural priors such as semantic layout maps or edge sketches are used to guide the generator in spatial-level structural modeling. This enhances the layout completeness and detail fidelity of the generated images. Within the overall framework, the model jointly optimizes contrastive loss, structural consistency loss, and semantic preservation loss. A multi-objective supervision mechanism is adopted to improve the semantic consistency and controllability of the generated content. Systematic experiments are conducted on the COCO-2014 dataset. Sensitivity analyses are performed on embedding dimensions, text length, and structural guidance strength. Quantitative metrics confirm the superior performance of the proposed method in terms of CLIP Score, FID, and SSIM. The results show that the method effectively bridges the gap between semantic alignment and structural fidelity without increasing computational complexity. It demonstrates a strong ability to generate semantically clear and structurally complete images, offering a viable technical path for joint text-image modeling and image generation.
</details>

<details>
    <summary>Key points</summary>
    * Contrastive learning module for cross-modal alignment
    * Structural priors like layout maps for spatial guidance
    * Joint optimization of multiple losses (contrastive, structural, semantic)
</details>
</details>

---


<details>
<summary><b> CEIDM: A Controlled Entity and Interaction Diffusion Model for Enhanced Text-to-Image Generation</b></summary>

* **Authors:** Mingyue Yang, Dianxi Shi, Jialu Zhou, Xinyu Wei, Leqian Li, Shaowu Yang, Chunping Qiu
* **arXiv ID:** 2508.17760
* **One-liner:** Introduced CEIDM, a diffusion-based method with dual controls for entity and interaction in T2I generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2508.17760) | [[PDF]](https://arxiv.org/pdf/2508.17760)

> **Core Innovation**
> Improved control over entities and their interactions by mining implicit relationships and clustering actions.

<details>
    <summary>Abstract</summary>
    In Text-to-Image (T2I) generation, the complexity of entities and their intricate interactions pose a significant challenge for T2I method based on diffusion model: how to effectively control entity and their interactions to produce high-quality images. To address this, we propose CEIDM, a image generation method based on diffusion model with dual controls for entity and interaction. First, we propose an entity interactive relationships mining approach based on Large Language Models (LLMs), extracting reasonable and rich implicit interactive relationships through chain of thought to guide diffusion models to generate high-quality images that are closer to realistic logic and have more reasonable interactive relationships. Furthermore, We propose an interactive action clustering and offset method to cluster and offset the interactive action features contained in each text prompts. By constructing global and local bidirectional offsets, we enhance semantic understanding and detail supplementation of original actions, making the model&#39;s understanding of the concept of interactive &#34;actions&#34; more accurate and generating images with more accurate interactive actions. Finally, we design an entity control network which generates masks with entity semantic guidance, then leveraging multi-scale convolutional network to enhance entity feature and dynamic network to fuse feature. It effectively controls entities and significantly improves image quality. Experiments show that the proposed CEIDM method is better than the most representative existing methods in both entity control and their interaction control.
</details>

<details>
    <summary>Key points</summary>
    * LLM-based entity interactive relationships mining
    * Interactive action clustering and offset method
    * Entity control network with multi-scale and dynamic feature fusion
</details>
</details>

---


<details>
<summary><b> HADIS: Hybrid Adaptive Diffusion Model Serving for Efficient Text-to-Image Generation</b></summary>

* **Authors:** Qizheng Yang, Tung-I Chen, Siyu Zhao, Ramesh K. Sitaraman, Hui Guan
* **arXiv ID:** 2509.00642
* **One-liner:** Developed HADIS, a hybrid adaptive diffusion model serving system for efficient real-time deployment.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2509.00642) | [[PDF]](https://arxiv.org/pdf/2509.00642)

> **Core Innovation**
> Optimized cascade model selection, query routing, and resource allocation to improve response quality and reduce latency.

<details>
    <summary>Abstract</summary>
    Text-to-image diffusion models have achieved remarkable visual quality but incur high computational costs, making real-time, scalable deployment challenging. Existing query-aware serving systems mitigate the cost by cascading lightweight and heavyweight models, but most rely on a fixed cascade configuration and route all prompts through an initial lightweight stage, wasting resources on complex queries. We present HADIS, a hybrid adaptive diffusion model serving system that jointly optimizes cascade model selection, query routing, and resource allocation. HADIS employs a rule-based prompt router to send clearly hard queries directly to heavyweight models, bypassing the overhead of the lightweight stage. To reduce the complexity of resource management, HADIS uses an offline profiling phase to produce a Pareto-optimal cascade configuration table. At runtime, HADIS selects the best cascade configuration and GPU allocation given latency and workload constraints. Empirical evaluations on real-world traces demonstrate that HADIS improves response quality by up to 35% while reducing latency violation rates by 2.7-45$\times$ compared to state-of-the-art model serving systems.
</details>

<details>
    <summary>Key points</summary>
    * Rule-based prompt router for direct heavyweight model routing
    * Offline profiling for Pareto-optimal cascade configurations
    * Runtime selection of configurations and GPU allocation
</details>
</details>

---


<details>
<summary><b> Data-Driven Loss Functions for Inference-Time Optimization in Text-to-Image Generation</b></summary>

* **Authors:** Sapir Esther Yiflach, Yuval Atzmon, Gal Chechik
* **arXiv ID:** 2509.02295
* **One-liner:** Introduced Learn-to-Steer, a framework for learning data-driven objectives to improve spatial reasoning in T2I models.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2509.02295) | [[PDF]](https://arxiv.org/pdf/2509.02295)

> **Core Innovation**
> Dramatically enhanced spatial accuracy by decoding relationships from cross-attention maps and using a learned loss.

<details>
    <summary>Abstract</summary>
    Text-to-image diffusion models can generate stunning visuals, yet they often fail at tasks children find trivial--like placing a dog to the right of a teddy bear rather than to the left. When combinations get more unusual--a giraffe above an airplane--these failures become even more pronounced. Existing methods attempt to fix these spatial reasoning failures through model fine-tuning or test-time optimization with handcrafted losses that are suboptimal. Rather than imposing our assumptions about spatial encoding, we propose learning these objectives directly from the model&#39;s internal representations. We introduce Learn-to-Steer, a novel framework that learns data-driven objectives for test-time optimization rather than handcrafting them. Our key insight is to train a lightweight classifier that decodes spatial relationships from the diffusion model&#39;s cross-attention maps, then deploy this classifier as a learned loss function during inference. Training such classifiers poses a surprising challenge: they can take shortcuts by detecting linguistic traces rather than learning true spatial patterns. We solve this with a dual-inversion strategy that enforces geometric understanding. Our method dramatically improves spatial accuracy: from 0.20 to 0.61 on FLUX.1-dev and from 0.07 to 0.54 on SD2.1 across standard benchmarks. Moreover, our approach generalizes to multiple relations and significantly improves accuracy.
</details>

<details>
    <summary>Key points</summary>
    * Lightweight classifier trained on cross-attention maps
    * Dual-inversion strategy to enforce geometric understanding
    * Deployment as a learned loss function during inference
</details>
</details>

---


<details>
<summary><b> PromptEnhancer: A Simple Approach to Enhance Text-to-Image Models via Chain-of-Thought Prompt Rewriting</b></summary>

* **Authors:** Linqing Wang, Ximing Xing, Yiji Cheng, Zhiyuan Zhao, Donghao Li, Tiankai Hang, Jiale Tao, Qixun Wang, Ruihuang Li, Comi Chen, Xin Li, Mingrui Wu, Xinchi Deng, Shuyang Gu, Chunyu Wang, Qinglin Lu
* **arXiv ID:** 2509.04545
* **One-liner:** Proposed PromptEnhancer, a universal prompt rewriting framework to enhance T2I model alignment without weight modifications.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2509.04545) | [[PDF]](https://arxiv.org/pdf/2509.04545)

> **Core Innovation**
> Improved image-text alignment by training a CoT rewriter with a fine-grained reward model.

<details>
    <summary>Abstract</summary>
    Recent advancements in text-to-image (T2I) diffusion models have demonstrated remarkable capabilities in generating high-fidelity images. However, these models often struggle to faithfully render complex user prompts, particularly in aspects like attribute binding, negation, and compositional relationships. This leads to a significant mismatch between user intent and the generated output. To address this challenge, we introduce PromptEnhancer, a novel and universal prompt rewriting framework that enhances any pretrained T2I model without requiring modifications to its weights. Unlike prior methods that rely on model-specific fine-tuning or implicit reward signals like image-reward scores, our framework decouples the rewriter from the generator. We achieve this by training a Chain-of-Thought (CoT) rewriter through reinforcement learning, guided by a dedicated reward model we term the AlignEvaluator. The AlignEvaluator is trained to provide explicit and fine-grained feedback based on a systematic taxonomy of 24 key points, which are derived from a comprehensive analysis of common T2I failure modes. By optimizing the CoT rewriter to maximize the reward from our AlignEvaluator, our framework learns to generate prompts that are more precisely interpreted by T2I models. Extensive experiments on the HunyuanImage 2.1 model demonstrate that PromptEnhancer significantly improves image-text alignment across a wide range of semantic and compositional challenges. Furthermore, we introduce a new, high-quality human preference benchmark to facilitate future research in this direction.
</details>

<details>
    <summary>Key points</summary>
    * Chain-of-Thought rewriter trained via reinforcement learning
    * AlignEvaluator reward model based on 24 key points
    * Decoupled rewriter and generator for universal application
</details>
</details>

---


<details>
<summary><b> EditIDv2: Editable ID Customization with Data-Lubricated ID Feature Integration for Text-to-Image Generation</b></summary>

* **Authors:** Guandong Li, Zhaobin Chu
* **arXiv ID:** 2509.05659
* **One-liner:** Presented EditIDv2, a tuning-free solution for high-complexity narrative scenes and long text inputs in character editing.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2509.05659) | [[PDF]](https://arxiv.org/pdf/2509.05659)

> **Core Innovation**
> Achieved deep semantic editing with identity consistency using minimal data and advanced attention decomposition.

<details>
    <summary>Abstract</summary>
    We propose EditIDv2, a tuning-free solution specifically designed for high-complexity narrative scenes and long text inputs. Existing character editing methods perform well under simple prompts, but often suffer from degraded editing capabilities, semantic understanding biases, and identity consistency breakdowns when faced with long text narratives containing multiple semantic layers, temporal logic, and complex contextual relationships. In EditID, we analyzed the impact of the ID integration module on editability. In EditIDv2, we further explore and address the influence of the ID feature integration module. The core of EditIDv2 is to discuss the issue of editability injection under minimal data lubrication. Through a sophisticated decomposition of PerceiverAttention, the introduction of ID loss and joint dynamic training with the diffusion model, as well as an offline fusion strategy for the integration module, we achieve deep, multi-level semantic editing while maintaining identity consistency in complex narrative environments using only a small amount of data lubrication. This meets the demands of long prompts and high-quality image generation, and achieves excellent results in the IBench evaluation.
</details>

<details>
    <summary>Key points</summary>
    * Decomposition of PerceiverAttention
    * ID loss and joint dynamic training
    * Offline fusion strategy for integration module
</details>
</details>

---


<details>
<summary><b> BiasMap: Leveraging Cross-Attentions to Discover and Mitigate Hidden Social Biases in Text-to-Image Generation</b></summary>

* **Authors:** Rajatsubhra Chakraborty, Xujun Che, Depeng Xu, Cori Faklaris, Xi Niu, Shuhan Yuan
* **arXiv ID:** 2509.13496
* **One-liner:** Proposed BiasMap, a framework for uncovering and mitigating latent concept-level representational biases in stable diffusion models.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2509.13496) | [[PDF]](https://arxiv.org/pdf/2509.13496)

> **Core Innovation**
> Revealed and reduced concept entanglement between demographics and semantics using cross-attention attribution.

<details>
    <summary>Abstract</summary>
    Bias discovery is critical for black-box generative models, especiall text-to-image (TTI) models. Existing works predominantly focus on output-level demographic distributions, which do not necessarily guarantee concept representations to be disentangled post-mitigation. We propose BiasMap, a model-agnostic framework for uncovering latent concept-level representational biases in stable diffusion models. BiasMap leverages cross-attention attribution maps to reveal structural entanglements between demographics (e.g., gender, race) and semantics (e.g., professions), going deeper into representational bias during the image generation. Using attribution maps of these concepts, we quantify the spatial demographics-semantics concept entanglement via Intersection over Union (IoU), offering a lens into bias that remains hidden in existing fairness discovery approaches. In addition, we further utilize BiasMap for bias mitigation through energy-guided diffusion sampling that directly modifies latent noise space and minimizes the expected SoftIoU during the denoising process. Our findings show that existing fairness interventions may reduce the output distributional gap but often fail to disentangle concept-level coupling, whereas our mitigation method can mitigate concept entanglement in image generation while complementing distributional bias mitigation.
</details>

<details>
    <summary>Key points</summary>
    * Cross-attention attribution maps for bias discovery
    * IoU-based quantification of concept entanglement
    * Energy-guided diffusion sampling for mitigation
</details>
</details>

---


<details>
<summary><b> DEFT: Decompositional Efficient Fine-Tuning for Text-to-Image Models</b></summary>

* **Authors:** Komal Kumar, Rao Muhammad Anwer, Fahad Shahbaz Khan, Salman Khan, Ivan Laptev, Hisham Cholakkal
* **arXiv ID:** 2509.22793
* **One-liner:** Introduced DEFT, a decompositional efficient fine-tuning framework for T2I models.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2509.22793) | [[PDF]](https://arxiv.org/pdf/2509.22793)

> **Core Innovation**
> Achieved state-of-the-art performance in personalization and multi-task adaptation with minimal trainable parameters.

<details>
    <summary>Abstract</summary>
    Efficient fine-tuning of pre-trained Text-to-Image (T2I) models involves adjusting the model to suit a particular task or dataset while minimizing computational resources and limiting the number of trainable parameters. However, it often faces challenges in striking a trade-off between aligning with the target distribution: learning a novel concept from a limited image for personalization and retaining the instruction ability needed for unifying multiple tasks, all while maintaining editability (aligning with a variety of prompts or in-context generation). In this work, we introduce DEFT, Decompositional Efficient Fine-Tuning, an efficient fine-tuning framework that adapts a pre-trained weight matrix by decomposing its update into two components with two trainable matrices: (1) a projection onto the complement of a low-rank subspace spanned by a low-rank matrix, and (2) a low-rank update. The single trainable low-rank matrix defines the subspace, while the other trainable low-rank matrix enables flexible parameter adaptation within that subspace. We conducted extensive experiments on the Dreambooth and Dreambench Plus datasets for personalization, the InsDet dataset for object and scene adaptation, and the VisualCloze dataset for a universal image generation framework through visual in-context learning with both Stable Diffusion and a unified model. Our results demonstrated state-of-the-art performance, highlighting the emergent properties of efficient fine-tuning. Our code is available on \href{<a href="https://github.com/MAXNORM8650/DEFT" rel="external noopener nofollow" class="link-external link-https">this https URL</a>}{DEFTBase}.
</details>

<details>
    <summary>Key points</summary>
    * Decomposition of weight matrix updates into two components
    * Low-rank subspace projection and update
    * Extensive experiments on various datasets
</details>
</details>

---


<details>
<summary><b> No Concept Left Behind: Test-Time Optimization for Compositional Text-to-Image Generation</b></summary>

* **Authors:** Mohammad Hossein Sameti, Amir M. Mansourian, Arash Marioriyad, Soheil Fadaee Oshyani, Mohammad Hossein Rohban, Mahdieh Soleymani Baghshah
* **arXiv ID:** 2509.23457
* **One-liner:** Proposed a fine-grained test-time optimization framework to enhance compositional faithfulness in T2I generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2509.23457) | [[PDF]](https://arxiv.org/pdf/2509.23457)

> **Core Innovation**
> Improved concept coverage and faithfulness by decomposing prompts and using concept-level feedback.

<details>
    <summary>Abstract</summary>
    Despite recent advances in text-to-image (T2I) models, they often fail to faithfully render all elements of complex prompts, frequently omitting or misrepresenting specific objects and attributes. Test-time optimization has emerged as a promising approach to address this limitation by refining generation without the need for retraining. In this paper, we propose a fine-grained test-time optimization framework that enhances compositional faithfulness in T2I generation. Unlike most of prior approaches that rely solely on a global image/text similarity score, our method decomposes the input prompt into semantic concepts and evaluates alignment at both the global and concept levels. A fine-grained variant of CLIP is used to compute concept-level correspondence, producing detailed feedback on missing or inaccurate concepts. This feedback is fed into an iterative prompt refinement loop, enabling the large language model to propose improved prompts. Experiments on DrawBench and CompBench prompts demonstrate that our method significantly improves concept coverage and human-judged faithfulness over both standard test-time optimization and the base T2I model. Code is available at: <a href="https://github.com/AmirMansurian/NoConceptLeftBehind" rel="external noopener nofollow" class="link-external link-https">this https URL</a>
</details>

<details>
    <summary>Key points</summary>
    * Fine-grained CLIP for concept-level alignment
    * Iterative prompt refinement loop with LLM
    * Evaluation on global and concept levels
</details>
</details>

---


<details>
<summary><b> Free Lunch Alignment of Text-to-Image Diffusion Models without Preference Image Pairs</b></summary>

* **Authors:** Jia Jun Cheng Xian, Muchen Li, Haotian Yang, Xin Tao, Pengfei Wan, Leonid Sigal, Renjie Liao
* **arXiv ID:** 2509.25771
* **One-liner:** Introduced Text Preference Optimization (TPO), a framework for aligning T2I models without paired image preference data.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2509.25771) | [[PDF]](https://arxiv.org/pdf/2509.25771)

> **Core Innovation**
> Achieved better human preference scores and alignment by training on perturbed prompts using LLM.

<details>
    <summary>Abstract</summary>
    Recent advances in diffusion-based text-to-image (T2I) models have led to remarkable success in generating high-quality images from textual prompts. However, ensuring accurate alignment between the text and the generated image remains a significant challenge for state-of-the-art diffusion models. To address this, existing studies employ reinforcement learning with human feedback (RLHF) to align T2I outputs with human preferences. These methods, however, either rely directly on paired image preference data or require a learned reward function, both of which depend heavily on costly, high-quality human annotations and thus face scalability limitations. In this work, we introduce Text Preference Optimization (TPO), a framework that enables &#34;free-lunch&#34; alignment of T2I models, achieving alignment without the need for paired image preference data. TPO works by training the model to prefer matched prompts over mismatched prompts, which are constructed by perturbing original captions using a large language model. Our framework is general and compatible with existing preference-based algorithms. We extend both DPO and KTO to our setting, resulting in TDPO and TKTO. Quantitative and qualitative evaluations across multiple benchmarks show that our methods consistently outperform their original counterparts, delivering better human preference scores and improved text-to-image alignment. Our Open-source code is available at <a href="https://github.com/DSL-Lab/T2I-Free-Lunch-Alignment" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Training with matched vs. mismatched prompts
    * Extension of DPO and KTO to TDPO and TKTO
    * General framework compatible with preference-based algorithms
</details>
</details>

---


<details>
<summary><b> VQGAN-CLIP: Open Domain Image Generation and Editing with Natural Language Guidance</b></summary>

* **Authors:** Katherine Crowson, Stella Biderman, Daniel Kornis, Dashiell Stander, Eric Hallahan, Louis Castricato, Edward Raff
* **arXiv ID:** 2204.08583
* **One-liner:** Introduced a training-free method using CLIP to guide VQGAN for high-quality image generation and editing from complex text prompts.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2204.08583) | [[PDF]](https://arxiv.org/pdf/2204.08583)

> **Core Innovation**
> Achieved superior visual quality in image generation and editing without model training by leveraging multimodal guidance.

<details>
    <summary>Abstract</summary>
    Generating and editing images from open domain text prompts is a challenging task that heretofore has required expensive and specially trained models. We demonstrate a novel methodology for both tasks which is capable of producing images of high visual quality from text prompts of significant semantic complexity without any training by using a multimodal encoder to guide image generations. We demonstrate on a variety of tasks how using CLIP [37] to guide VQGAN [11] produces higher visual quality outputs than prior, less flexible approaches like DALL-E [38], GLIDE [33] and Open-Edit [24], despite not being trained for the tasks presented. Our code is available in a public repository.
</details>

<details>
    <summary>Key points</summary>
    * Used CLIP to guide VQGAN for image generation
    * Applied to various tasks without additional training
    * Outperformed models like DALL-E and GLIDE in visual quality
</details>
</details>

---


<details>
<summary><b> InstructPix2Pix: Learning to Follow Image Editing Instructions</b></summary>

* **Authors:** Tim Brooks, Aleksander Holynski, Alexei A. Efros
* **arXiv ID:** 2211.09800
* **One-liner:** Developed InstructPix2Pix, a model for fast image editing from human instructions using generated training data.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2211.09800) | [[PDF]](https://arxiv.org/pdf/2211.09800)

> **Core Innovation**
> Enabled quick image edits in seconds by training on a dataset synthesized with GPT-3 and Stable Diffusion.

<details>
    <summary>Abstract</summary>
    We propose a method for editing images from human instructions: given an input image and a written instruction that tells the model what to do, our model follows these instructions to edit the image. To obtain training data for this problem, we combine the knowledge of two large pretrained models -- a language model (GPT-3) and a text-to-image model (Stable Diffusion) -- to generate a large dataset of image editing examples. Our conditional diffusion model, InstructPix2Pix, is trained on our generated data, and generalizes to real images and user-written instructions at inference time. Since it performs edits in the forward pass and does not require per example fine-tuning or inversion, our model edits images quickly, in a matter of seconds. We show compelling editing results for a diverse collection of input images and written instructions.
</details>

<details>
    <summary>Key points</summary>
    * Combined GPT-3 and Stable Diffusion to generate training data
    * Trained a conditional diffusion model for instruction-based editing
    * Generalized to real images and user instructions without fine-tuning
</details>
</details>

---


<details>
<summary><b> MagicBrush: A Manually Annotated Dataset for Instruction-Guided Image Editing</b></summary>

* **Authors:** Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, Yu Su
* **arXiv ID:** 2306.10012
* **One-liner:** Created MagicBrush, the first large-scale manually annotated dataset for instruction-guided image editing.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2306.10012) | [[PDF]](https://arxiv.org/pdf/2306.10012)

> **Core Innovation**
> Improved model performance by fine-tuning on high-quality annotated data, addressing noise in automated datasets.

<details>
    <summary>Abstract</summary>
    Text-guided image editing is widely needed in daily life, ranging from personal use to professional applications such as Photoshop. However, existing methods are either zero-shot or trained on an automatically synthesized dataset, which contains a high volume of noise. Thus, they still require lots of manual tuning to produce desirable outcomes in practice. To address this issue, we introduce MagicBrush (<a href="https://osu-nlp-group.github.io/MagicBrush/" rel="external noopener nofollow" class="link-external link-https">this https URL</a>), the first large-scale, manually annotated dataset for instruction-guided real image editing that covers diverse scenarios: single-turn, multi-turn, mask-provided, and mask-free editing. MagicBrush comprises over 10K manually annotated triplets (source image, instruction, target image), which supports trainining large-scale text-guided image editing models. We fine-tune InstructPix2Pix on MagicBrush and show that the new model can produce much better images according to human evaluation. We further conduct extensive experiments to evaluate current image editing baselines from multiple dimensions including quantitative, qualitative, and human evaluations. The results reveal the challenging nature of our dataset and the gap between current baselines and real-world editing needs.
</details>

<details>
    <summary>Key points</summary>
    * Collected over 10K manually annotated triplets (source image, instruction, target image)
    * Fine-tuned InstructPix2Pix on MagicBrush
    * Conducted evaluations showing better image quality and dataset challenge
</details>
</details>

---


<details>
<summary><b> An Item is Worth a Prompt: Versatile Image Editing with Disentangled Control</b></summary>

* **Authors:** Aosong Feng, Weikang Qiu, Jinbin Bai, Xiao Zhang, Zhen Dong, Kaicheng Zhou, Rex Ying, Leandros Tassiulas
* **arXiv ID:** 2403.04880
* **One-liner:** Proposed D-Edit, a framework for versatile image editing by disentangling item-prompt interactions in diffusion models.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2403.04880) | [[PDF]](https://arxiv.org/pdf/2403.04880)

> **Core Innovation**
> Achieved state-of-the-art editing in multiple types (image-based, text-based, mask-based, item removal) with a unified approach.

<details>
    <summary>Abstract</summary>
    Building on the success of text-to-image diffusion models (DPMs), image editing is an important application to enable human interaction with AI-generated content. Among various editing methods, editing within the prompt space gains more attention due to its capacity and simplicity of controlling semantics. However, since diffusion models are commonly pretrained on descriptive text captions, direct editing of words in text prompts usually leads to completely different generated images, violating the requirements for image editing. On the other hand, existing editing methods usually consider introducing spatial masks to preserve the identity of unedited regions, which are usually ignored by DPMs and therefore lead to inharmonic editing results. Targeting these two challenges, in this work, we propose to disentangle the comprehensive image-prompt interaction into several item-prompt interactions, with each item linked to a special learned prompt. The resulting framework, named D-Edit, is based on pretrained diffusion models with cross-attention layers disentangled and adopts a two-step optimization to build item-prompt associations. Versatile image editing can then be applied to specific items by manipulating the corresponding prompts. We demonstrate state-of-the-art results in four types of editing operations including image-based, text-based, mask-based editing, and item removal, covering most types of editing applications, all within a single unified framework. Notably, D-Edit is the first framework that can (1) achieve item editing through mask editing and (2) combine image and text-based editing. We demonstrate the quality and versatility of the editing results for a diverse collection of images through both qualitative and quantitative evaluations.
</details>

<details>
    <summary>Key points</summary>
    * Disentangled cross-attention layers in diffusion models
    * Used two-step optimization for item-prompt associations
    * Enabled item editing through mask manipulation and combined editing types
</details>
</details>

---


<details>
<summary><b> InstructGIE: Towards Generalizable Image Editing</b></summary>

* **Authors:** Zichong Meng, Changdi Yang, Jun Liu, Hao Tang, Pu Zhao, Yanzhi Wang
* **arXiv ID:** 2403.05018
* **One-liner:** Introduced a robust image editing framework with enhanced in-context learning and language unification.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2403.05018) | [[PDF]](https://arxiv.org/pdf/2403.05018)

> **Core Innovation**
> Improved generalization and quality by incorporating VMamba Block, editing-shift matching, and selective area-matching.

<details>
    <summary>Abstract</summary>
    Recent advances in image editing have been driven by the development of denoising diffusion models, marking a significant leap forward in this field. Despite these advances, the generalization capabilities of recent image editing approaches remain constrained. In response to this challenge, our study introduces a novel image editing framework with enhanced generalization robustness by boosting in-context learning capability and unifying language instruction. This framework incorporates a module specifically optimized for image editing tasks, leveraging the VMamba Block and an editing-shift matching strategy to augment in-context learning. Furthermore, we unveil a selective area-matching technique specifically engineered to address and rectify corrupted details in generated images, such as human facial features, to further improve the quality. Another key innovation of our approach is the integration of a language unification technique, which aligns language embeddings with editing semantics to elevate the quality of image editing. Moreover, we compile the first dataset for image editing with visual prompts and editing instructions that could be used to enhance in-context capability. Trained on this dataset, our methodology not only achieves superior synthesis quality for trained tasks, but also demonstrates robust generalization capability across unseen vision tasks through tailored prompts.
</details>

<details>
    <summary>Key points</summary>
    * Leveraged VMamba Block and editing-shift matching for in-context learning
    * Applied selective area-matching to correct corrupted details
    * Used language unification to align embeddings with editing semantics
</details>
</details>

---


<details>
<summary><b> Leveraging LLMs for On-the-Fly Instruction Guided Image Editing</b></summary>

* **Authors:** Rodrigo Santos, João Silva, António Branco
* **arXiv ID:** 2403.08004
* **One-liner:** Developed a preparation-free method for instruction-guided image editing using image captioning and DDIM inversion.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2403.08004) | [[PDF]](https://arxiv.org/pdf/2403.08004)

> **Core Innovation**
> Achieved competitive performance without preliminary training or fine-tuning.

<details>
    <summary>Abstract</summary>
    The combination of language processing and image processing keeps attracting increased interest given recent impressive advances that leverage the combined strengths of both domains of research. Among these advances, the task of editing an image on the basis solely of a natural language instruction stands out as a most challenging endeavour. While recent approaches for this task resort, in one way or other, to some form of preliminary preparation, training or fine-tuning, this paper explores a novel approach: We propose a preparation-free method that permits instruction-guided image editing on the fly. This approach is organized along three steps properly orchestrated that resort to image captioning and DDIM inversion, followed by obtaining the edit direction embedding, followed by image editing proper. While dispensing with preliminary preparation, our approach demonstrates to be effective and competitive, outperforming recent, state of the art models for this task when evaluated on the MAGICBRUSH dataset.
</details>

<details>
    <summary>Key points</summary>
    * Orchestrated three steps: image captioning, DDIM inversion, and edit direction embedding
    * Dispensed with preparation steps
    * Outperformed state-of-the-art models on MAGICBRUSH dataset
</details>
</details>

---


<details>
<summary><b> Enhancing Text-to-Image Editing via Hybrid Mask-Informed Fusion</b></summary>

* **Authors:** Aoxue Li, Mingyang Yi, Zhenguo Li
* **arXiv ID:** 2405.15313
* **One-liner:** Proposed MaSaFusion to improve text-guided image editing by incorporating mask-informed fusion in self-attention modules.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2405.15313) | [[PDF]](https://arxiv.org/pdf/2405.15313)

> **Core Innovation**
> Addressed inconsistencies in diffusion-based editing by reducing interference in texture retention and new character creation.

<details>
    <summary>Abstract</summary>
    Recently, text-to-image (T2I) editing has been greatly pushed forward by applying diffusion models. Despite the visual promise of the generated images, inconsistencies with the expected textual prompt remain prevalent. This paper aims to systematically improve the text-guided image editing techniques based on diffusion models, by addressing their limitations. Notably, the common idea in diffusion-based editing firstly reconstructs the source image via inversion techniques e.g., DDIM Inversion. Then following a fusion process that carefully integrates the source intermediate (hidden) states (obtained by inversion) with the ones of the target image. Unfortunately, such a standard pipeline fails in many cases due to the interference of texture retention and the new characters creation in some regions. To mitigate this, we incorporate human annotation as an external knowledge to confine editing within a ``Mask-informed&#39;&#39; region. Then we carefully Fuse the edited image with the source image and a constructed intermediate image within the model&#39;s Self-Attention module. Extensive empirical results demonstrate the proposed ``MaSaFusion&#39;&#39; significantly improves the existing T2I editing techniques.
</details>

<details>
    <summary>Key points</summary>
    * Used human annotation to define mask-informed regions
    * Fused source and target images in self-attention modules
    * Improved editing accuracy and quality empirically
</details>
</details>

---


<details>
<summary><b> Text Guided Image Editing with Automatic Concept Locating and Forgetting</b></summary>

* **Authors:** Jia Li, Lijie Hu, Zhixian He, Jingfeng Zhang, Tianhang Zheng, Di Wang
* **arXiv ID:** 2405.19708
* **One-liner:** Introduced Locate and Forget (LaF) method for precise text-guided image editing by comparing syntactic trees.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2405.19708) | [[PDF]](https://arxiv.org/pdf/2405.19708)

> **Core Innovation**
> Enhanced semantic alignment between text prompts and image modifications by locating and forgetting target concepts.

<details>
    <summary>Abstract</summary>
    With the advancement of image-to-image diffusion models guided by text, significant progress has been made in image editing. However, a persistent challenge remains in seamlessly incorporating objects into images based on textual instructions, without relying on extra user-provided guidance. Text and images are inherently distinct modalities, bringing out difficulties in fully capturing the semantic intent conveyed through language and accurately translating that into the desired visual modifications. Therefore, text-guided image editing models often produce generations with residual object attributes that do not fully align with human expectations. To address this challenge, the models should comprehend the image content effectively away from a disconnect between the provided textual editing prompts and the actual modifications made to the image. In our paper, we propose a novel method called Locate and Forget (LaF), which effectively locates potential target concepts in the image for modification by comparing the syntactic trees of the target prompt and scene descriptions in the input image, intending to forget their existence clues in the generated image. Compared to the baselines, our method demonstrates its superiority in text-guided image editing tasks both qualitatively and quantitatively.
</details>

<details>
    <summary>Key points</summary>
    * Compared syntactic trees of target prompt and scene descriptions
    * Located target concepts for modification
    * Forgot existence clues in generated images to improve alignment
</details>
</details>

---


<details>
<summary><b> Empowering Visual Creativity: A Vision-Language Assistant to Image Editing Recommendations</b></summary>

* **Authors:** Tiancheng Shen, Jun Hao Liew, Long Mai, Lu Qi, Jiashi Feng, Jiaya Jia
* **arXiv ID:** 2406.00121
* **One-liner:** Proposed Image Editing Recommendation (IER) task and Creativity-VLA framework for generating creative editing instructions from vague prompts.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2406.00121) | [[PDF]](https://arxiv.org/pdf/2406.00121)

> **Core Innovation**
> Automated the generation of diverse and relevant editing instructions to bridge the gap in user ideation.

<details>
    <summary>Abstract</summary>
    Advances in text-based image generation and editing have revolutionized content creation, enabling users to create impressive content from imaginative text prompts. However, existing methods are not designed to work well with the oversimplified prompts that are often encountered in typical scenarios when users start their editing with only vague or abstract purposes in mind. Those scenarios demand elaborate ideation efforts from the users to bridge the gap between such vague starting points and the detailed creative ideas needed to depict the desired results. In this paper, we introduce the task of Image Editing Recommendation (IER). This task aims to automatically generate diverse creative editing instructions from an input image and a simple prompt representing the users&#39; under-specified editing purpose. To this end, we introduce Creativity-Vision Language Assistant~(Creativity-VLA), a multimodal framework designed specifically for edit-instruction generation. We train Creativity-VLA on our edit-instruction dataset specifically curated for IER. We further enhance our model with a novel &#39;token-for-localization&#39; mechanism, enabling it to support both global and local editing operations. Our experimental results demonstrate the effectiveness of \ours{} in suggesting instructions that not only contain engaging creative elements but also maintain high relevance to both the input image and the user&#39;s initial hint.
</details>

<details>
    <summary>Key points</summary>
    * Introduced IER task for automatic instruction generation
    * Trained Creativity-VLA on a curated edit-instruction dataset
    * Used token-for-localization mechanism for global and local editing
</details>
</details>

---


<details>
<summary><b> The Curious Case of End Token: A Zero-Shot Disentangled Image Editing using CLIP</b></summary>

* **Authors:** Hidir Yesiltepe, Yusuf Dalva, Pinar Yanardag
* **arXiv ID:** 2406.00457
* **One-liner:** Demonstrated that CLIP enables disentangled editing in diffusion models in a zero-shot manner.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2406.00457) | [[PDF]](https://arxiv.org/pdf/2406.00457)

> **Core Innovation**
> Provided a lightweight approach for precise attribute manipulation without compromising image coherence.

<details>
    <summary>Abstract</summary>
    Diffusion models have become prominent in creating high-quality images. However, unlike GAN models celebrated for their ability to edit images in a disentangled manner, diffusion-based text-to-image models struggle to achieve the same level of precise attribute manipulation without compromising image coherence. In this paper, CLIP which is often used in popular text-to-image diffusion models such as Stable Diffusion is capable of performing disentangled editing in a zero-shot manner. Through both qualitative and quantitative comparisons with state-of-the-art editing methods, we show that our approach yields competitive results. This insight may open opportunities for applying this method to various tasks, including image and video editing, providing a lightweight and efficient approach for disentangled editing.
</details>

<details>
    <summary>Key points</summary>
    * Applied CLIP for zero-shot disentangled editing
    * Compared with state-of-the-art methods qualitatively and quantitatively
    * Highlighted potential for image and video editing applications
</details>
</details>

---


<details>
<summary><b> UltraEdit: Instruction-based Fine-Grained Image Editing at Scale</b></summary>

* **Authors:** Haozhe Zhao, Xiaojian Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, Baobao Chang
* **arXiv ID:** 2407.05282
* **One-liner:** Created a large-scale, high-quality dataset for instruction-based image editing.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2407.05282) | [[PDF]](https://arxiv.org/pdf/2407.05282)

> **Core Innovation**
> Systematically generated a dataset with broader editing instructions, real image sources, and region-based editing support.

<details>
    <summary>Abstract</summary>
    This paper presents UltraEdit, a large-scale (approximately 4 million editing samples), automatically generated dataset for instruction-based image editing. Our key idea is to address the drawbacks in existing image editing datasets like InstructPix2Pix and MagicBrush, and provide a systematic approach to producing massive and high-quality image editing samples. UltraEdit offers several distinct advantages: 1) It features a broader range of editing instructions by leveraging the creativity of large language models (LLMs) alongside in-context editing examples from human raters; 2) Its data sources are based on real images, including photographs and artworks, which provide greater diversity and reduced bias compared to datasets solely generated by text-to-image models; 3) It also supports region-based editing, enhanced by high-quality, automatically produced region annotations. Our experiments show that canonical diffusion-based editing baselines trained on UltraEdit set new records on MagicBrush and Emu-Edit benchmarks. Our analysis further confirms the crucial role of real image anchors and region-based editing data. The dataset, code, and models can be found in <a href="https://ultra-editing.github.io" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Leveraged LLMs and human examples for diverse instructions
    * Used real images for diversity and reduced bias
    * Included region-based editing with automatic annotations
</details>
</details>

---


<details>
<summary><b> EditScribe: Non-Visual Image Editing with Natural Language Verification Loops</b></summary>

* **Authors:** Ruei-Che Chang, Yuxuan Liu, Lotus Zhang, Anhong Guo
* **arXiv ID:** 2408.06632
* **One-liner:** Developed an accessible image editing system for blind and low-vision users.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2408.06632) | [[PDF]](https://arxiv.org/pdf/2408.06632)

> **Core Innovation**
> Introduced natural language verification loops for non-visual image editing and feedback.

<details>
    <summary>Abstract</summary>
    Image editing is an iterative process that requires precise visual evaluation and manipulation for the output to match the editing intent. However, current image editing tools do not provide accessible interaction nor sufficient feedback for blind and low vision individuals to achieve this level of control. To address this, we developed EditScribe, a prototype system that makes image editing accessible using natural language verification loops powered by large multimodal models. Using EditScribe, the user first comprehends the image content through initial general and object descriptions, then specifies edit actions using open-ended natural language prompts. EditScribe performs the image edit, and provides four types of verification feedback for the user to verify the performed edit, including a summary of visual changes, AI judgement, and updated general and object descriptions. The user can ask follow-up questions to clarify and probe into the edits or verification feedback, before performing another edit. In a study with ten blind or low-vision users, we found that EditScribe supported participants to perform and verify image edit actions non-visually. We observed different prompting strategies from participants, and their perceptions on the various types of verification feedback. Finally, we discuss the implications of leveraging natural language verification loops to make visual authoring non-visually accessible.
</details>

<details>
    <summary>Key points</summary>
    * Used initial descriptions for image comprehension
    * Provided four types of verification feedback
    * Enabled follow-up questions for clarification
</details>
</details>

---


<details>
<summary><b> FastEdit: Fast Text-Guided Single-Image Editing via Semantic-Aware Diffusion Fine-Tuning</b></summary>

* **Authors:** Zhi Chen, Zecheng Zhao, Yadan Luo, Zi Huang
* **arXiv ID:** 2408.03355
* **One-liner:** Accelerated text-guided single-image editing to 17 seconds.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2408.03355) | [[PDF]](https://arxiv.org/pdf/2408.03355)

> **Core Innovation**
> Streamlined fine-tuning and used semantic-aware diffusion for faster processing.

<details>
    <summary>Abstract</summary>
    Conventional Text-guided single-image editing approaches require a two-step process, including fine-tuning the target text embedding for over 1K iterations and the generative model for another 1.5K iterations. Although it ensures that the resulting image closely aligns with both the input image and the target text, this process often requires 7 minutes per image, posing a challenge for practical application due to its time-intensive nature. To address this bottleneck, we introduce FastEdit, a fast text-guided single-image editing method with semantic-aware diffusion fine-tuning, dramatically accelerating the editing process to only 17 seconds. FastEdit streamlines the generative model&#39;s fine-tuning phase, reducing it from 1.5K to a mere 50 iterations. For diffusion fine-tuning, we adopt certain time step values based on the semantic discrepancy between the input image and target text. Furthermore, FastEdit circumvents the initial fine-tuning step by utilizing an image-to-image model that conditions on the feature space, rather than the text embedding space. It can effectively align the target text prompt and input image within the same feature space and save substantial processing time. Additionally, we apply the parameter-efficient fine-tuning technique LoRA to U-net. With LoRA, FastEdit minimizes the model&#39;s trainable parameters to only 0.37\% of the original size. At the same time, we can achieve comparable editing outcomes with significantly reduced computational overhead. We conduct extensive experiments to validate the editing performance of our approach and show promising editing capabilities, including content addition, style transfer, background replacement, and posture manipulation, etc.
</details>

<details>
    <summary>Key points</summary>
    * Reduced fine-tuning iterations from 1.5K to 50
    * Adopted time steps based on semantic discrepancy
    * Applied LoRA for parameter efficiency
</details>
</details>

---


<details>
<summary><b> TurboEdit: Instant text-based image editing</b></summary>

* **Authors:** Zongze Wu, Nicholas Kolkin, Jonathan Brandt, Richard Zhang, Eli Shechtman
* **arXiv ID:** 2408.08332
* **One-liner:** Enabled precise image inversion and disentangled editing in few-step diffusion models.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2408.08332) | [[PDF]](https://arxiv.org/pdf/2408.08332)

> **Core Innovation**
> Used encoder-based iterative inversion and text conditioning for real-time edits.

<details>
    <summary>Abstract</summary>
    We address the challenges of precise image inversion and disentangled image editing in the context of few-step diffusion models. We introduce an encoder based iterative inversion technique. The inversion network is conditioned on the input image and the reconstructed image from the previous step, allowing for correction of the next reconstruction towards the input image. We demonstrate that disentangled controls can be easily achieved in the few-step diffusion model by conditioning on an (automatically generated) detailed text prompt. To manipulate the inverted image, we freeze the noise maps and modify one attribute in the text prompt (either manually or via instruction based editing driven by an LLM), resulting in the generation of a new image similar to the input image with only one attribute changed. It can further control the editing strength and accept instructive text prompt. Our approach facilitates realistic text-guided image edits in real-time, requiring only 8 number of functional evaluations (NFEs) in inversion (one-time cost) and 4 NFEs per edit. Our method is not only fast, but also significantly outperforms state-of-the-art multi-step diffusion editing techniques.
</details>

<details>
    <summary>Key points</summary>
    * Introduced iterative inversion with correction
    * Froze noise maps for attribute changes
    * Achieved edits with low NFEs (8 for inversion, 4 per edit)
</details>
</details>

---


<details>
<summary><b> ReEdit: Multimodal Exemplar-Based Image Editing with Diffusion Models</b></summary>

* **Authors:** Ashutosh Srivastava, Tarun Ram Menta, Abhinav Java, Avadhoot Jadhav, Silky Singh, Surgan Jandial, Balaji Krishnamurthy
* **arXiv ID:** 2411.03982
* **One-liner:** Proposed an efficient exemplar-based image editing framework.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2411.03982) | [[PDF]](https://arxiv.org/pdf/2411.03982)

> **Core Innovation**
> Captured edits in text and image modalities for high fidelity and speed.

<details>
    <summary>Abstract</summary>
    Modern Text-to-Image (T2I) Diffusion models have revolutionized image editing by enabling the generation of high-quality photorealistic images. While the de facto method for performing edits with T2I models is through text instructions, this approach non-trivial due to the complex many-to-many mapping between natural language and images. In this work, we address exemplar-based image editing -- the task of transferring an edit from an exemplar pair to a content image(s). We propose ReEdit, a modular and efficient end-to-end framework that captures edits in both text and image modalities while ensuring the fidelity of the edited image. We validate the effectiveness of ReEdit through extensive comparisons with state-of-the-art baselines and sensitivity analyses of key design choices. Our results demonstrate that ReEdit consistently outperforms contemporary approaches both qualitatively and quantitatively. Additionally, ReEdit boasts high practical applicability, as it does not require any task-specific optimization and is four times faster than the next best baseline.
</details>

<details>
    <summary>Key points</summary>
    * Modular end-to-end framework
    * No task-specific optimization required
    * Four times faster than baselines
</details>
</details>

---


<details>
<summary><b> Multi-Reward as Condition for Instruction-based Image Editing</b></summary>

* **Authors:** Xin Gu, Ming Li, Libo Zhang, Fan Chen, Longyin Wen, Tiejian Luo, Sijie Zhu
* **arXiv ID:** 2411.04713
* **One-liner:** Improved training data quality with multi-perspective reward data.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2411.04713) | [[PDF]](https://arxiv.org/pdf/2411.04713)

> **Core Innovation**
> Designed a metric system and training framework for better editing models.

<details>
    <summary>Abstract</summary>
    High-quality training triplets (instruction, original image, edited image) are essential for instruction-based image editing. Predominant training datasets (e.g., InsPix2Pix) are created using text-to-image generative models (e.g., Stable Diffusion, DALL-E) which are not trained for image editing. Accordingly, these datasets suffer from inaccurate instruction following, poor detail preserving, and generation artifacts. In this paper, we propose to address the training data quality issue with multi-perspective reward data instead of refining the ground-truth image quality. 1) we first design a quantitative metric system based on best-in-class LVLM (Large Vision Language Model), i.e., GPT-4o in our case, to evaluate the generation quality from 3 perspectives, namely, instruction following, detail preserving, and generation quality. For each perspective, we collected quantitative score in $0\sim 5$ and text descriptive feedback on the specific failure points in ground-truth edited images, resulting in a high-quality editing reward dataset, i.e., RewardEdit20K. 2) We further proposed a novel training framework to seamlessly integrate the metric output, regarded as multi-reward, into editing models to learn from the imperfect training triplets. During training, the reward scores and text descriptions are encoded as embeddings and fed into both the latent space and the U-Net of the editing models as auxiliary conditions. 3) We also build a challenging evaluation benchmark with real-world images/photos and diverse editing instructions, named Real-Edit. Experiments indicate that our multi-reward conditioned model outperforms its no-reward counterpart on two popular editing pipelines, i.e., InsPix2Pix and SmartEdit. Code is released at <a href="https://github.com/bytedance/Multi-Reward-Editing" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Used LVLM for evaluation from three perspectives
    * Integrated rewards as embeddings in models
    * Built a challenging benchmark (Real-Edit)
</details>
</details>

---


<details>
<summary><b> OmniEdit: Building Image Editing Generalist Models Through Specialist Supervision</b></summary>

* **Authors:** Cong Wei, Zheyang Xiong, Weiming Ren, Xinrun Du, Ge Zhang, Wenhu Chen
* **arXiv ID:** 2411.07199
* **One-liner:** Developed an omnipotent editor for multiple tasks and aspect ratios.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2411.07199) | [[PDF]](https://arxiv.org/pdf/2411.07199)

> **Core Innovation**
> Utilized supervision from specialist models and improved data quality.

<details>
    <summary>Abstract</summary>
    Instruction-guided image editing methods have demonstrated significant potential by training diffusion models on automatically synthesized or manually annotated image editing pairs. However, these methods remain far from practical, real-life applications. We identify three primary challenges contributing to this gap. Firstly, existing models have limited editing skills due to the biased synthesis process. Secondly, these methods are trained with datasets with a high volume of noise and artifacts. This is due to the application of simple filtering methods like CLIP-score. Thirdly, all these datasets are restricted to a single low resolution and fixed aspect ratio, limiting the versatility to handle real-world use cases. In this paper, we present \omniedit, which is an omnipotent editor to handle seven different image editing tasks with any aspect ratio seamlessly. Our contribution is in four folds: (1) \omniedit is trained by utilizing the supervision from seven different specialist models to ensure task coverage. (2) we utilize importance sampling based on the scores provided by large multimodal models (like GPT-4o) instead of CLIP-score to improve the data quality. (3) we propose a new editing architecture called EditNet to greatly boost the editing success rate, (4) we provide images with different aspect ratios to ensure that our model can handle any image in the wild. We have curated a test set containing images of different aspect ratios, accompanied by diverse instructions to cover different tasks. Both automatic evaluation and human evaluations demonstrate that \omniedit can significantly outperform all the existing models. Our code, dataset and model will be available at <a href="https://tiger-ai-lab.github.io/OmniEdit/" rel="external noopener nofollow" class="link-external link-https">this https URL</a>
</details>

<details>
    <summary>Key points</summary>
    * Trained with seven specialist models
    * Used importance sampling with LMM scores
    * Proposed EditNet architecture for higher success rate
</details>
</details>

---


<details>
<summary><b> AnyEdit: Mastering Unified High-Quality Image Editing for Any Idea</b></summary>

* **Authors:** Qifan Yu, Wei Chow, Zhongqi Yue, Kaihang Pan, Yang Wu, Xiaoyang Wan, Juncheng Li, Siliang Tang, Hanwang Zhang, Yueting Zhuang
* **arXiv ID:** 2411.15738
* **One-liner:** Created a comprehensive multi-modal instruction editing dataset.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2411.15738) | [[PDF]](https://arxiv.org/pdf/2411.15738)

> **Core Innovation**
> Ensured diversity and quality across editing types and domains.

<details>
    <summary>Abstract</summary>
    Instruction-based image editing aims to modify specific image elements with natural language instructions. However, current models in this domain often struggle to accurately execute complex user instructions, as they are trained on low-quality data with limited editing types. We present AnyEdit, a comprehensive multi-modal instruction editing dataset, comprising 2.5 million high-quality editing pairs spanning over 20 editing types and five domains. We ensure the diversity and quality of the AnyEdit collection through three aspects: initial data diversity, adaptive editing process, and automated selection of editing results. Using the dataset, we further train a novel AnyEdit Stable Diffusion with task-aware routing and learnable task embedding for unified image editing. Comprehensive experiments on three benchmark datasets show that AnyEdit consistently boosts the performance of diffusion-based editing models. This presents prospects for developing instruction-driven image editing models that support human creativity.
</details>

<details>
    <summary>Key points</summary>
    * Collected 2.5 million high-quality pairs
    * Used adaptive editing and automated selection
    * Trained model with task-aware routing
</details>
</details>

---


<details>
<summary><b> TPIE: Topology-Preserved Image Editing With Text Instructions</b></summary>

* **Authors:** Nivetha Jayakumar, Srivardhan Reddy Gadila, Tonmoy Hossain, Yangfeng Ji, Miaomiao Zhang
* **arXiv ID:** 2411.16714
* **One-liner:** Introduced topology-preserved image editing for sensitive domains.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2411.16714) | [[PDF]](https://arxiv.org/pdf/2411.16714)

> **Core Innovation**
> Ensured geometry remains intact through deformable variations and diffusion models.

<details>
    <summary>Abstract</summary>
    Preserving topological structures is important in real-world applications, particularly in sensitive domains such as healthcare and medicine, where the correctness of human anatomy is critical. However, most existing image editing models focus on manipulating intensity and texture features, often overlooking object geometry within images. To address this issue, this paper introduces a novel method, Topology-Preserved Image Editing with text instructions (TPIE), that for the first time ensures the topology and geometry remaining intact in edited images through text-guided generative diffusion models. More specifically, our method treats newly generated samples as deformable variations of a given input template, allowing for controllable and structure-preserving edits. Our proposed TPIE framework consists of two key modules: (i) an autoencoder-based registration network that learns latent representations of object transformations, parameterized by velocity fields, from pairwise training images; and (ii) a novel latent conditional geometric diffusion (LCDG) model efficiently capturing the data distribution of learned transformation features conditioned on custom-defined text instructions. We validate TPIE on a diverse set of 2D and 3D images and compare them with state-of-the-art image editing approaches. Experimental results show that our method outperforms other baselines in generating more realistic images with well-preserved topology. Our code will be made publicly available on Github.
</details>

<details>
    <summary>Key points</summary>
    * Autoencoder-based registration network
    * Latent conditional geometric diffusion model
    * Validated on 2D and 3D images
</details>
</details>

---


<details>
<summary><b> InsightEdit: Towards Better Instruction Following for Image Editing</b></summary>

* **Authors:** Yingjing Xu, Jie Kong, Jiazhi Wang, Xiao Pan, Bo Lin, Qiang Liu
* **arXiv ID:** 2411.17323
* **One-liner:** Enhanced instruction-based editing with better dataset and feature utilization.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2411.17323) | [[PDF]](https://arxiv.org/pdf/2411.17323)

> **Core Innovation**
> Curated a high-quality dataset and used two-stream bridging for precise guidance.

<details>
    <summary>Abstract</summary>
    In this paper, we focus on the task of instruction-based image editing. Previous works like InstructPix2Pix, InstructDiffusion, and SmartEdit have explored end-to-end editing. However, two limitations still remain: First, existing datasets suffer from low resolution, poor background consistency, and overly simplistic instructions. Second, current approaches mainly condition on the text while the rich image information is underexplored, therefore inferior in complex instruction following and maintaining background consistency. Targeting these issues, we first curated the AdvancedEdit dataset using a novel data construction pipeline, formulating a large-scale dataset with high visual quality, complex instructions, and good background consistency. Then, to further inject the rich image information, we introduce a two-stream bridging mechanism utilizing both the textual and visual features reasoned by the powerful Multimodal Large Language Models (MLLM) to guide the image editing process more precisely. Extensive results demonstrate that our approach, InsightEdit, achieves state-of-the-art performance, excelling in complex instruction following and maintaining high background consistency with the original image.
</details>

<details>
    <summary>Key points</summary>
    * AdvancedEdit dataset with complex instructions
    * Two-stream mechanism with textual and visual features
    * Improved background consistency and instruction following
</details>
</details>

---


<details>
<summary><b> UIP2P: Unsupervised Instruction-based Image Editing via Edit Reversibility Constraint</b></summary>

* **Authors:** Enis Simsar, Alessio Tonioni, Yongqin Xian, Thomas Hofmann, Federico Tombari
* **arXiv ID:** 2412.15216
* **One-liner:** Proposed an unsupervised instruction-based image editing approach eliminating the need for ground-truth edited images during training.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2412.15216) | [[PDF]](https://arxiv.org/pdf/2412.15216)

> **Core Innovation**
> Introduced the Edit Reversibility Constraint (ERC) to apply forward and reverse edits, enabling training on datasets with real image-caption pairs or triplets without ground-truth edited images.

<details>
    <summary>Abstract</summary>
    We propose an unsupervised instruction-based image editing approach that removes the need for ground-truth edited images during training. Existing methods rely on supervised learning with triplets of input images, ground-truth edited images, and edit instructions. These triplets are typically generated either by existing editing methods, introducing biases, or through human annotations, which are costly and limit generalization. Our approach addresses these challenges by introducing a novel editing mechanism called Edit Reversibility Constraint (ERC), which applies forward and reverse edits in one training step and enforces alignment in image, text, and attention spaces. This allows us to bypass the need for ground-truth edited images and unlock training for the first time on datasets comprising either real image-caption pairs or image-caption-instruction triplets. We empirically show that our approach performs better across a broader range of edits with high-fidelity and precision. By eliminating the need for pre-existing datasets of triplets, reducing biases associated with current methods, and proposing ERC, our work represents a significant advancement in unblocking scaling of instruction-based image editing.
</details>

<details>
    <summary>Key points</summary>
    * Edit Reversibility Constraint (ERC) for forward and reverse edits
    * Alignment enforcement in image, text, and attention spaces
    * Training on datasets without ground-truth edited images
</details>
</details>

---


<details>
<summary><b> Textualize Visual Prompt for Image Editing via Diffusion Bridge</b></summary>

* **Authors:** Pengcheng Xu, Qingnan Fan, Fei Kou, Shuai Qin, Hong Gu, Ruoyu Zhao, Charles Ling, Boyu Wang
* **arXiv ID:** 2501.03495
* **One-liner:** Developed a framework for visual prompt-based image editing using any single text-to-image model without relying on an explicit image-to-image model.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2501.03495) | [[PDF]](https://arxiv.org/pdf/2501.03495)

> **Core Innovation**
> Leveraged probability-flow ordinary equation to construct a diffusion bridge for distribution transfer and optimized text embeddings via differential attention control.

<details>
    <summary>Abstract</summary>
    Visual prompt, a pair of before-and-after edited images, can convey indescribable imagery transformations and prosper in image editing. However, current visual prompt methods rely on a pretrained text-guided image-to-image generative model that requires a triplet of text, before, and after images for retraining over a text-to-image model. Such crafting triplets and retraining processes limit the scalability and generalization of editing. In this paper, we present a framework based on any single text-to-image model without reliance on the explicit image-to-image model thus enhancing the generalizability and scalability. Specifically, by leveraging the probability-flow ordinary equation, we construct a diffusion bridge to transfer the distribution between before-and-after images under the text guidance. By optimizing the text via the bridge, the framework adaptively textualizes the editing transformation conveyed by visual prompts into text embeddings without other models. Meanwhile, we introduce differential attention control during text optimization, which disentangles the text embedding from the invariance of the before-and-after images and makes it solely capture the delicate transformation and generalize to edit various images. Experiments on real images validate competitive results on the generalization, contextual coherence, and high fidelity for delicate editing with just one image pair as the visual prompt.
</details>

<details>
    <summary>Key points</summary>
    * Diffusion bridge construction using probability-flow ordinary equation
    * Text optimization for embedding textualization
    * Differential attention control for transformation disentanglement
</details>
</details>

---


<details>
<summary><b> Hands-off Image Editing: Language-guided Editing without any Task-specific Labeling, Masking or even Training</b></summary>

* **Authors:** Rodrigo Santos, António Branco, João Silva, João Rodrigues
* **arXiv ID:** 2502.10064
* **One-liner:** Proposed a novel unsupervised approach for instruction-guided image editing without task-specific supervision.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2502.10064) | [[PDF]](https://arxiv.org/pdf/2502.10064)

> **Core Innovation**
> Achieved competitive performance by eliminating the need for task-specific labeling, masking, or training.

<details>
    <summary>Abstract</summary>
    Instruction-guided image editing consists in taking an image and an instruction and deliverring that image altered according to that instruction. State-of-the-art approaches to this task suffer from the typical scaling up and domain adaptation hindrances related to supervision as they eventually resort to some kind of task-specific labelling, masking or training. We propose a novel approach that does without any such task-specific supervision and offers thus a better potential for improvement. Its assessment demonstrates that it is highly effective, achieving very competitive performance.
</details>

<details>
    <summary>Key points</summary>
    * Unsupervised learning without task-specific supervision
    * Effective handling of instruction-guided edits
    * Competitive performance assessment
</details>
</details>

---


<details>
<summary><b> PromptArtisan: Multi-instruction Image Editing in Single Pass with Complete Attention Control</b></summary>

* **Authors:** Kunal Swami, Raghu Chittersu, Pranav Adlinge, Rajeev Irny, Shashavali Doodekula, Alok Shukla
* **arXiv ID:** 2502.10258
* **One-liner:** Introduced PromptArtisan for multi-instruction image editing in a single pass without iterative refinement.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2502.10258) | [[PDF]](https://arxiv.org/pdf/2502.10258)

> **Core Innovation**
> Integrated a pre-trained InstructPix2Pix model with a Complete Attention Control Mechanism (CACM) for precise, zero-shot editing.

<details>
    <summary>Abstract</summary>
    We present PromptArtisan, a groundbreaking approach to multi-instruction image editing that achieves remarkable results in a single pass, eliminating the need for time-consuming iterative refinement. Our method empowers users to provide multiple editing instructions, each associated with a specific mask within the image. This flexibility allows for complex edits involving mask intersections or overlaps, enabling the realization of intricate and nuanced image transformations. PromptArtisan leverages a pre-trained InstructPix2Pix model in conjunction with a novel Complete Attention Control Mechanism (CACM). This mechanism ensures precise adherence to user instructions, granting fine-grained control over the editing process. Furthermore, our approach is zero-shot, requiring no additional training, and boasts improved processing complexity compared to traditional iterative methods. By seamlessly integrating multi-instruction capabilities, single-pass efficiency, and complete attention control, PromptArtisan unlocks new possibilities for creative and efficient image editing workflows, catering to both novice and expert users alike.
</details>

<details>
    <summary>Key points</summary>
    * Multi-instruction editing with mask associations
    * Complete Attention Control Mechanism (CACM)
    * Zero-shot, single-pass efficiency
</details>
</details>

---


<details>
<summary><b> Instruct-CLIP: Improving Instruction-Guided Image Editing with Automated Data Refinement Using Contrastive Learning</b></summary>

* **Authors:** Sherry X. Chen, Misha Sra, Pradeep Sen
* **arXiv ID:** 2503.18406
* **One-liner:** Presented Instruct-CLIP, a self-supervised method to refine instruction-image alignment in datasets for training latent diffusion models.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2503.18406) | [[PDF]](https://arxiv.org/pdf/2503.18406)

> **Core Innovation**
> Adapted Instruct-CLIP to handle noisy latent images and diffusion timesteps, enabling alignment enforcement in latent space.

<details>
    <summary>Abstract</summary>
    Although natural language instructions offer an intuitive way to guide automated image editing, deep-learning models often struggle to achieve high-quality results, largely due to the difficulty of creating large, high-quality training datasets. To do this, previous approaches have typically relied on text-to-image (T2I) generative models to produce pairs of original and edited images that simulate the input/output of an instruction-guided image-editing model. However, these image pairs often fail to align with the specified edit instructions due to the limitations of T2I models, which negatively impacts models trained on such datasets. To address this, we present Instruct-CLIP (I-CLIP), a selfsupervised method that learns the semantic changes between original and edited images to refine and better align the instructions in existing datasets. Furthermore, we adapt Instruct-CLIP to handle noisy latent images and diffusion timesteps so that it can be used to train latent diffusion models (LDMs) and efficiently enforce alignment between the edit instruction and the image changes in latent space at any step of the diffusion pipeline. We use Instruct-CLIP to correct the InstructPix2Pix dataset and get over 120K refined samples we then use to fine-tune their model, guided by our novel I-CLIP-based loss function. The resulting model can produce edits that are more aligned with the given instructions. Our code and dataset are available at <a href="https://github.com/SherryXTChen/Instruct-CLIP.git" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Self-supervised learning for semantic change refinement
    * Handling noisy latent images and diffusion timesteps
    * I-CLIP-based loss function for model fine-tuning
</details>
</details>

---


<details>
<summary><b> Tuning-Free Image Editing with Fidelity and Editability via Unified Latent Diffusion Model</b></summary>

* **Authors:** Qi Mao, Lan Chen, Yuchao Gu, Mike Zheng Shou, Ming-Hsuan Yang
* **arXiv ID:** 2504.05594
* **One-liner:** Introduced UnifyEdit, a tuning-free method for balancing fidelity and editability in text-based image editing.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2504.05594) | [[PDF]](https://arxiv.org/pdf/2504.05594)

> **Core Innovation**
> Developed self-attention and cross-attention constraints with an adaptive time-step scheduler to prevent gradient conflicts.

<details>
    <summary>Abstract</summary>
    Balancing fidelity and editability is essential in text-based image editing (TIE), where failures commonly lead to over- or under-editing issues. Existing methods typically rely on attention injections for structure preservation and leverage the inherent text alignment capabilities of pre-trained text-to-image (T2I) models for editability, but they lack explicit and unified mechanisms to properly balance these two objectives. In this work, we introduce UnifyEdit, a tuning-free method that performs diffusion latent optimization to enable a balanced integration of fidelity and editability within a unified framework. Unlike direct attention injections, we develop two attention-based constraints: a self-attention (SA) preservation constraint for structural fidelity, and a cross-attention (CA) alignment constraint to enhance text alignment for improved editability. However, simultaneously applying both constraints can lead to gradient conflicts, where the dominance of one constraint results in over- or under-editing. To address this challenge, we introduce an adaptive time-step scheduler that dynamically adjusts the influence of these constraints, guiding the diffusion latent toward an optimal balance. Extensive quantitative and qualitative experiments validate the effectiveness of our approach, demonstrating its superiority in achieving a robust balance between structure preservation and text alignment across various editing tasks, outperforming other state-of-the-art methods. The source code will be available at <a href="https://github.com/CUC-MIPG/UnifyEdit" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Self-attention preservation constraint for fidelity
    * Cross-attention alignment constraint for editability
    * Adaptive time-step scheduler for balance optimization
</details>
</details>

---


<details>
<summary><b> Omni$^2$: Unifying Omnidirectional Image Generation and Editing in an Omni Model</b></summary>

* **Authors:** Liu Yang, Huiyu Duan, Yucheng Zhu, Xiaohong Liu, Lu Liu, Zitong Xu, Guangji Ma, Xiongkuo Min, Guangtao Zhai, Patrick Le Callet
* **arXiv ID:** 2504.11379
* **One-liner:** Constructed Any2Omni, the first comprehensive dataset for omnidirectional image generation and editing, and proposed the Omni² model.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2504.11379) | [[PDF]](https://arxiv.org/pdf/2504.11379)

> **Core Innovation**
> Enabled handling of various ODI tasks under diverse input conditions using a single model.

<details>
    <summary>Abstract</summary>
    $360^{\circ}$ omnidirectional images (ODIs) have gained considerable attention recently, and are widely used in various virtual reality (VR) and augmented reality (AR) applications. However, capturing such images is expensive and requires specialized equipment, making ODI synthesis increasingly important. While common 2D image generation and editing methods are rapidly advancing, these models struggle to deliver satisfactory results when generating or editing ODIs due to the unique format and broad 360$^{\circ}$ Field-of-View (FoV) of ODIs. To bridge this gap, we construct \textbf{\textit{Any2Omni}}, the first comprehensive ODI generation-editing dataset comprises 60,000+ training data covering diverse input conditions and up to 9 ODI generation and editing tasks. Built upon Any2Omni, we propose an \textbf{\underline{Omni}} model for \textbf{\underline{Omni}}-directional image generation and editing (\textbf{\textit{Omni$^2$}}), with the capability of handling various ODI generation and editing tasks under diverse input conditions using one model. Extensive experiments demonstrate the superiority and effectiveness of the proposed Omni$^2$ model for both the ODI generation and editing tasks. Both the Any2Omni dataset and the Omni$^2$ model are publicly available at: <a href="https://github.com/IntMeGroup/Omni2" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Any2Omni dataset with 60,000+ training samples
    * Omni² model for multi-task ODI generation and editing
    * Support for diverse input conditions
</details>
</details>

---


<details>
<summary><b> X-Edit: Detecting and Localizing Edits in Images Altered by Text-Guided Diffusion Models</b></summary>

* **Authors:** Valentina Bazyleva, Nicolo Bonettini, Gaurav Bharaj
* **arXiv ID:** 2505.11753
* **One-liner:** Introduced X-Edit, a method for localizing diffusion-based edits in images to detect deepfake manipulations.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2505.11753) | [[PDF]](https://arxiv.org/pdf/2505.11753)

> **Core Innovation**
> Used inverted diffusion features with a segmentation network and combined segmentation and relevance losses for accurate mask prediction.

<details>
    <summary>Abstract</summary>
    Text-guided diffusion models have significantly advanced image editing, enabling highly realistic and local modifications based on textual prompts. While these developments expand creative possibilities, their malicious use poses substantial challenges for detection of such subtle deepfake edits. To this end, we introduce Explain Edit (X-Edit), a novel method for localizing diffusion-based edits in images. To localize the edits for an image, we invert the image using a pretrained diffusion model, then use these inverted features as input to a segmentation network that explicitly predicts the edited masked regions via channel and spatial attention. Further, we finetune the model using a combined segmentation and relevance loss. The segmentation loss ensures accurate mask prediction by balancing pixel-wise errors and perceptual similarity, while the relevance loss guides the model to focus on low-frequency regions and mitigate high-frequency artifacts, enhancing the localization of subtle edits. To the best of our knowledge, we are the first to address and model the problem of localizing diffusion-based modified regions in images. We additionally contribute a new dataset of paired original and edited images addressing the current lack of resources for this task. Experimental results demonstrate that X-Edit accurately localizes edits in images altered by text-guided diffusion models, outperforming baselines in PSNR and SSIM metrics. This highlights X-Edit&#39;s potential as a robust forensic tool for detecting and pinpointing manipulations introduced by advanced image editing techniques.
</details>

<details>
    <summary>Key points</summary>
    * Image inversion using pretrained diffusion model
    * Segmentation network with channel and spatial attention
    * Combined segmentation and relevance losses for localization
</details>
</details>

---


<details>
<summary><b> Step1X-Edit: A Practical Framework for General Image Editing</b></summary>

* **Authors:** Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, Guopeng Li, Yuang Peng, Quan Sun, Jingwei Wu, Yan Cai, Zheng Ge, Ranchen Ming, Lei Xia, Xianfang Zeng, Yibo Zhu, Binxing Jiao, Xiangyu Zhang, Gang Yu, Daxin Jiang
* **arXiv ID:** 2504.17761
* **One-liner:** Released Step1X-Edit, an open-source image editing model achieving performance comparable to closed-source models like GPT-4o.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2504.17761) | [[PDF]](https://arxiv.org/pdf/2504.17761)

> **Core Innovation**
> Integrated multimodal LLM for instruction processing and diffusion decoder, trained on a high-quality dataset with GEdit-Bench evaluation.

<details>
    <summary>Abstract</summary>
    In recent years, image editing models have witnessed remarkable and rapid development. The recent unveiling of cutting-edge multimodal models such as GPT-4o and Gemini2 Flash has introduced highly promising image editing capabilities. These models demonstrate an impressive aptitude for fulfilling a vast majority of user-driven editing requirements, marking a significant advancement in the field of image manipulation. However, there is still a large gap between the open-source algorithm with these closed-source models. Thus, in this paper, we aim to release a state-of-the-art image editing model, called Step1X-Edit, which can provide comparable performance against the closed-source models like GPT-4o and Gemini2 Flash. More specifically, we adopt the Multimodal LLM to process the reference image and the user&#39;s editing instruction. A latent embedding has been extracted and integrated with a diffusion image decoder to obtain the target image. To train the model, we build a data generation pipeline to produce a high-quality dataset. For evaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world user instructions. Experimental results on GEdit-Bench demonstrate that Step1X-Edit outperforms existing open-source baselines by a substantial margin and approaches the performance of leading proprietary models, thereby making significant contributions to the field of image editing.
</details>

<details>
    <summary>Key points</summary>
    * Multimodal LLM for instruction and image processing
    * High-quality dataset generation pipeline
    * GEdit-Bench for real-world evaluation
</details>
</details>

---


<details>
<summary><b> FLUX.1 Kontext: Flow Matching for In-Context Image Generation and Editing in Latent Space</b></summary>

* **Authors:** Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, Kyle Lacey, Yam Levi, Cheng Li, Dominik Lorenz, Jonas Müller, Dustin Podell, Robin Rombach, Harry Saini, Axel Sauer, Luke Smith
* **arXiv ID:** 2506.15742
* **One-liner:** Evaluated FLUX.1 Kontext, a unified generative flow matching model for image generation and editing with improved consistency.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2506.15742) | [[PDF]](https://arxiv.org/pdf/2506.15742)

> **Core Innovation**
> Achieved competitive performance and faster generation times, validated on KontextBench for multi-turn tasks.

<details>
    <summary>Abstract</summary>
    We present evaluation results for FLUX.1 Kontext, a generative flow matching model that unifies image generation and editing. The model generates novel output views by incorporating semantic context from text and image inputs. Using a simple sequence concatenation approach, FLUX.1 Kontext handles both local editing and generative in-context tasks within a single unified architecture. Compared to current editing models that exhibit degradation in character consistency and stability across multiple turns, we observe that FLUX.1 Kontext improved preservation of objects and characters, leading to greater robustness in iterative workflows. The model achieves competitive performance with current state-of-the-art systems while delivering significantly faster generation times, enabling interactive applications and rapid prototyping workflows. To validate these improvements, we introduce KontextBench, a comprehensive benchmark with 1026 image-prompt pairs covering five task categories: local editing, global editing, character reference, style reference and text editing. Detailed evaluations show the superior performance of FLUX.1 Kontext in terms of both single-turn quality and multi-turn consistency, setting new standards for unified image processing models.
</details>

<details>
    <summary>Key points</summary>
    * Unified architecture for generation and editing
    * Sequence concatenation for context handling
    * KontextBench for comprehensive evaluation
</details>
</details>

---


<details>
<summary><b> Towards Efficient Exemplar Based Image Editing with Multimodal VLMs</b></summary>

* **Authors:** Avadhoot Jadhav, Ashutosh Srivastava, Abhinav Java, Silky Singh, Tarun Ram Menta, Surgan Jandial, Balaji Krishnamurthy
* **arXiv ID:** 2506.20155
* **One-liner:** Enabled exemplar-based image editing using pretrained models without optimization.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2506.20155) | [[PDF]](https://arxiv.org/pdf/2506.20155)

> **Core Innovation**
> Leveraged pretrained text-to-image diffusion models and multimodal VLMs to transfer edits from exemplar pairs to content images.

<details>
    <summary>Abstract</summary>
    Text-to-Image Diffusion models have enabled a wide array of image editing applications. However, capturing all types of edits through text alone can be challenging and cumbersome. The ambiguous nature of certain image edits is better expressed through an exemplar pair, i.e., a pair of images depicting an image before and after an edit respectively. In this work, we tackle exemplar-based image editing -- the task of transferring an edit from an exemplar pair to a content image(s), by leveraging pretrained text-to-image diffusion models and multimodal VLMs. Even though our end-to-end pipeline is optimization-free, our experiments demonstrate that it still outperforms baselines on multiple types of edits while being ~4x faster.
</details>

<details>
    <summary>Key points</summary>
    * Utilized exemplar pairs for ambiguous edits
    * Optimization-free pipeline
    * Multimodal VLM integration
</details>
</details>

---


<details>
<summary><b> Reasoning to Edit: Hypothetical Instruction-Based Image Editing with Visual Reasoning</b></summary>

* **Authors:** Qingdong He, Xueqin Chen, Chaoyi Wang, Yanjie Pan, Xiaobin Hu, Zhenye Gan, Yabiao Wang, Chengjie Wang, Xiangtai Li, Jiangning Zhang
* **arXiv ID:** 2507.01908
* **One-liner:** Introduced a dataset and framework for reasoning-aware image editing.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2507.01908) | [[PDF]](https://arxiv.org/pdf/2507.01908)

> **Core Innovation**
> Proposed Reason50K dataset and ReasonBrain framework to handle implicit hypothetical instructions with fine-grained reasoning.

<details>
    <summary>Abstract</summary>
    Instruction-based image editing (IIE) has advanced rapidly with the success of diffusion models. However, existing efforts primarily focus on simple and explicit instructions to execute editing operations such as adding, deleting, moving, or swapping objects. They struggle to handle more complex implicit hypothetical instructions that require deeper reasoning to infer plausible visual changes and user intent. Additionally, current datasets provide limited support for training and evaluating reasoning-aware editing capabilities. Architecturally, these methods also lack mechanisms for fine-grained detail extraction that support such reasoning. To address these limitations, we propose Reason50K, a large-scale dataset specifically curated for training and evaluating hypothetical instruction reasoning image editing, along with ReasonBrain, a novel framework designed to reason over and execute implicit hypothetical instructions across diverse scenarios. Reason50K includes over 50K samples spanning four key reasoning scenarios: Physical, Temporal, Causal, and Story reasoning. ReasonBrain leverages Multimodal Large Language Models (MLLMs) for editing guidance generation and a diffusion model for image synthesis, incorporating a Fine-grained Reasoning Cue Extraction (FRCE) module to capture detailed visual and textual semantics essential for supporting instruction reasoning. To mitigate the semantic loss, we further introduce a Cross-Modal Enhancer (CME) that enables rich interactions between the fine-grained cues and MLLM-derived features. Extensive experiments demonstrate that ReasonBrain consistently outperforms state-of-the-art baselines on reasoning scenarios while exhibiting strong zero-shot generalization to conventional IIE tasks. Our dataset and code will be released publicly.
</details>

<details>
    <summary>Key points</summary>
    * Reason50K dataset with 50K samples
    * Fine-grained Reasoning Cue Extraction (FRCE)
    * Cross-Modal Enhancer (CME)
</details>
</details>

---


<details>
<summary><b> Beyond Simple Edits: X-Planner for Complex Instruction-Based Image Editing</b></summary>

* **Authors:** Chun-Hsiao Yeh, Yilin Wang, Nanxuan Zhao, Richard Zhang, Yuheng Li, Yi Ma, Krishna Kumar Singh
* **arXiv ID:** 2507.05259
* **One-liner:** Developed an MLLM-based planning system for complex instruction decomposition.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2507.05259) | [[PDF]](https://arxiv.org/pdf/2507.05259)

> **Core Innovation**
> Introduced X-Planner to decompose instructions into sub-instructions with automatic mask generation for identity-preserving edits.

<details>
    <summary>Abstract</summary>
    Recent diffusion-based image editing methods have significantly advanced text-guided tasks but often struggle to interpret complex, indirect instructions. Moreover, current models frequently suffer from poor identity preservation, unintended edits, or rely heavily on manual masks. To address these challenges, we introduce X-Planner, a Multimodal Large Language Model (MLLM)-based planning system that effectively bridges user intent with editing model capabilities. X-Planner employs chain-of-thought reasoning to systematically decompose complex instructions into simpler, clear sub-instructions. For each sub-instruction, X-Planner automatically generates precise edit types and segmentation masks, eliminating manual intervention and ensuring localized, identity-preserving edits. Additionally, we propose a novel automated pipeline for generating large-scale data to train X-Planner which achieves state-of-the-art results on both existing benchmarks and our newly introduced complex editing benchmark.
</details>

<details>
    <summary>Key points</summary>
    * Chain-of-thought reasoning
    * Automatic mask generation
    * Automated data generation pipeline
</details>
</details>

---


<details>
<summary><b> NoHumansRequired: Autonomous High-Quality Image Editing Triplet Mining</b></summary>

* **Authors:** Maksim Kuprashevich, Grigorii Alekseenko, Irina Tolstykh, Georgii Fedorov, Bulat Suleimanov, Vladimir Dokholyan, Aleksandr Gordeev
* **arXiv ID:** 2507.14119
* **One-liner:** Automated the creation of high-quality training data for image editing.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2507.14119) | [[PDF]](https://arxiv.org/pdf/2507.14119)

> **Core Innovation**
> Presented a pipeline to mine and validate triplets using generative models and a Gemini validator, releasing NHR-Edit dataset.

<details>
    <summary>Abstract</summary>
    Recent advances in generative modeling enable image editing assistants that follow natural language instructions without additional user input. Their supervised training requires millions of triplets (original image, instruction, edited image), yet mining pixel-accurate examples is hard. Each edit must affect only prompt-specified regions, preserve stylistic coherence, respect physical plausibility, and retain visual appeal. The lack of robust automated edit-quality metrics hinders reliable automation at scale. We present an automated, modular pipeline that mines high-fidelity triplets across domains, resolutions, instruction complexities, and styles. Built on public generative models and running without human intervention, our system uses a task-tuned Gemini validator to score instruction adherence and aesthetics directly, removing any need for segmentation or grounding models. Inversion and compositional bootstrapping enlarge the mined set by approx. 2.6x, enabling large-scale high-fidelity training data. By automating the most repetitive annotation steps, the approach allows a new scale of training without human labeling effort. To democratize research in this resource-intensive area, we release NHR-Edit, an open dataset of 720k high-quality triplets, curated at industrial scale via millions of guided generations and validator passes, and we analyze the pipeline&#39;s stage-wise survival rates, providing a framework for estimating computational effort across different model stacks. In the largest cross-dataset evaluation, it surpasses all public alternatives. We also release Bagel-NHR-Edit, a fine-tuned Bagel model with state-of-the-art metrics.
</details>

<details>
    <summary>Key points</summary>
    * Automated triplet mining
    * Gemini validator for scoring
    * Inversion and bootstrapping for data expansion
</details>
</details>

---


<details>
<summary><b> Qwen-Image Technical Report</b></summary>

* **Authors:** Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, Deqing Li, Hang Zhang, Hao Meng, Hu Wei, Jingyuan Ni, Kai Chen, Kuan Cao, Liang Peng, Lin Qu, Minggang Wu, Peng Wang, Shuting Yu, Tingkun Wen, Wensen Feng, Xiaoxiao Xu, Yi Wang, Yichang Zhang, Yongqiang Zhu, Yujia Wu, Yuxuan Cai, Zenan Liu
* **arXiv ID:** 2508.02324
* **One-liner:** Advanced text rendering and editing consistency in image generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2508.02324) | [[PDF]](https://arxiv.org/pdf/2508.02324)

> **Core Innovation**
> Improved Qwen-Image with progressive training and dual-encoding for better text rendering and editing fidelity.

<details>
    <summary>Abstract</summary>
    We present Qwen-Image, an image generation foundation model in the Qwen series that achieves significant advances in complex text rendering and precise image editing. To address the challenges of complex text rendering, we design a comprehensive data pipeline that includes large-scale data collection, filtering, annotation, synthesis, and balancing. Moreover, we adopt a progressive training strategy that starts with non-text-to-text rendering, evolves from simple to complex textual inputs, and gradually scales up to paragraph-level descriptions. This curriculum learning approach substantially enhances the model&#39;s native text rendering capabilities. As a result, Qwen-Image not only performs exceptionally well in alphabetic languages such as English, but also achieves remarkable progress on more challenging logographic languages like Chinese. To enhance image editing consistency, we introduce an improved multi-task training paradigm that incorporates not only traditional text-to-image (T2I) and text-image-to-image (TI2I) tasks but also image-to-image (I2I) reconstruction, effectively aligning the latent representations between Qwen2.5-VL and MMDiT. Furthermore, we separately feed the original image into Qwen2.5-VL and the VAE encoder to obtain semantic and reconstructive representations, respectively. This dual-encoding mechanism enables the editing module to strike a balance between preserving semantic consistency and maintaining visual fidelity. Qwen-Image achieves state-of-the-art performance, demonstrating its strong capabilities in both image generation and editing across multiple benchmarks.
</details>

<details>
    <summary>Key points</summary>
    * Progressive training strategy
    * Dual-encoding mechanism
    * Multi-task training paradigm
</details>
</details>

---


<details>
<summary><b> Talk2Image: A Multi-Agent System for Multi-Turn Image Generation and Editing</b></summary>

* **Authors:** Shichao Ma, Yunhe Guo, Jiahao Su, Qihe Huang, Zhengyang Zhou, Yang Wang
* **arXiv ID:** 2508.06916
* **One-liner:** Enabled interactive multi-turn image editing with a multi-agent system.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2508.06916) | [[PDF]](https://arxiv.org/pdf/2508.06916)

> **Core Innovation**
> Introduced Talk2Image for dialogue-based editing with intention parsing and collaborative agents.

<details>
    <summary>Abstract</summary>
    Text-to-image generation tasks have driven remarkable advances in diverse media applications, yet most focus on single-turn scenarios and struggle with iterative, multi-turn creative tasks. Recent dialogue-based systems attempt to bridge this gap, but their single-agent, sequential paradigm often causes intention drift and incoherent edits. To address these limitations, we present Talk2Image, a novel multi-agent system for interactive image generation and editing in multi-turn dialogue scenarios. Our approach integrates three key components: intention parsing from dialogue history, task decomposition and collaborative execution across specialized agents, and feedback-driven refinement based on a multi-view evaluation mechanism. Talk2Image enables step-by-step alignment with user intention and consistent image editing. Experiments demonstrate that Talk2Image outperforms existing baselines in controllability, coherence, and user satisfaction across iterative image generation and editing tasks.
</details>

<details>
    <summary>Key points</summary>
    * Multi-agent system
    * Intention parsing from dialogue
    * Feedback-driven refinement
</details>
</details>

---


<details>
<summary><b> CannyEdit: Selective Canny Control and Dual-Prompt Guidance for Training-Free Image Editing</b></summary>

* **Authors:** Weiyan Xie, Han Gao, Didan Deng, Kaican Li, April Hua Liu, Yongxiang Huang, Nevin L. Zhang
* **arXiv ID:** 2508.06937
* **One-liner:** Achieved balanced local editing with training-free structural guidance.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2508.06937) | [[PDF]](https://arxiv.org/pdf/2508.06937)

> **Core Innovation**
> Proposed CannyEdit using Selective Canny Control and Dual-Prompt Guidance for seamless edits.

<details>
    <summary>Abstract</summary>
    Recent advances in text-to-image (T2I) models have enabled training-free regional image editing by leveraging the generative priors of foundation models. However, existing methods struggle to balance text adherence in edited regions, context fidelity in unedited areas, and seamless integration of edits. We introduce CannyEdit, a novel training-free framework that addresses this trilemma through two key innovations. First, Selective Canny Control applies structural guidance from a Canny ControlNet only to the unedited regions, preserving the original image&#39;s details while allowing for precise, text-driven changes in the specified editable area. Second, Dual-Prompt Guidance utilizes both a local prompt for the specific edit and a global prompt for overall scene coherence. Through this synergistic approach, these components enable controllable local editing for object addition, replacement, and removal, achieving a superior trade-off among text adherence, context fidelity, and editing seamlessness compared to current region-based methods. Beyond this, CannyEdit offers exceptional flexibility: it operates effectively with rough masks or even single-point hints in addition tasks. Furthermore, the framework can seamlessly integrate with vision-language models in a training-free manner for complex instruction-based editing that requires planning and reasoning. Our extensive evaluations demonstrate CannyEdit&#39;s strong performance against leading instruction-based editors in complex object addition scenarios.
</details>

<details>
    <summary>Key points</summary>
    * Selective Canny Control
    * Dual-Prompt Guidance
    * Training-free framework
</details>
</details>

---


<details>
<summary><b> Exploring Multimodal Diffusion Transformers for Enhanced Prompt-based Image Editing</b></summary>

* **Authors:** Joonghyuk Shin, Alchan Hwang, Yujin Kim, Daneul Kim, Jaesik Park
* **arXiv ID:** 2508.07519
* **One-liner:** Adapted editing methods for MM-DiT architectures with bidirectional attention.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2508.07519) | [[PDF]](https://arxiv.org/pdf/2508.07519)

> **Core Innovation**
> Analyzed MM-DiT attention and proposed a prompt-based editing method for global to local edits.

<details>
    <summary>Abstract</summary>
    Transformer-based diffusion models have recently superseded traditional U-Net architectures, with multimodal diffusion transformers (MM-DiT) emerging as the dominant approach in state-of-the-art models like Stable Diffusion 3 and Flux.1. Previous approaches have relied on unidirectional cross-attention mechanisms, with information flowing from text embeddings to image latents. In contrast, MMDiT introduces a unified attention mechanism that concatenates input projections from both modalities and performs a single full attention operation, allowing bidirectional information flow between text and image branches. This architectural shift presents significant challenges for existing editing techniques. In this paper, we systematically analyze MM-DiT&#39;s attention mechanism by decomposing attention matrices into four distinct blocks, revealing their inherent characteristics. Through these analyses, we propose a robust, prompt-based image editing method for MM-DiT that supports global to local edits across various MM-DiT variants, including few-step models. We believe our findings bridge the gap between existing U-Net-based methods and emerging architectures, offering deeper insights into MMDiT&#39;s behavioral patterns.
</details>

<details>
    <summary>Key points</summary>
    * Attention matrix decomposition
    * Bidirectional information flow
    * Compatibility with MM-DiT variants
</details>
</details>

---


<details>
<summary><b> X2Edit: Revisiting Arbitrary-Instruction Image Editing through Self-Constructed Data and Task-Aware Representation Learning</b></summary>

* **Authors:** Jian Ma, Xujie Zhu, Zihao Pan, Qirong Peng, Xu Guo, Chen Chen, Haonan Lu
* **arXiv ID:** 2508.07607
* **One-liner:** Created a large-scale dataset and efficient model for diverse editing tasks.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2508.07607) | [[PDF]](https://arxiv.org/pdf/2508.07607)

> **Core Innovation**
> Introduced X2Edit dataset and task-aware MoE-LoRA training with contrastive learning.

<details>
    <summary>Abstract</summary>
    Existing open-source datasets for arbitrary-instruction image editing remain suboptimal, while a plug-and-play editing module compatible with community-prevalent generative models is notably absent. In this paper, we first introduce the X2Edit Dataset, a comprehensive dataset covering 14 diverse editing tasks, including subject-driven generation. We utilize the industry-leading unified image generation models and expert models to construct the data. Meanwhile, we design reasonable editing instructions with the VLM and implement various scoring mechanisms to filter the data. As a result, we construct 3.7 million high-quality data with balanced categories. Second, to better integrate seamlessly with community image generation models, we design task-aware MoE-LoRA training based on FLUX.1, with only 8\% of the parameters of the full model. To further improve the final performance, we utilize the internal representations of the diffusion model and define positive/negative samples based on image editing types to introduce contrastive learning. Extensive experiments demonstrate that the model&#39;s editing performance is competitive among many excellent models. Additionally, the constructed dataset exhibits substantial advantages over existing open-source datasets. The open-source code, checkpoints, and datasets for X2Edit can be found at the following link: <a href="https://github.com/OPPO-Mente-Lab/X2Edit" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * X2Edit dataset with 3.7M samples
    * MoE-LoRA training
    * Contrastive learning integration
</details>
</details>

---


<details>
<summary><b> An LLM-LVLM Driven Agent for Iterative and Fine-Grained Image Editing</b></summary>

* **Authors:** Zihan Liang, Jiahao Sun, Haoran Ma
* **arXiv ID:** 2508.17435
* **One-liner:** Developed a training-free agent for iterative and context-aware editing.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2508.17435) | [[PDF]](https://arxiv.org/pdf/2508.17435)

> **Core Innovation**
> Introduced RefineEdit-Agent with LLM and LVLM integration for complex editing and feedback loops.

<details>
    <summary>Abstract</summary>
    Despite the remarkable capabilities of text-to-image (T2I) generation models, real-world applications often demand fine-grained, iterative image editing that existing methods struggle to provide. Key challenges include granular instruction understanding, robust context preservation during modifications, and the lack of intelligent feedback mechanisms for iterative refinement. This paper introduces RefineEdit-Agent, a novel, training-free intelligent agent framework designed to address these limitations by enabling complex, iterative, and context-aware image editing. RefineEdit-Agent leverages the powerful planning capabilities of Large Language Models (LLMs) and the advanced visual understanding and evaluation prowess of Vision-Language Large Models (LVLMs) within a closed-loop system. Our framework comprises an LVLM-driven instruction parser and scene understanding module, a multi-level LLM-driven editing planner for goal decomposition, tool selection, and sequence generation, an iterative image editing module, and a crucial LVLM-driven feedback and evaluation loop. To rigorously evaluate RefineEdit-Agent, we propose LongBench-T2I-Edit, a new benchmark featuring 500 initial images with complex, multi-turn editing instructions across nine visual dimensions. Extensive experiments demonstrate that RefineEdit-Agent significantly outperforms state-of-the-art baselines, achieving an average score of 3.67 on LongBench-T2I-Edit, compared to 2.29 for Direct Re-Prompting, 2.91 for InstructPix2Pix, 3.16 for GLIGEN-based Edit, and 3.39 for ControlNet-XL. Ablation studies, human evaluations, and analyses of iterative refinement, backbone choices, tool usage, and robustness to instruction complexity further validate the efficacy of our agentic design in delivering superior edit fidelity and context preservation.
</details>

<details>
    <summary>Key points</summary>
    * LVLM-driven instruction parsing
    * LLM-driven planning
    * Iterative feedback and evaluation
</details>
</details>

---


<details>
<summary><b> Describe, Don&#39;t Dictate: Semantic Image Editing with Natural Language Intent</b></summary>

* **Authors:** En Ci, Shanyan Guan, Yanhao Ge, Yilin Zhang, Wei Li, Zhenyu Zhang, Jian Yang, Ying Tai
* **arXiv ID:** 2508.20505
* **One-liner:** Proposed a descriptive-prompt-based editing framework to improve semantic image editing.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2508.20505) | [[PDF]](https://arxiv.org/pdf/2508.20505)

> **Core Innovation**
> Reframed instruction-based image editing as reference-image-based text-to-image generation to leverage well-trained models without modifications.

<details>
    <summary>Abstract</summary>
    Despite the progress in text-to-image generation, semantic image editing remains a challenge. Inversion-based algorithms unavoidably introduce reconstruction errors, while instruction-based models mainly suffer from limited dataset quality and scale. To address these problems, we propose a descriptive-prompt-based editing framework, named DescriptiveEdit. The core idea is to re-frame `instruction-based image editing&#39; as `reference-image-based text-to-image generation&#39;, which preserves the generative power of well-trained Text-to-Image models without architectural modifications or inversion. Specifically, taking the reference image and a prompt as input, we introduce a Cross-Attentive UNet, which newly adds attention bridges to inject reference image features into the prompt-to-edit-image generation process. Owing to its text-to-image nature, DescriptiveEdit overcomes limitations in instruction dataset quality, integrates seamlessly with ControlNet, IP-Adapter, and other extensions, and is more scalable. Experiments on the Emu Edit benchmark show it improves editing accuracy and consistency.
</details>

<details>
    <summary>Key points</summary>
    * Introduced Cross-Attentive UNet with attention bridges
    * Injected reference image features into generation process
    * Seamlessly integrated with ControlNet and IP-Adapter
</details>
</details>

---


<details>
<summary><b> Draw-In-Mind: Rebalancing Designer-Painter Roles in Unified Multimodal Models Benefits Image Editing</b></summary>

* **Authors:** Ziyun Zeng, Junhao Zhang, Wei Li, Mike Zheng Shou
* **arXiv ID:** 2509.01986
* **One-liner:** Introduced a unified model with balanced responsibilities for precise image editing.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2509.01986) | [[PDF]](https://arxiv.org/pdf/2509.01986)

> **Core Innovation**
> Addressed imbalance by assigning design responsibility to the understanding module using a dataset with chain-of-thought imaginations.

<details>
    <summary>Abstract</summary>
    In recent years, integrating multimodal understanding and generation into a single unified model has emerged as a promising paradigm. While this approach achieves strong results in text-to-image (T2I) generation, it still struggles with precise image editing. We attribute this limitation to an imbalanced division of responsibilities. The understanding module primarily functions as a translator that encodes user instructions into semantic conditions, while the generation module must simultaneously act as designer and painter, inferring the original layout, identifying the target editing region, and rendering the new content. This imbalance is counterintuitive because the understanding module is typically trained with several times more data on complex reasoning tasks than the generation module. To address this issue, we introduce Draw-In-Mind (DIM), a dataset comprising two complementary subsets: (i) DIM-T2I, containing 14M long-context image-text pairs to enhance complex instruction comprehension; and (ii) DIM-Edit, consisting of 233K chain-of-thought imaginations generated by GPT-4o, serving as explicit design blueprints for image edits. We connect a frozen Qwen2.5-VL-3B with a trainable SANA1.5-1.6B via a lightweight two-layer MLP, and train it on the proposed DIM dataset, resulting in DIM-4.6B-T2I/Edit. Despite its modest parameter scale, DIM-4.6B-Edit achieves SOTA or competitive performance on the ImgEdit and GEdit-Bench benchmarks, outperforming much larger models such as UniWorld-V1 and Step1X-Edit. These findings demonstrate that explicitly assigning the design responsibility to the understanding module provides significant benefits for image editing. Our dataset and models are available at <a href="https://github.com/showlab/DIM" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Created DIM dataset with DIM-T2I and DIM-Edit subsets
    * Connected frozen Qwen2.5-VL-3B with trainable SANA1.5-1.6B via MLP
    * Achieved SOTA performance on benchmarks
</details>
</details>

---


<details>
<summary><b> MultiEdit: Advancing Instruction-based Image Editing on Diverse and Challenging Tasks</b></summary>

* **Authors:** Mingsong Li, Lin Liu, Hongjun Wang, Haoxing Chen, Xijun Gu, Shizhan Liu, Dong Gong, Junbo Zhao, Zhenzhong Lan, Jianguo Li
* **arXiv ID:** 2509.14638
* **One-liner:** Developed a comprehensive dataset to enhance instruction-based image editing.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2509.14638) | [[PDF]](https://arxiv.org/pdf/2509.14638)

> **Core Innovation**
> Constructed MultiEdit dataset with diverse editing tasks and types to overcome dataset limitations.

<details>
    <summary>Abstract</summary>
    Current instruction-based image editing (IBIE) methods struggle with challenging editing tasks, as both editing types and sample counts of existing datasets are limited. Moreover, traditional dataset construction often contains noisy image-caption pairs, which may introduce biases and limit model capabilities in complex editing scenarios. To address these limitations, we introduce MultiEdit, a comprehensive dataset featuring over 107K high-quality image editing samples. It encompasses 6 challenging editing tasks through a diverse collection of 18 non-style-transfer editing types and 38 style transfer operations, covering a spectrum from sophisticated style transfer to complex semantic operations like person reference editing and in-image text editing. We employ a novel dataset construction pipeline that utilizes two multi-modal large language models (MLLMs) to generate visual-adaptive editing instructions and produce high-fidelity edited images, respectively. Extensive experiments demonstrate that fine-tuning foundational open-source models with our MultiEdit-Train set substantially improves models&#39; performance on sophisticated editing tasks in our proposed MultiEdit-Test benchmark, while effectively preserving their capabilities on the standard editing benchmark. We believe MultiEdit provides a valuable resource for advancing research into more diverse and challenging IBIE capabilities. Our dataset is available at <a href="https://huggingface.co/datasets/inclusionAI/MultiEdit" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Employed MLLMs for generating instructions and edited images
    * Covered 18 non-style-transfer and 38 style transfer operations
    * Improved model performance on sophisticated editing tasks
</details>
</details>

---


<details>
<summary><b> AutoEdit: Automatic Hyperparameter Tuning for Image Editing</b></summary>

* **Authors:** Chau Pham, Quan Dao, Mahesh Bhosale, Yunjie Tian, Dimitris Metaxas, David Doermann
* **arXiv ID:** 2509.15031
* **One-liner:** Proposed a reinforcement learning framework for efficient hyperparameter tuning in diffusion-based editing.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2509.15031) | [[PDF]](https://arxiv.org/pdf/2509.15031)

> **Core Innovation**
> Modeled hyperparameter search as a sequential decision-making task to reduce computational costs.

<details>
    <summary>Abstract</summary>
    Recent advances in diffusion models have revolutionized text-guided image editing, yet existing editing methods face critical challenges in hyperparameter identification. To get the reasonable editing performance, these methods often require the user to brute-force tune multiple interdependent hyperparameters, such as inversion timesteps and attention modification. This process incurs high computational costs due to the huge hyperparameter search space. We consider searching optimal editing&#39;s hyperparameters as a sequential decision-making task within the diffusion denoising process. Specifically, we propose a reinforcement learning framework, which establishes a Markov Decision Process that dynamically adjusts hyperparameters across denoising steps, integrating editing objectives into a reward function. The method achieves time efficiency through proximal policy optimization while maintaining optimal hyperparameter configurations. Experiments demonstrate significant reduction in search time and computational overhead compared to existing brute-force approaches, advancing the practical deployment of a diffusion-based image editing framework in the real world. Codes can be found at <a href="https://github.com/chaupham1709/AutoEdit.git" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Established Markov Decision Process for dynamic hyperparameter adjustment
    * Used proximal policy optimization for time efficiency
    * Integrated editing objectives into reward function
</details>
</details>

---


<details>
<summary><b> CAMILA: Context-Aware Masking for Image Editing with Language Alignment</b></summary>

* **Authors:** Hyunseung Kim, Chiho Choi, Srikanth Malla, Sai Prahladh Padmanabhan, Saurabh Bagchi, Joon Hee Choi
* **arXiv ID:** 2509.19731
* **One-liner:** Introduced a context-aware method to handle infeasible instructions in image editing.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2509.19731) | [[PDF]](https://arxiv.org/pdf/2509.19731)

> **Core Innovation**
> Designed CAMILA to validate instruction coherence and apply only relevant edits.

<details>
    <summary>Abstract</summary>
    Text-guided image editing has been allowing users to transform and synthesize images through natural language instructions, offering considerable flexibility. However, most existing image editing models naively attempt to follow all user instructions, even if those instructions are inherently infeasible or contradictory, often resulting in nonsensical output. To address these challenges, we propose a context-aware method for image editing named as CAMILA (Context-Aware Masking for Image Editing with Language Alignment). CAMILA is designed to validate the contextual coherence between instructions and the image, ensuring that only relevant edits are applied to the designated regions while ignoring non-executable instructions. For comprehensive evaluation of this new method, we constructed datasets for both single- and multi-instruction image editing, incorporating the presence of infeasible requests. Our method achieves better performance and higher semantic alignment than state-of-the-art models, demonstrating its effectiveness in handling complex instruction challenges while preserving image integrity.
</details>

<details>
    <summary>Key points</summary>
    * Constructed datasets for single- and multi-instruction editing
    * Ensured contextual coherence between instructions and images
    * Achieved better semantic alignment than SOTA models
</details>
</details>

---


<details>
<summary><b> EditVerse: Unifying Image and Video Editing and Generation with In-Context Learning</b></summary>

* **Authors:** Xuan Ju, Tianyu Wang, Yuqian Zhou, He Zhang, Qing Liu, Nanxuan Zhao, Zhifei Zhang, Yijun Li, Yuanhao Cai, Shaoteng Liu, Daniil Pakhomov, Zhe Lin, Soo Ye Kim, Qiang Xu
* **arXiv ID:** 2509.20360
* **One-liner:** Created a unified framework for image and video generation and editing.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2509.20360) | [[PDF]](https://arxiv.org/pdf/2509.20360)

> **Core Innovation**
> Represented all modalities as unified token sequences to enable cross-modal learning.

<details>
    <summary>Abstract</summary>
    Recent advances in foundation models highlight a clear trend toward unification and scaling, showing emergent capabilities across diverse domains. While image generation and editing have rapidly transitioned from task-specific to unified frameworks, video generation and editing remain fragmented due to architectural limitations and data scarcity. In this work, we introduce EditVerse, a unified framework for image and video generation and editing within a single model. By representing all modalities, i.e., text, image, and video, as a unified token sequence, EditVerse leverages self-attention to achieve robust in-context learning, natural cross-modal knowledge transfer, and flexible handling of inputs and outputs with arbitrary resolutions and durations. To address the lack of video editing training data, we design a scalable data pipeline that curates 232K video editing samples and combines them with large-scale image and video datasets for joint training. Furthermore, we present EditVerseBench, the first benchmark for instruction-based video editing covering diverse tasks and resolutions. Extensive experiments and user studies demonstrate that EditVerse achieves state-of-the-art performance, surpassing existing open-source and commercial models, while exhibiting emergent editing and generation abilities across modalities.
</details>

<details>
    <summary>Key points</summary>
    * Designed scalable data pipeline with 232K video editing samples
    * Leveraged self-attention for in-context learning
    * Introduced EditVerseBench benchmark
</details>
</details>

---


<details>
<summary><b> EditScore: Unlocking Online RL for Image Editing via High-Fidelity Reward Modeling</b></summary>

* **Authors:** Xin Luo, Jiahao Wang, Chenyuan Wu, Shitao Xiao, Xiyan Jiang, Defu Lian, Jiajun Zhang, Dong Liu, Zheng liu
* **arXiv ID:** 2509.23909
* **One-liner:** Developed a high-fidelity reward model to enable reinforcement learning in image editing.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2509.23909) | [[PDF]](https://arxiv.org/pdf/2509.23909)

> **Core Innovation**
> Introduced EditScore reward models and benchmark to overcome RL adoption barriers.

<details>
    <summary>Abstract</summary>
    Instruction-guided image editing has achieved remarkable progress, yet current models still face challenges with complex instructions and often require multiple samples to produce a desired result. Reinforcement Learning (RL) offers a promising solution, but its adoption in image editing has been severely hindered by the lack of a high-fidelity, efficient reward signal. In this work, we present a comprehensive methodology to overcome this barrier, centered on the development of a state-of-the-art, specialized reward model. We first introduce EditReward-Bench, a comprehensive benchmark to systematically evaluate reward models on editing quality. Building on this benchmark, we develop EditScore, a series of reward models (7B-72B) for evaluating the quality of instruction-guided image editing. Through meticulous data curation and filtering, EditScore effectively matches the performance of learning proprietary VLMs. Furthermore, coupled with an effective self-ensemble strategy tailored for the generative nature of EditScore, our largest variant even surpasses GPT-5 in the benchmark. We then demonstrate that a high-fidelity reward model is the key to unlocking online RL for image editing. Our experiments show that, while even the largest open-source VLMs fail to provide an effective learning signal, EditScore enables efficient and robust policy optimization. Applying our framework to a strong base model, OmniGen2, results in a final model that shows a substantial and consistent performance uplift. Overall, this work provides the first systematic path from benchmarking to reward modeling to RL training in image editing, showing that a high-fidelity, domain-specialized reward model is the key to unlocking the full potential of RL in this domain.
</details>

<details>
    <summary>Key points</summary>
    * Created EditReward-Bench for systematic evaluation
    * Used self-ensemble strategy for improved performance
    * Enabled efficient policy optimization with EditScore
</details>
</details>

---


<details>
<summary><b> EditReward: A Human-Aligned Reward Model for Instruction-Guided Image Editing</b></summary>

* **Authors:** Keming Wu, Sicong Jiang, Max Ku, Ping Nie, Minghao Liu, Wenhu Chen
* **arXiv ID:** 2509.26346
* **One-liner:** Built a reward model to scale high-quality synthetic training data for image editing.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2509.26346) | [[PDF]](https://arxiv.org/pdf/2509.26346)

> **Core Innovation**
> Trained \mname on large-scale human preference dataset to align with human preferences.

<details>
    <summary>Abstract</summary>
    Recently, we have witnessed great progress in image editing with natural language instructions. Several closed-source models like GPT-Image-1, Seedream, and Google-Nano-Banana have shown highly promising progress. However, the open-source models are still lagging. The main bottleneck is the lack of a reliable reward model to scale up high-quality synthetic training data. To address this critical bottleneck, we built \mname, trained with our new large-scale human preference dataset, meticulously annotated by trained experts following a rigorous protocol containing over 200K preference pairs. \mname demonstrates superior alignment with human preferences in instruction-guided image editing tasks. Experiments show that \mname achieves state-of-the-art human correlation on established benchmarks such as GenAI-Bench, AURORA-Bench, ImagenHub, and our new \benchname, outperforming a wide range of VLM-as-judge models. Furthermore, we use \mname to select a high-quality subset from the existing noisy ShareGPT-4o-Image dataset. We train Step1X-Edit on the selected subset, which shows significant improvement over training on the full set. This demonstrates \mname&#39;s ability to serve as a reward model to scale up high-quality training data for image editing. Furthermore, its strong alignment suggests potential for advanced applications like reinforcement learning-based post-training and test-time scaling of image editing models. \mname with its training dataset will be released to help the community build more high-quality image editing training datasets.
</details>

<details>
    <summary>Key points</summary>
    * Annotated over 200K preference pairs with experts
    * Used \mname to select high-quality data subsets
    * Achieved SOTA human correlation on benchmarks
</details>
</details>

---


<details>
<summary><b> Query-Kontext: An Unified Multimodal Model for Image Generation and Editing</b></summary>

* **Authors:** Yuxin Song, Wenkai Dong, Shizun Wang, Qi Zhang, Song Xue, Tao Yuan, Hu Yang, Haocheng Feng, Hang Zhou, Xinyan Xiao, Jingdong Wang
* **arXiv ID:** 2509.26641
* **One-liner:** Introduced a novel approach to disentangle generative reasoning from synthesis in unified models.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2509.26641) | [[PDF]](https://arxiv.org/pdf/2509.26641)

> **Core Innovation**
> Bridged VLM and diffusion model via multimodal kontext for improved editing and generation.

<details>
    <summary>Abstract</summary>
    Unified Multimodal Models (UMMs) have demonstrated remarkable performance in text-to-image generation (T2I) and editing (TI2I), whether instantiated as assembled unified frameworks which couple powerful vision-language model (VLM) with diffusion-based generator, or as naive Unified Multimodal Models with an early fusion of understanding and generation modalities. We contend that in current unified frameworks, the crucial capability of multimodal generative reasoning which encompasses instruction understanding, grounding, and image referring for identity preservation and faithful reconstruction, is intrinsically entangled with high-fidelity synthesis. In this work, we introduce Query-Kontext, a novel approach that bridges the VLM and diffusion model via a multimodal ``kontext&#39;&#39; composed of semantic cues and coarse-grained image conditions encoded from multimodal inputs. This design delegates the complex ability of multimodal generative reasoning to powerful VLM while reserving diffusion model&#39;s role for high-quality visual synthesis. To achieve this, we propose a three-stage progressive training strategy. First, we connect the VLM to a lightweight diffusion head via multimodal kontext tokens to unleash the VLM&#39;s generative reasoning ability. Second, we scale this head to a large, pre-trained diffusion model to enhance visual detail and realism. Finally, we introduce a low-level image encoder to improve image fidelity and perform instruction tuning on downstream tasks. Furthermore, we build a comprehensive data pipeline integrating real, synthetic, and open-source datasets, covering diverse multimodal reference-to-image scenarios, including image generation, instruction-driven editing, customized generation, and multi-subject composition. Experiments show that our approach matches strong unified baselines and even outperforms task-specific state-of-the-art methods in several cases.
</details>

<details>
    <summary>Key points</summary>
    * Proposed three-stage progressive training strategy
    * Used multimodal kontext tokens for semantic cues
    * Built comprehensive data pipeline for diverse scenarios
</details>
</details>

---


<details>
<summary><b> GODIVA: Generating Open-DomaIn Videos from nAtural Descriptions</b></summary>

* **Authors:** Chenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji, Fan Yang, Guillermo Sapiro, Nan Duan
* **arXiv ID:** 2104.14806
* **One-liner:** Proposed an open-domain text-to-video pretrained model with good generalization.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2104.14806) | [[PDF]](https://arxiv.org/pdf/2104.14806)

> **Core Innovation**
> Pretrained GODIVA on large-scale dataset for auto-regressive video generation.

<details>
    <summary>Abstract</summary>
    Generating videos from text is a challenging task due to its high computational requirements for training and infinite possible answers for evaluation. Existing works typically experiment on simple or small datasets, where the generalization ability is quite limited. In this work, we propose GODIVA, an open-domain text-to-video pretrained model that can generate videos from text in an auto-regressive manner using a three-dimensional sparse attention mechanism. We pretrain our model on Howto100M, a large-scale text-video dataset that contains more than 136 million text-video pairs. Experiments show that GODIVA not only can be fine-tuned on downstream video generation tasks, but also has a good zero-shot capability on unseen texts. We also propose a new metric called Relative Matching (RM) to automatically evaluate the video generation quality. Several challenges are listed and discussed as future work.
</details>

<details>
    <summary>Key points</summary>
    * Used three-dimensional sparse attention mechanism
    * Trained on Howto100M with 136M text-video pairs
    * Introduced Relative Matching metric for evaluation
</details>
</details>

---


<details>
<summary><b> NÜWA: Visual Synthesis Pre-training for Neural visUal World creAtion</b></summary>

* **Authors:** Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, Nan Duan
* **arXiv ID:** 2111.12417
* **One-liner:** Introduced NÜWA, a unified multimodal pre-trained model for generating and manipulating visual data.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2111.12417) | [[PDF]](https://arxiv.org/pdf/2111.12417)

> **Core Innovation**
> Designed a 3D transformer encoder-decoder framework with 3DNA mechanism to handle text, image, and video data efficiently.

<details>
    <summary>Abstract</summary>
    This paper presents a unified multimodal pre-trained model called NÜWA that can generate new or manipulate existing visual data (i.e., images and videos) for various visual synthesis tasks. To cover language, image, and video at the same time for different scenarios, a 3D transformer encoder-decoder framework is designed, which can not only deal with videos as 3D data but also adapt to texts and images as 1D and 2D data, respectively. A 3D Nearby Attention (3DNA) mechanism is also proposed to consider the nature of the visual data and reduce the computational complexity. We evaluate NÜWA on 8 downstream tasks. Compared to several strong baselines, NÜWA achieves state-of-the-art results on text-to-image generation, text-to-video generation, video prediction, etc. Furthermore, it also shows surprisingly good zero-shot capabilities on text-guided image and video manipulation tasks. Project repo is <a href="https://github.com/microsoft/NUWA" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * 3D transformer encoder-decoder framework
    * 3D Nearby Attention (3DNA) mechanism
    * State-of-the-art results on multiple tasks
    * Zero-shot capabilities on manipulation tasks
</details>
</details>

---


<details>
<summary><b> VideoGen: A Reference-Guided Latent Diffusion Approach for High Definition Text-to-Video Generation</b></summary>

* **Authors:** Xin Li, Wenqing Chu, Ye Wu, Weihang Yuan, Fanglong Liu, Qi Zhang, Fu Li, Haocheng Feng, Errui Ding, Jingdong Wang
* **arXiv ID:** 2309.00398
* **One-liner:** Developed VideoGen, a text-to-video generation method using reference-guided latent diffusion.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2309.00398) | [[PDF]](https://arxiv.org/pdf/2309.00398)

> **Core Innovation**
> Leveraged a text-to-image model for reference images and cascaded latent diffusion for high-fidelity video generation.

<details>
    <summary>Abstract</summary>
    In this paper, we present VideoGen, a text-to-video generation approach, which can generate a high-definition video with high frame fidelity and strong temporal consistency using reference-guided latent diffusion. We leverage an off-the-shelf text-to-image generation model, e.g., Stable Diffusion, to generate an image with high content quality from the text prompt, as a reference image to guide video generation. Then, we introduce an efficient cascaded latent diffusion module conditioned on both the reference image and the text prompt, for generating latent video representations, followed by a flow-based temporal upsampling step to improve the temporal resolution. Finally, we map latent video representations into a high-definition video through an enhanced video decoder. During training, we use the first frame of a ground-truth video as the reference image for training the cascaded latent diffusion module. The main characterises of our approach include: the reference image generated by the text-to-image model improves the visual fidelity; using it as the condition makes the diffusion model focus more on learning the video dynamics; and the video decoder is trained over unlabeled video data, thus benefiting from high-quality easily-available videos. VideoGen sets a new state-of-the-art in text-to-video generation in terms of both qualitative and quantitative evaluation. See \url{<a href="https://videogen.github.io/VideoGen/" rel="external noopener nofollow" class="link-external link-https">this https URL</a>} for more samples.
</details>

<details>
    <summary>Key points</summary>
    * Reference image from text-to-image model
    * Cascaded latent diffusion module
    * Flow-based temporal upsampling
    * Enhanced video decoder
</details>
</details>

---


<details>
<summary><b> Hierarchical Spatio-temporal Decoupling for Text-to-Video Generation</b></summary>

* **Authors:** Zhiwu Qing, Shiwei Zhang, Jiayu Wang, Xiang Wang, Yujie Wei, Yingya Zhang, Changxin Gao, Nong Sang
* **arXiv ID:** 2312.04483
* **One-liner:** Proposed HiGen, a diffusion model that decouples spatial and temporal factors for improved video generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2312.04483) | [[PDF]](https://arxiv.org/pdf/2312.04483)

> **Core Innovation**
> Decomposed T2V into spatial and temporal reasoning steps and used content cues for motion and appearance guidance.

<details>
    <summary>Abstract</summary>
    Despite diffusion models having shown powerful abilities to generate photorealistic images, generating videos that are realistic and diverse still remains in its infancy. One of the key reasons is that current methods intertwine spatial content and temporal dynamics together, leading to a notably increased complexity of text-to-video generation (T2V). In this work, we propose HiGen, a diffusion model-based method that improves performance by decoupling the spatial and temporal factors of videos from two perspectives, i.e., structure level and content level. At the structure level, we decompose the T2V task into two steps, including spatial reasoning and temporal reasoning, using a unified denoiser. Specifically, we generate spatially coherent priors using text during spatial reasoning and then generate temporally coherent motions from these priors during temporal reasoning. At the content level, we extract two subtle cues from the content of the input video that can express motion and appearance changes, respectively. These two cues then guide the model&#39;s training for generating videos, enabling flexible content variations and enhancing temporal stability. Through the decoupled paradigm, HiGen can effectively reduce the complexity of this task and generate realistic videos with semantics accuracy and motion stability. Extensive experiments demonstrate the superior performance of HiGen over the state-of-the-art T2V methods.
</details>

<details>
    <summary>Key points</summary>
    * Decoupling spatial and temporal factors
    * Spatial reasoning and temporal reasoning steps
    * Extraction of motion and appearance cues
</details>
</details>

---


<details>
<summary><b> UniCtrl: Improving the Spatiotemporal Consistency of Text-to-Video Diffusion Models via Training-Free Unified Attention Control</b></summary>

* **Authors:** Tian Xia, Xuweiyi Chen, Sihan Xu
* **arXiv ID:** 2403.02332
* **One-liner:** Introduced UniCtrl, a plug-and-play method to enhance spatiotemporal consistency in text-to-video models.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2403.02332) | [[PDF]](https://arxiv.org/pdf/2403.02332)

> **Core Innovation**
> Used cross-frame self-attention control and motion injection for consistency without additional training.

<details>
    <summary>Abstract</summary>
    Video Diffusion Models have been developed for video generation, usually integrating text and image conditioning to enhance control over the generated content. Despite the progress, ensuring consistency across frames remains a challenge, particularly when using text prompts as control conditions. To address this problem, we introduce UniCtrl, a novel, plug-and-play method that is universally applicable to improve the spatiotemporal consistency and motion diversity of videos generated by text-to-video models without additional training. UniCtrl ensures semantic consistency across different frames through cross-frame self-attention control, and meanwhile, enhances the motion quality and spatiotemporal consistency through motion injection and spatiotemporal synchronization. Our experimental results demonstrate UniCtrl&#39;s efficacy in enhancing various text-to-video models, confirming its effectiveness and universality.
</details>

<details>
    <summary>Key points</summary>
    * Cross-frame self-attention control
    * Motion injection
    * Spatiotemporal synchronization
</details>
</details>

---


<details>
<summary><b> ViD-GPT: Introducing GPT-style Autoregressive Generation in Video Diffusion Models</b></summary>

* **Authors:** Kaifeng Gao, Jiaxin Shi, Hanwang Zhang, Chunping Wang, Jun Xiao
* **arXiv ID:** 2406.10981
* **One-liner:** Presented ViD-GPT, a causal video diffusion model for long-term consistent video generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2406.10981) | [[PDF]](https://arxiv.org/pdf/2406.10981)

> **Core Innovation**
> Incorporated causal temporal attention and frame-as-prompt with kv-cache for efficient inference.

<details>
    <summary>Abstract</summary>
    With the advance of diffusion models, today&#39;s video generation has achieved impressive quality. But generating temporal consistent long videos is still challenging. A majority of video diffusion models (VDMs) generate long videos in an autoregressive manner, i.e., generating subsequent clips conditioned on last frames of previous clip. However, existing approaches all involve bidirectional computations, which restricts the receptive context of each autoregression step, and results in the model lacking long-term dependencies. Inspired from the huge success of large language models (LLMs) and following GPT (generative pre-trained transformer), we bring causal (i.e., unidirectional) generation into VDMs, and use past frames as prompt to generate future frames. For Causal Generation, we introduce causal temporal attention into VDM, which forces each generated frame to depend on its previous frames. For Frame as Prompt, we inject the conditional frames by concatenating them with noisy frames (frames to be generated) along the temporal axis. Consequently, we present Video Diffusion GPT (ViD-GPT). Based on the two key designs, in each autoregression step, it is able to acquire long-term context from prompting frames concatenated by all previously generated frames. Additionally, we bring the kv-cache mechanism to VDMs, which eliminates the redundant computation from overlapped frames, significantly boosting the inference speed. Extensive experiments demonstrate that our ViD-GPT achieves state-of-the-art performance both quantitatively and qualitatively on long video generation. Code will be available at <a href="https://github.com/Dawn-LX/Causal-VideoGen" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Causal temporal attention
    * Frame-as-prompt mechanism
    * Kv-cache for speed boost
</details>
</details>

---


<details>
<summary><b> TALC: Time-Aligned Captions for Multi-Scene Text-to-Video Generation</b></summary>

* **Authors:** Hritik Bansal, Yonatan Bitton, Michal Yarom, Idan Szpektor, Aditya Grover, Kai-Wei Chang
* **arXiv ID:** 2405.04682
* **One-liner:** Developed TALC framework for generating multi-scene videos from text descriptions.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2405.04682) | [[PDF]](https://arxiv.org/pdf/2405.04682)

> **Core Innovation**
> Enhanced text-conditioning with temporal alignment and fine-tuned on multi-scene data.

<details>
    <summary>Abstract</summary>
    Most of these text-to-video (T2V) generative models often produce single-scene video clips that depict an entity performing a particular action (e.g., &#39;a red panda climbing a tree&#39;). However, it is pertinent to generate multi-scene videos since they are ubiquitous in the real-world (e.g., &#39;a red panda climbing a tree&#39; followed by &#39;the red panda sleeps on the top of the tree&#39;). To generate multi-scene videos from the pretrained T2V model, we introduce a simple and effective Time-Aligned Captions (TALC) framework. Specifically, we enhance the text-conditioning mechanism in the T2V architecture to recognize the temporal alignment between the video scenes and scene descriptions. For instance, we condition the visual features of the earlier and later scenes of the generated video with the representations of the first scene description (e.g., &#39;a red panda climbing a tree&#39;) and second scene description (e.g., &#39;the red panda sleeps on the top of the tree&#39;), respectively. As a result, we show that the T2V model can generate multi-scene videos that adhere to the multi-scene text descriptions and be visually consistent (e.g., entity and background). Further, we finetune the pretrained T2V model with multi-scene video-text data using the TALC framework. We show that the TALC-finetuned model outperforms the baseline by achieving a relative gain of 29% in the overall score, which averages visual consistency and text adherence using human evaluation.
</details>

<details>
    <summary>Key points</summary>
    * Time-aligned captions
    * Temporal alignment of scenes
    * Fine-tuning with multi-scene data
</details>
</details>

---


<details>
<summary><b> DisenStudio: Customized Multi-subject Text-to-Video Generation with Disentangled Spatial Control</b></summary>

* **Authors:** Hong Chen, Xin Wang, Yipeng Zhang, Yuwei Zhou, Zeyang Zhang, Siao Tang, Wenwu Zhu
* **arXiv ID:** 2405.12796
* **One-liner:** Proposed DisenStudio for generating videos with multiple customized subjects.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2405.12796) | [[PDF]](https://arxiv.org/pdf/2405.12796)

> **Core Innovation**
> Used spatial-disentangled cross-attention and motion-preserved fine-tuning strategies.

<details>
    <summary>Abstract</summary>
    Generating customized content in videos has received increasing attention recently. However, existing works primarily focus on customized text-to-video generation for single subject, suffering from subject-missing and attribute-binding problems when the video is expected to contain multiple subjects. Furthermore, existing models struggle to assign the desired actions to the corresponding subjects (action-binding problem), failing to achieve satisfactory multi-subject generation performance. To tackle the problems, in this paper, we propose DisenStudio, a novel framework that can generate text-guided videos for customized multiple subjects, given few images for each subject. Specifically, DisenStudio enhances a pretrained diffusion-based text-to-video model with our proposed spatial-disentangled cross-attention mechanism to associate each subject with the desired action. Then the model is customized for the multiple subjects with the proposed motion-preserved disentangled finetuning, which involves three tuning strategies: multi-subject co-occurrence tuning, masked single-subject tuning, and multi-subject motion-preserved tuning. The first two strategies guarantee the subject occurrence and preserve their visual attributes, and the third strategy helps the model maintain the temporal motion-generation ability when finetuning on static images. We conduct extensive experiments to demonstrate our proposed DisenStudio significantly outperforms existing methods in various metrics. Additionally, we show that DisenStudio can be used as a powerful tool for various controllable generation applications.
</details>

<details>
    <summary>Key points</summary>
    * Spatial-disentangled cross-attention
    * Multi-subject co-occurrence tuning
    * Motion-preserved fine-tuning
</details>
</details>

---


<details>
<summary><b> MotionBooth: Motion-Aware Customized Text-to-Video Generation</b></summary>

* **Authors:** Jianzong Wu, Xiangtai Li, Yanhong Zeng, Jiangning Zhang, Qianyu Zhou, Yining Li, Yunhai Tong, Kai Chen
* **arXiv ID:** 2406.17758
* **One-liner:** Introduced MotionBooth for animating customized subjects with precise motion control.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2406.17758) | [[PDF]](https://arxiv.org/pdf/2406.17758)

> **Core Innovation**
> Employed fine-tuning with specialized losses and training-free techniques for motion control.

<details>
    <summary>Abstract</summary>
    In this work, we present MotionBooth, an innovative framework designed for animating customized subjects with precise control over both object and camera movements. By leveraging a few images of a specific object, we efficiently fine-tune a text-to-video model to capture the object&#39;s shape and attributes accurately. Our approach presents subject region loss and video preservation loss to enhance the subject&#39;s learning performance, along with a subject token cross-attention loss to integrate the customized subject with motion control signals. Additionally, we propose training-free techniques for managing subject and camera motions during inference. In particular, we utilize cross-attention map manipulation to govern subject motion and introduce a novel latent shift module for camera movement control as well. MotionBooth excels in preserving the appearance of subjects while simultaneously controlling the motions in generated videos. Extensive quantitative and qualitative evaluations demonstrate the superiority and effectiveness of our method. Our project page is at <a href="https://jianzongwu.github.io/projects/motionbooth" rel="external noopener nofollow" class="link-external link-https">this https URL</a>
</details>

<details>
    <summary>Key points</summary>
    * Subject region and video preservation losses
    * Cross-attention map manipulation
    * Latent shift module for camera control
</details>
</details>

---


<details>
<summary><b> Text-Animator: Controllable Visual Text Video Generation</b></summary>

* **Authors:** Lin Liu, Quande Liu, Shengju Qian, Yuan Zhou, Wengang Zhou, Houqiang Li, Lingxi Xie, Qi Tian
* **arXiv ID:** 2406.17777
* **One-liner:** Presented Text-Animator for generating videos with visualized text.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2406.17777) | [[PDF]](https://arxiv.org/pdf/2406.17777)

> **Core Innovation**
> Developed text embedding injection and camera control modules for text stability.

<details>
    <summary>Abstract</summary>
    Video generation is a challenging yet pivotal task in various industries, such as gaming, e-commerce, and advertising. One significant unresolved aspect within T2V is the effective visualization of text within generated videos. Despite the progress achieved in Text-to-Video~(T2V) generation, current methods still cannot effectively visualize texts in videos directly, as they mainly focus on summarizing semantic scene information, understanding, and depicting actions. While recent advances in image-level visual text generation show promise, transitioning these techniques into the video domain faces problems, notably in preserving textual fidelity and motion coherence. In this paper, we propose an innovative approach termed Text-Animator for visual text video generation. Text-Animator contains a text embedding injection module to precisely depict the structures of visual text in generated videos. Besides, we develop a camera control module and a text refinement module to improve the stability of generated visual text by controlling the camera movement as well as the motion of visualized text. Quantitative and qualitative experimental results demonstrate the superiority of our approach to the accuracy of generated visual text over state-of-the-art video generation methods. The project page can be found at <a href="https://laulampaul.github.io/text-animator.html" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Text embedding injection module
    * Camera control module
    * Text refinement module
</details>
</details>

---


<details>
<summary><b> CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer</b></summary>

* **Authors:** Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Yuxuan Zhang, Weihan Wang, Yean Cheng, Bin Xu, Xiaotao Gu, Yuxiao Dong, Jie Tang
* **arXiv ID:** 2408.06072
* **One-liner:** Developed CogVideoX, a large-scale diffusion transformer model for coherent long-duration video generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2408.06072) | [[PDF]](https://arxiv.org/pdf/2408.06072)

> **Core Innovation**
> Used 3D VAE, expert transformer, and progressive training for high-quality text-video alignment.

<details>
    <summary>Abstract</summary>
    We present CogVideoX, a large-scale text-to-video generation model based on diffusion transformer, which can generate 10-second continuous videos aligned with text prompt, with a frame rate of 16 fps and resolution of 768 * 1360 pixels. Previous video generation models often had limited movement and short durations, and is difficult to generate videos with coherent narratives based on text. We propose several designs to address these issues. First, we propose a 3D Variational Autoencoder (VAE) to compress videos along both spatial and temporal dimensions, to improve both compression rate and video fidelity. Second, to improve the text-video alignment, we propose an expert transformer with the expert adaptive LayerNorm to facilitate the deep fusion between the two modalities. Third, by employing a progressive training and multi-resolution frame pack technique, CogVideoX is adept at producing coherent, long-duration, different shape videos characterized by significant motions. In addition, we develop an effective text-video data processing pipeline that includes various data preprocessing strategies and a video captioning method, greatly contributing to the generation quality and semantic alignment. Results show that CogVideoX demonstrates state-of-the-art performance across both multiple machine metrics and human evaluations. The model weight of both 3D Causal VAE, Video caption model and CogVideoX are publicly available at <a href="https://github.com/THUDM/CogVideo" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * 3D Variational Autoencoder
    * Expert transformer with adaptive LayerNorm
    * Progressive training and multi-resolution frame pack
</details>
</details>

---


<details>
<summary><b> Qihoo-T2X: An Efficient Proxy-Tokenized Diffusion Transformer for Text-to-Any-Task</b></summary>

* **Authors:** Jing Wang, Ao Ma, Jiasong Feng, Dawei Leng, Yuhui Yin, Xiaodan Liang
* **arXiv ID:** 2409.04005
* **One-liner:** Proposed PT-DiT for efficient diffusion transformers by reducing redundant computation with proxy tokens.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2409.04005) | [[PDF]](https://arxiv.org/pdf/2409.04005)

> **Core Innovation**
> Introduced a sparse representative token attention mechanism using proxy tokens to capture global semantics efficiently, with window and shift window attention for detail modeling.

<details>
    <summary>Abstract</summary>
    The global self-attention mechanism in diffusion transformers involves redundant computation due to the sparse and redundant nature of visual information, and the attention map of tokens within a spatial window shows significant similarity. To address this redundancy, we propose the Proxy-Tokenized Diffusion Transformer (PT-DiT), which employs sparse representative token attention (where the number of representative tokens is much smaller than the total number of tokens) to model global visual information efficiently. Specifically, within each transformer block, we compute an averaging token from each spatial-temporal window to serve as a proxy token for that region. The global semantics are captured through the self-attention of these proxy tokens and then injected into all latent tokens via cross-attention. Simultaneously, we introduce window and shift window attention to address the limitations in detail modeling caused by the sparse attention mechanism. Building on the well-designed PT-DiT, we further develop the Qihoo-T2X family, which includes a variety of models for T2I, T2V, and T2MV tasks. Experimental results show that PT-DiT achieves competitive performance while reducing the computational complexity in both image and video generation tasks (e.g., a 49% reduction compared to DiT and a 34% reduction compared to PixArt-$\alpha$). The visual exhibition and source code of Qihoo-T2X is available at <a href="https://360cvgroup.github.io/Qihoo-T2X/" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Employed averaging tokens as proxy tokens for each spatial-temporal window
    * Used self-attention of proxy tokens and cross-attention to inject global semantics into latent tokens
    * Integrated window and shift window attention to enhance detail modeling
</details>
</details>

---


<details>
<summary><b> Loong: Generating Minute-level Long Videos with Autoregressive Language Models</b></summary>

* **Authors:** Yuqing Wang, Tianwei Xiong, Daquan Zhou, Zhijie Lin, Yang Zhao, Bingyi Kang, Jiashi Feng, Xihui Liu
* **arXiv ID:** 2410.02757
* **One-liner:** Developed Loong, an autoregressive LLM-based video generator for minute-long videos.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2410.02757) | [[PDF]](https://arxiv.org/pdf/2410.02757)

> **Core Innovation**
> Modeled text and video tokens as a unified sequence with progressive short-to-long training and inference strategies to mitigate error accumulation.

<details>
    <summary>Abstract</summary>
    It is desirable but challenging to generate content-rich long videos in the scale of minutes. Autoregressive large language models (LLMs) have achieved great success in generating coherent and long sequences of tokens in the domain of natural language processing, while the exploration of autoregressive LLMs for video generation is limited to generating short videos of several seconds. In this work, we conduct a deep analysis of the challenges that prevent autoregressive LLM-based video generators from generating long videos. Based on the observations and analysis, we propose Loong, a new autoregressive LLM-based video generator that can generate minute-long videos. Specifically, we model the text tokens and video tokens as a unified sequence for autoregressive LLMs and train the model from scratch. We propose progressive short-to-long training with a loss re-weighting scheme to mitigate the loss imbalance problem for long video training. We further investigate inference strategies, including video token re-encoding and sampling strategies, to diminish error accumulation during inference. Our proposed Loong can be trained on 10-second videos and be extended to generate minute-level long videos conditioned on text prompts, as demonstrated by the results. More samples are available at: <a href="https://yuqingwang1029.github.io/Loong-video" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Unified text and video tokens for autoregressive modeling
    * Applied progressive short-to-long training with loss re-weighting
    * Used video token re-encoding and sampling strategies during inference
</details>
</details>

---


<details>
<summary><b> DiCoDe: Diffusion-Compressed Deep Tokens for Autoregressive Video Generation with Language Models</b></summary>

* **Authors:** Yizhuo Li, Yuying Ge, Yixiao Ge, Ping Luo, Ying Shan
* **arXiv ID:** 2412.04446
* **One-liner:** Introduced DiCoDe for scalable video generation using deep tokens with high compression.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2412.04446) | [[PDF]](https://arxiv.org/pdf/2412.04446)

> **Core Innovation**
> Leveraged diffusion-compressed deep tokens for autoregressive video generation, enabling efficient training and scalability with AR language models.

<details>
    <summary>Abstract</summary>
    Videos are inherently temporal sequences by their very nature. In this work, we explore the potential of modeling videos in a chronological and scalable manner with autoregressive (AR) language models, inspired by their success in natural language processing. We introduce DiCoDe, a novel approach that leverages Diffusion-Compressed Deep Tokens to generate videos with a language model in an autoregressive manner. Unlike existing methods that employ low-level representations with limited compression rates, DiCoDe utilizes deep tokens with a considerable compression rate (a 1000x reduction in token count). This significant compression is made possible by a tokenizer trained through leveraging the prior knowledge of video diffusion models. Deep tokens enable DiCoDe to employ vanilla AR language models for video generation, akin to translating one visual &#34;language&#34; into another. By treating videos as temporal sequences, DiCoDe fully harnesses the capabilities of language models for autoregressive generation. DiCoDe is scalable using readily available AR architectures, and is capable of generating videos ranging from a few seconds to one minute using only 4 A100 GPUs for training. We evaluate DiCoDe both quantitatively and qualitatively, demonstrating that it performs comparably to existing methods in terms of quality while ensuring efficient training. To showcase its scalability, we release a series of DiCoDe configurations with varying parameter sizes and observe a consistent improvement in performance as the model size increases from 100M to 3B. We believe that DiCoDe&#39;s exploration in academia represents a promising initial step toward scalable video modeling with AR language models, paving the way for the development of larger and more powerful video generation models.
</details>

<details>
    <summary>Key points</summary>
    * Utilized deep tokens with 1000x compression via a tokenizer trained on video diffusion prior
    * Employed vanilla AR language models for video generation
    * Scaled model sizes from 100M to 3B parameters for consistent performance improvement
</details>
</details>

---


<details>
<summary><b> InstanceCap: Improving Text-to-Video Generation via Instance-aware Structured Caption</b></summary>

* **Authors:** Tiehan Fan, Kepan Nan, Rui Xie, Penghao Zhou, Zhenheng Yang, Chaoyou Fu, Xiang Li, Jian Yang, Ying Tai
* **arXiv ID:** 2412.09283
* **One-liner:** Proposed InstanceCap for instance-level fine-grained video captioning to improve fidelity.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2412.09283) | [[PDF]](https://arxiv.org/pdf/2412.09283)

> **Core Innovation**
> Designed an instance-aware structured caption framework with auxiliary models and a curated dataset to reduce hallucinations and enhance caption precision.

<details>
    <summary>Abstract</summary>
    Text-to-video generation has evolved rapidly in recent years, delivering remarkable results. Training typically relies on video-caption paired data, which plays a crucial role in enhancing generation performance. However, current video captions often suffer from insufficient details, hallucinations and imprecise motion depiction, affecting the fidelity and consistency of generated videos. In this work, we propose a novel instance-aware structured caption framework, termed InstanceCap, to achieve instance-level and fine-grained video caption for the first time. Based on this scheme, we design an auxiliary models cluster to convert original video into instances to enhance instance fidelity. Video instances are further used to refine dense prompts into structured phrases, achieving concise yet precise descriptions. Furthermore, a 22K InstanceVid dataset is curated for training, and an enhancement pipeline that tailored to InstanceCap structure is proposed for inference. Experimental results demonstrate that our proposed InstanceCap significantly outperform previous models, ensuring high fidelity between captions and videos while reducing hallucinations.
</details>

<details>
    <summary>Key points</summary>
    * Used auxiliary models to convert videos into instances for fidelity enhancement
    * Refined dense prompts into structured phrases for concise descriptions
    * Curated a 22K InstanceVid dataset and proposed an enhancement pipeline
</details>
</details>

---


<details>
<summary><b> TIV-Diffusion: Towards Object-Centric Movement for Text-driven Image to Video Generation</b></summary>

* **Authors:** Xingrui Wang, Xin Li, Yaosi Hu, Hanxin Zhu, Chen Hou, Cuiling Lan, Zhibo Chen
* **arXiv ID:** 2412.10275
* **One-liner:** Developed TIV-Diffusion for precise text-driven image-to-video generation via object-centric alignment.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2412.10275) | [[PDF]](https://arxiv.org/pdf/2412.10275)

> **Core Innovation**
> Incorporated fused textual and visual knowledge with scale-offset modulation and an object-centric alignment module to ensure motion consistency and high quality.

<details>
    <summary>Abstract</summary>
    Text-driven Image to Video Generation (TI2V) aims to generate controllable video given the first frame and corresponding textual description. The primary challenges of this task lie in two parts: (i) how to identify the target objects and ensure the consistency between the movement trajectory and the textual description. (ii) how to improve the subjective quality of generated videos. To tackle the above challenges, we propose a new diffusion-based TI2V framework, termed TIV-Diffusion, via object-centric textual-visual alignment, intending to achieve precise control and high-quality video generation based on textual-described motion for different objects. Concretely, we enable our TIV-Diffuion model to perceive the textual-described objects and their motion trajectory by incorporating the fused textual and visual knowledge through scale-offset modulation. Moreover, to mitigate the problems of object disappearance and misaligned objects and motion, we introduce an object-centric textual-visual alignment module, which reduces the risk of misaligned objects/motion by decoupling the objects in the reference image and aligning textual features with each object individually. Based on the above innovations, our TIV-Diffusion achieves state-of-the-art high-quality video generation compared with existing TI2V methods.
</details>

<details>
    <summary>Key points</summary>
    * Applied scale-offset modulation for textual-visual fusion
    * Introduced object-centric textual-visual alignment module to decouple and align objects
    * Addressed object disappearance and misalignment issues
</details>
</details>

---


<details>
<summary><b> LTX-Video: Realtime Video Latent Diffusion</b></summary>

* **Authors:** Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, Poriya Panet, Sapir Weissbuch, Victor Kulikov, Yaki Bitterman, Zeev Melumian, Ofir Bibi
* **arXiv ID:** 2501.00103
* **One-liner:** Introduced LTX-Video, a holistic latent diffusion model for efficient high-resolution video generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2501.00103) | [[PDF]](https://arxiv.org/pdf/2501.00103)

> **Core Innovation**
> Integrated Video-VAE and denoising transformer with high compression and full spatiotemporal self-attention, enabling fast generation without separate upsampling.

<details>
    <summary>Abstract</summary>
    We introduce LTX-Video, a transformer-based latent diffusion model that adopts a holistic approach to video generation by seamlessly integrating the responsibilities of the Video-VAE and the denoising transformer. Unlike existing methods, which treat these components as independent, LTX-Video aims to optimize their interaction for improved efficiency and quality. At its core is a carefully designed Video-VAE that achieves a high compression ratio of 1:192, with spatiotemporal downscaling of 32 x 32 x 8 pixels per token, enabled by relocating the patchifying operation from the transformer&#39;s input to the VAE&#39;s input. Operating in this highly compressed latent space enables the transformer to efficiently perform full spatiotemporal self-attention, which is essential for generating high-resolution videos with temporal consistency. However, the high compression inherently limits the representation of fine details. To address this, our VAE decoder is tasked with both latent-to-pixel conversion and the final denoising step, producing the clean result directly in pixel space. This approach preserves the ability to generate fine details without incurring the runtime cost of a separate upsampling module. Our model supports diverse use cases, including text-to-video and image-to-video generation, with both capabilities trained simultaneously. It achieves faster-than-real-time generation, producing 5 seconds of 24 fps video at 768x512 resolution in just 2 seconds on an Nvidia H100 GPU, outperforming all existing models of similar scale. The source code and pre-trained models are publicly available, setting a new benchmark for accessible and scalable video generation.
</details>

<details>
    <summary>Key points</summary>
    * Designed Video-VAE with 1:192 compression ratio and spatiotemporal downscaling
    * Used full spatiotemporal self-attention in latent space
    * Tasked VAE decoder with latent-to-pixel conversion and final denoising
</details>
</details>

---


<details>
<summary><b> Step-Video-T2V Technical Report: The Practice, Challenges, and Future of Video Foundation Model</b></summary>

* **Authors:** Guoqing Ma, Haoyang Huang, Kun Yan, Liangyu Chen, Nan Duan, Shengming Yin, Changyi Wan, Ranchen Ming, Xiaoniu Song, Xing Chen, Yu Zhou, Deshan Sun, Deyu Zhou, Jian Zhou, Kaijun Tan, Kang An, Mei Chen, Wei Ji, Qiling Wu, Wen Sun, Xin Han, Yanan Wei, Zheng Ge, Aojie Li, Bin Wang, Bizhu Huang, Bo Wang, Brian Li, Changxing Miao, Chen Xu, Chenfei Wu, Chenguang Yu, Dapeng Shi, Dingyuan Hu, Enle Liu, Gang Yu, Ge Yang, Guanzhe Huang, Gulin Yan, Haiyang Feng, Hao Nie, Haonan Jia, Hanpeng Hu, Hanqi Chen, Haolong Yan, Heng Wang, Hongcheng Guo, Huilin Xiong, Huixin Xiong, Jiahao Gong, Jianchang Wu, Jiaoren Wu, Jie Wu, Jie Yang, Jiashuai Liu, Jiashuo Li, Jingyang Zhang, Junjing Guo, Junzhe Lin, Kaixiang Li, Lei Liu, Lei Xia, Liang Zhao, Liguo Tan, Liwen Huang, Liying Shi, Ming Li, Mingliang Li, Muhua Cheng, Na Wang, Qiaohui Chen, Qinglin He, Qiuyan Liang, Quan Sun, Ran Sun, Rui Wang, Shaoliang Pang, Shiliang Yang, Sitong Liu, Siqi Liu, Shuli Gao, Tiancheng Cao, Tianyu Wang, Weipeng Ming, Wenqing He, Xu Zhao, Xuelin Zhang, Xianfang Zeng, Xiaojia Liu, Xuan Yang, Yaqi Dai, Yanbo Yu, Yang Li, Yineng Deng, Yingming Wang, Yilei Wang, Yuanwei Lu, Yu Chen, Yu Luo, Yuchu Luo, Yuhe Yin, Yuheng Feng, Yuxiang Yang, Zecheng Tang, Zekai Zhang, Zidong Yang, Binxing Jiao, Jiansheng Chen, Jing Li, Shuchang Zhou, Xiangyu Zhang, Xinhao Zhang, Yibo Zhu, Heung-Yeung Shum, Daxin Jiang
* **arXiv ID:** 2502.10248
* **One-liner:** Presented Step-Video-T2V, a large-scale text-to-video model with 30B parameters for high-quality generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2502.10248) | [[PDF]](https://arxiv.org/pdf/2502.10248)

> **Core Innovation**
> Employed a deep compression Video-VAE, DiT with 3D full attention, and Video-DPO for artifact reduction and improved visual quality.

<details>
    <summary>Abstract</summary>
    We present Step-Video-T2V, a state-of-the-art text-to-video pre-trained model with 30B parameters and the ability to generate videos up to 204 frames in length. A deep compression Variational Autoencoder, Video-VAE, is designed for video generation tasks, achieving 16x16 spatial and 8x temporal compression ratios, while maintaining exceptional video reconstruction quality. User prompts are encoded using two bilingual text encoders to handle both English and Chinese. A DiT with 3D full attention is trained using Flow Matching and is employed to denoise input noise into latent frames. A video-based DPO approach, Video-DPO, is applied to reduce artifacts and improve the visual quality of the generated videos. We also detail our training strategies and share key observations and insights. Step-Video-T2V&#39;s performance is evaluated on a novel video generation benchmark, Step-Video-T2V-Eval, demonstrating its state-of-the-art text-to-video quality when compared with both open-source and commercial engines. Additionally, we discuss the limitations of current diffusion-based model paradigm and outline future directions for video foundation models. We make both Step-Video-T2V and Step-Video-T2V-Eval available at <a href="https://github.com/stepfun-ai/Step-Video-T2V" rel="external noopener nofollow" class="link-external link-https">this https URL</a>. The online version can be accessed from <a href="https://yuewen.cn/videos" rel="external noopener nofollow" class="link-external link-https">this https URL</a> as well. Our goal is to accelerate the innovation of video foundation models and empower video content creators.
</details>

<details>
    <summary>Key points</summary>
    * Designed Video-VAE with 16x16 spatial and 8x temporal compression
    * Trained DiT using Flow Matching for denoising
    * Applied Video-DPO to enhance video quality
</details>
</details>

---


<details>
<summary><b> Wan: Open and Advanced Large-Scale Video Generative Models</b></summary>

* **Authors:** Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, Ziyu Liu
* **arXiv ID:** 2503.20314
* **One-liner:** Introduced Wan, an open suite of video foundation models with leading performance and comprehensiveness.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2503.20314) | [[PDF]](https://arxiv.org/pdf/2503.20314)

> **Core Innovation**
> Achieved advancements through novel VAE, scalable pre-training, large-scale data curation, and automated evaluation, covering multiple tasks and model sizes.

<details>
    <summary>Abstract</summary>
    This report presents Wan, a comprehensive and open suite of video foundation models designed to push the boundaries of video generation. Built upon the mainstream diffusion transformer paradigm, Wan achieves significant advancements in generative capabilities through a series of innovations, including our novel VAE, scalable pre-training strategies, large-scale data curation, and automated evaluation metrics. These contributions collectively enhance the model&#39;s performance and versatility. Specifically, Wan is characterized by four key features: Leading Performance: The 14B model of Wan, trained on a vast dataset comprising billions of images and videos, demonstrates the scaling laws of video generation with respect to both data and model size. It consistently outperforms the existing open-source models as well as state-of-the-art commercial solutions across multiple internal and external benchmarks, demonstrating a clear and significant performance superiority. Comprehensiveness: Wan offers two capable models, i.e., 1.3B and 14B parameters, for efficiency and effectiveness respectively. It also covers multiple downstream applications, including image-to-video, instruction-guided video editing, and personal video generation, encompassing up to eight tasks. Consumer-Grade Efficiency: The 1.3B model demonstrates exceptional resource efficiency, requiring only 8.19 GB VRAM, making it compatible with a wide range of consumer-grade GPUs. Openness: We open-source the entire series of Wan, including source code and all models, with the goal of fostering the growth of the video generation community. This openness seeks to significantly expand the creative possibilities of video production in the industry and provide academia with high-quality video foundation models. All the code and models are available at <a href="https://github.com/Wan-Video/Wan2.1" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Developed novel VAE and scalable pre-training strategies
    * Curated large-scale dataset with billions of images and videos
    * Offered models from 1.3B to 14B parameters for various applications
</details>
</details>

---


<details>
<summary><b> VPO: Aligning Text-to-Video Generation Models with Prompt Optimization</b></summary>

* **Authors:** Jiale Cheng, Ruiliang Lyu, Xiaotao Gu, Xiao Liu, Jiazheng Xu, Yida Lu, Jiayan Teng, Zhuoyi Yang, Yuxiao Dong, Jie Tang, Hongning Wang, Minlie Huang
* **arXiv ID:** 2503.20491
* **One-liner:** Proposed VPO, a framework for prompt optimization to enhance video generation safety and quality.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2503.20491) | [[PDF]](https://arxiv.org/pdf/2503.20491)

> **Core Innovation**
> Employed a two-stage optimization with SFT and preference learning based on harmlessness, accuracy, and helpfulness principles.

<details>
    <summary>Abstract</summary>
    Video generation models have achieved remarkable progress in text-to-video tasks. These models are typically trained on text-video pairs with highly detailed and carefully crafted descriptions, while real-world user inputs during inference are often concise, vague, or poorly structured. This gap makes prompt optimization crucial for generating high-quality videos. Current methods often rely on large language models (LLMs) to refine prompts through in-context learning, but suffer from several limitations: they may distort user intent, omit critical details, or introduce safety risks. Moreover, they optimize prompts without considering the impact on the final video quality, which can lead to suboptimal results. To address these issues, we introduce VPO, a principled framework that optimizes prompts based on three core principles: harmlessness, accuracy, and helpfulness. The generated prompts faithfully preserve user intents and, more importantly, enhance the safety and quality of generated videos. To achieve this, VPO employs a two-stage optimization approach. First, we construct and refine a supervised fine-tuning (SFT) dataset based on principles of safety and alignment. Second, we introduce both text-level and video-level feedback to further optimize the SFT model with preference learning. Our extensive experiments demonstrate that VPO significantly improves safety, alignment, and video quality compared to baseline methods. Moreover, VPO shows strong generalization across video generation models. Furthermore, we demonstrate that VPO could outperform and be combined with RLHF methods on video generation models, underscoring the effectiveness of VPO in aligning video generation models. Our code and data are publicly available at <a href="https://github.com/thu-coai/VPO" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Constructed and refined SFT dataset for safety and alignment
    * Used text-level and video-level feedback for preference learning
    * Ensured prompts preserve user intent and improve video quality
</details>
</details>

---


<details>
<summary><b> DyST-XL: Dynamic Layout Planning and Content Control for Compositional Text-to-Video Generation</b></summary>

* **Authors:** Weijie He, Mushui Liu, Yunlong Yu, Zhao Wang, Chao Wu
* **arXiv ID:** 2504.15032
* **One-liner:** Developed DyST-XL, a training-free framework for compositional text-to-video generation with precise control.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2504.15032) | [[PDF]](https://arxiv.org/pdf/2504.15032)

> **Core Innovation**
> Integrated Dynamic Layout Planner, Dual-Prompt Controlled Attention, and Entity-Consistency Constraint to improve layout and entity consistency.

<details>
    <summary>Abstract</summary>
    Compositional text-to-video generation, which requires synthesizing dynamic scenes with multiple interacting entities and precise spatial-temporal relationships, remains a critical challenge for diffusion-based models. Existing methods struggle with layout discontinuity, entity identity drift, and implausible interaction dynamics due to unconstrained cross-attention mechanisms and inadequate physics-aware reasoning. To address these limitations, we propose DyST-XL, a \textbf{training-free} framework that enhances off-the-shelf text-to-video models (e.g., CogVideoX-5B) through frame-aware control. DyST-XL integrates three key innovations: (1) A Dynamic Layout Planner that leverages large language models (LLMs) to parse input prompts into entity-attribute graphs and generates physics-aware keyframe layouts, with intermediate frames interpolated via trajectory optimization; (2) A Dual-Prompt Controlled Attention Mechanism that enforces localized text-video alignment through frame-aware attention masking, achieving precise control over individual entities; and (3) An Entity-Consistency Constraint strategy that propagates first-frame feature embeddings to subsequent frames during denoising, preserving object identity without manual annotation. Experiments demonstrate that DyST-XL excels in compositional text-to-video generation, significantly improving performance on complex prompts and bridging a crucial gap in training-free video synthesis. The code is released in <a href="https://github.com/XiaoBuL/DyST-XL" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Leveraged LLMs for physics-aware keyframe layout planning
    * Applied frame-aware attention masking for localized alignment
    * Propagated first-frame features to preserve object identity
</details>
</details>

---


<details>
<summary><b> ShotAdapter: Text-to-Multi-Shot Video Generation with Diffusion Models</b></summary>

* **Authors:** Ozgur Kara, Krishna Kumar Singh, Feng Liu, Duygu Ceylan, James M. Rehg, Tobias Hinz
* **arXiv ID:** 2505.07652
* **One-liner:** Enabled text-to-multi-shot video generation with character consistency and user control over shots.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2505.07652) | [[PDF]](https://arxiv.org/pdf/2505.07652)

> **Core Innovation**
> Proposed a framework with dataset collection and architectural extensions to video diffusion models for generating multi-shot videos as a single video with full attention across frames.

<details>
    <summary>Abstract</summary>
    Current diffusion-based text-to-video methods are limited to producing short video clips of a single shot and lack the capability to generate multi-shot videos with discrete transitions where the same character performs distinct activities across the same or different backgrounds. To address this limitation we propose a framework that includes a dataset collection pipeline and architectural extensions to video diffusion models to enable text-to-multi-shot video generation. Our approach enables generation of multi-shot videos as a single video with full attention across all frames of all shots, ensuring character and background consistency, and allows users to control the number, duration, and content of shots through shot-specific conditioning. This is achieved by incorporating a transition token into the text-to-video model to control at which frames a new shot begins and a local attention masking strategy which controls the transition token&#39;s effect and allows shot-specific prompting. To obtain training data we propose a novel data collection pipeline to construct a multi-shot video dataset from existing single-shot video datasets. Extensive experiments demonstrate that fine-tuning a pre-trained text-to-video model for a few thousand iterations is enough for the model to subsequently be able to generate multi-shot videos with shot-specific control, outperforming the baselines. You can find more details in <a href="https://shotadapter.github.io/" rel="external noopener nofollow" class="link-external link-https">this https URL</a>
</details>

<details>
    <summary>Key points</summary>
    * Incorporated a transition token to control shot beginnings.
    * Used local attention masking for shot-specific prompting.
    * Constructed a multi-shot video dataset from single-shot datasets.
    * Fine-tuned pre-trained models for efficient multi-shot generation.
</details>
</details>

---


<details>
<summary><b> MOVi: Training-free Text-conditioned Multi-Object Video Generation</b></summary>

* **Authors:** Aimon Rahman, Jiang Liu, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Yusheng Su, Vishal M. Patel, Zicheng Liu, Emad Barsoum
* **arXiv ID:** 2505.22980
* **One-liner:** Enhanced multi-object video generation without training using LLM-guided trajectories and attention manipulation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2505.22980) | [[PDF]](https://arxiv.org/pdf/2505.22980)

> **Core Innovation**
> Introduced a training-free approach leveraging LLMs for object trajectory direction and noise re-initialization to control movements and prevent cross-object interference.

<details>
    <summary>Abstract</summary>
    Recent advances in diffusion-based text-to-video (T2V) models have demonstrated remarkable progress, but these models still face challenges in generating videos with multiple objects. Most models struggle with accurately capturing complex object interactions, often treating some objects as static background elements and limiting their movement. In addition, they often fail to generate multiple distinct objects as specified in the prompt, resulting in incorrect generations or mixed features across objects. In this paper, we present a novel training-free approach for multi-object video generation that leverages the open world knowledge of diffusion models and large language models (LLMs). We use an LLM as the ``director&#39;&#39; of object trajectories, and apply the trajectories through noise re-initialization to achieve precise control of realistic movements. We further refine the generation process by manipulating the attention mechanism to better capture object-specific features and motion patterns, and prevent cross-object feature interference. Extensive experiments validate the effectiveness of our training free approach in significantly enhancing the multi-object generation capabilities of existing video diffusion models, resulting in 42% absolute improvement in motion dynamics and object generation accuracy, while also maintaining high fidelity and motion smoothness.
</details>

<details>
    <summary>Key points</summary>
    * Used LLM as a director for object trajectories.
    * Applied noise re-initialization for precise movement control.
    * Manipulated attention mechanism to capture object-specific features.
    * Prevented cross-object feature interference.
</details>
</details>

---


<details>
<summary><b> Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion</b></summary>

* **Authors:** Xun Huang, Zhengqi Li, Guande He, Mingyuan Zhou, Eli Shechtman
* **arXiv ID:** 2506.08009
* **One-liner:** Addressed exposure bias in autoregressive video diffusion models for real-time streaming generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2506.08009) | [[PDF]](https://arxiv.org/pdf/2506.08009)

> **Core Innovation**
> Proposed Self Forcing, a training paradigm using autoregressive rollout with KV caching to condition on self-generated outputs and apply holistic video-level loss.

<details>
    <summary>Abstract</summary>
    We introduce Self Forcing, a novel training paradigm for autoregressive video diffusion models. It addresses the longstanding issue of exposure bias, where models trained on ground-truth context must generate sequences conditioned on their own imperfect outputs during inference. Unlike prior methods that denoise future frames based on ground-truth context frames, Self Forcing conditions each frame&#39;s generation on previously self-generated outputs by performing autoregressive rollout with key-value (KV) caching during training. This strategy enables supervision through a holistic loss at the video level that directly evaluates the quality of the entire generated sequence, rather than relying solely on traditional frame-wise objectives. To ensure training efficiency, we employ a few-step diffusion model along with a stochastic gradient truncation strategy, effectively balancing computational cost and performance. We further introduce a rolling KV cache mechanism that enables efficient autoregressive video extrapolation. Extensive experiments demonstrate that our approach achieves real-time streaming video generation with sub-second latency on a single GPU, while matching or even surpassing the generation quality of significantly slower and non-causal diffusion models. Project website: <a href="http://self-forcing.github.io/" rel="external noopener nofollow" class="link-external link-http">this http URL</a>
</details>

<details>
    <summary>Key points</summary>
    * Performed autoregressive rollout with KV caching during training.
    * Used holistic loss for entire generated sequence evaluation.
    * Employed few-step diffusion and stochastic gradient truncation for efficiency.
    * Introduced rolling KV cache for efficient video extrapolation.
</details>
</details>

---


<details>
<summary><b> Lumos-1: On Autoregressive Video Generation from a Unified Model Perspective</b></summary>

* **Authors:** Hangjie Yuan, Weihua Chen, Jun Cen, Hu Yu, Jingyun Liang, Shuning Chang, Zhihui Lin, Tao Feng, Pengwei Liu, Jiazheng Xing, Hao Luo, Jiasheng Tang, Fan Wang, Yi Yang
* **arXiv ID:** 2507.08801
* **One-liner:** Developed an autoregressive video generator (Lumos-1) with minimal LLM modifications for efficient multimodal spatiotemporal modeling.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2507.08801) | [[PDF]](https://arxiv.org/pdf/2507.08801)

> **Core Innovation**
> Retained LLM architecture with MM-RoPE for balanced frequency spectra and introduced AR-DF to handle frame-wise loss imbalance and spatial redundancy.

<details>
    <summary>Abstract</summary>
    Autoregressive large language models (LLMs) have unified a vast range of language tasks, inspiring preliminary efforts in autoregressive video generation. Existing autoregressive video generators either diverge from standard LLM architectures, depend on bulky external text encoders, or incur prohibitive latency due to next-token decoding. In this paper, we introduce Lumos-1, an autoregressive video generator that retains the LLM architecture with minimal architectural modifications. To inject spatiotemporal correlations in LLMs, we identify the efficacy of incorporating 3D RoPE and diagnose its imbalanced frequency spectrum ranges. Therefore, we propose MM-RoPE, a RoPE scheme that preserves the original textual RoPE while providing comprehensive frequency spectra and scaled 3D positions for modeling multimodal spatiotemporal data. Moreover, Lumos-1 resorts to a token dependency strategy that obeys intra-frame bidirectionality and inter-frame temporal causality. Based on this dependency strategy, we identify the issue of frame-wise loss imbalance caused by spatial information redundancy and solve it by proposing Autoregressive Discrete Diffusion Forcing (AR-DF). AR-DF introduces temporal tube masking during training with a compatible inference-time masking policy to avoid quality degradation. By using memory-efficient training techniques, we pre-train Lumos-1 on only 48 GPUs, achieving performance comparable to EMU3 on GenEval, COSMOS-Video2World on VBench-I2V, and OpenSoraPlan on VBench-T2V. Code and models are available at <a href="https://github.com/alibaba-damo-academy/Lumos" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Incorporated MM-RoPE for spatiotemporal correlations.
    * Used token dependency strategy with intra-frame bidirectionality and inter-frame causality.
    * Applied AR-DF with temporal tube masking during training.
    * Employed memory-efficient training techniques.
</details>
</details>

---


<details>
<summary><b> TITAN-Guide: Taming Inference-Time AligNment for Guided Text-to-Video Diffusion Models</b></summary>

* **Authors:** Christian Simon, Masato Ishii, Akio Hayakawa, Zhi Zhong, Shusuke Takahashi, Takashi Shibuya, Yuki Mitsufuji
* **arXiv ID:** 2508.00289
* **One-liner:** Improved training-free guidance for text-to-video diffusion models with efficient memory usage and optimal control.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2508.00289) | [[PDF]](https://arxiv.org/pdf/2508.00289)

> **Core Innovation**
> Proposed TITAN-Guide, using forward gradient descents for latent optimization without backpropagation to overcome memory and control limitations.

<details>
    <summary>Abstract</summary>
    In the recent development of conditional diffusion models still require heavy supervised fine-tuning for performing control on a category of tasks. Training-free conditioning via guidance with off-the-shelf models is a favorable alternative to avoid further fine-tuning on the base model. However, the existing training-free guidance frameworks either have heavy memory requirements or offer sub-optimal control due to rough estimation. These shortcomings limit the applicability to control diffusion models that require intense computation, such as Text-to-Video (T2V) diffusion models. In this work, we propose Taming Inference Time Alignment for Guided Text-to-Video Diffusion Model, so-called TITAN-Guide, which overcomes memory space issues, and provides more optimal control in the guidance process compared to the counterparts. In particular, we develop an efficient method for optimizing diffusion latents without backpropagation from a discriminative guiding model. In particular, we study forward gradient descents for guided diffusion tasks with various options on directional directives. In our experiments, we demonstrate the effectiveness of our approach in efficiently managing memory during latent optimization, while previous methods fall short. Our proposed approach not only minimizes memory requirements but also significantly enhances T2V performance across a range of diffusion guidance benchmarks. Code, models, and demo are available at <a href="https://titanguide.github.io" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Developed efficient latent optimization without backpropagation.
    * Used forward gradient descents with directional directives.
    * Minimized memory requirements during inference.
    * Enhanced T2V performance across benchmarks.
</details>
</details>

---


<details>
<summary><b> VidCLearn: A Continual Learning Approach for Text-to-Video Generation</b></summary>

* **Authors:** Luca Zanchetta, Lorenzo Papa, Luca Maiano, Irene Amerini
* **arXiv ID:** 2509.16956
* **One-liner:** Enabled continual learning for text-to-video generation without retraining from scratch.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2509.16956) | [[PDF]](https://arxiv.org/pdf/2509.16956)

> **Core Innovation**
> Introduced VidCLearn with a student-teacher architecture, generative replay, temporal consistency loss, and video retrieval for incremental updates and knowledge preservation.

<details>
    <summary>Abstract</summary>
    Text-to-video generation is an emerging field in generative AI, enabling the creation of realistic, semantically accurate videos from text prompts. While current models achieve impressive visual quality and alignment with input text, they typically rely on static knowledge, making it difficult to incorporate new data without retraining from scratch. To address this limitation, we propose VidCLearn, a continual learning framework for diffusion-based text-to-video generation. VidCLearn features a student-teacher architecture where the student model is incrementally updated with new text-video pairs, and the teacher model helps preserve previously learned knowledge through generative replay. Additionally, we introduce a novel temporal consistency loss to enhance motion smoothness and a video retrieval module to provide structural guidance at inference. Our architecture is also designed to be more computationally efficient than existing models while retaining satisfactory generation performance. Experimental results show VidCLearn&#39;s superiority over baseline methods in terms of visual quality, semantic alignment, and temporal coherence.
</details>

<details>
    <summary>Key points</summary>
    * Used student-teacher architecture for incremental learning.
    * Applied generative replay to preserve knowledge.
    * Introduced temporal consistency loss for motion smoothness.
    * Incorporated video retrieval for structural guidance.
</details>
</details>

---


<details>
<summary><b> Wan-Alpha: High-Quality Text-to-Video Generation with Alpha Channel</b></summary>

* **Authors:** Haotian Dong, Wenjing Wang, Chen Li, Di Lin
* **arXiv ID:** 2509.24979
* **One-liner:** Generated high-quality transparent videos with superior visual and motion realism.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2509.24979) | [[PDF]](https://arxiv.org/pdf/2509.24979)

> **Core Innovation**
> Proposed Wan-Alpha, a framework using a VAE to encode alpha channels into RGB latent space and trained on a diverse RGBA dataset for joint RGB and alpha generation.

<details>
    <summary>Abstract</summary>
    RGBA video generation, which includes an alpha channel to represent transparency, is gaining increasing attention across a wide range of applications. However, existing methods often neglect visual quality, limiting their practical usability. In this paper, we propose Wan-Alpha, a new framework that generates transparent videos by learning both RGB and alpha channels jointly. We design an effective variational autoencoder (VAE) that encodes the alpha channel into the RGB latent space. Then, to support the training of our diffusion transformer, we construct a high-quality and diverse RGBA video dataset. Compared with state-of-the-art methods, our model demonstrates superior performance in visual quality, motion realism, and transparency rendering. Notably, our model can generate a wide variety of semi-transparent objects, glowing effects, and fine-grained details such as hair strands. The released model is available on our website: <a href="https://donghaotian123.github.io/Wan-Alpha/" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Designed VAE for alpha channel encoding into RGB latent space.
    * Constructed high-quality RGBA video dataset.
    * Used diffusion transformer for training.
    * Generated semi-transparent objects and fine details.
</details>
</details>

---


<details>
<summary><b> TempoControl: Temporal Attention Guidance for Text-to-Video Models</b></summary>

* **Authors:** Shira Schiber, Ofir Lindenbaum, Idan Schwartz
* **arXiv ID:** 2510.02226
* **One-liner:** Enabled fine-grained temporal control in text-to-video generation without retraining.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2510.02226) | [[PDF]](https://arxiv.org/pdf/2510.02226)

> **Core Innovation**
> Introduced TempoControl, using cross-attention maps and optimization principles for temporal alignment of visual concepts during inference.

<details>
    <summary>Abstract</summary>
    Recent advances in generative video models have enabled the creation of high-quality videos based on natural language prompts. However, these models frequently lack fine-grained temporal control, meaning they do not allow users to specify when particular visual elements should appear within a generated sequence. In this work, we introduce TempoControl, a method that allows for temporal alignment of visual concepts during inference, without requiring retraining or additional supervision. TempoControl utilizes cross-attention maps, a key component of text-to-video diffusion models, to guide the timing of concepts through a novel optimization approach. Our method steers attention using three complementary principles: aligning its temporal shape with a control signal (via correlation), amplifying it where visibility is needed (via energy), and maintaining spatial focus (via entropy). TempoControl allows precise control over timing while ensuring high video quality and diversity. We demonstrate its effectiveness across various video generation applications, including temporal reordering for single and multiple objects, as well as action and audio-aligned generation.
</details>

<details>
    <summary>Key points</summary>
    * Utilized cross-attention maps for timing guidance.
    * Applied optimization with correlation, energy, and entropy principles.
    * Allowed precise control over concept timing.
    * Maintained high video quality and diversity.
</details>
</details>

---


<details>
<summary><b> CamViG: Camera Aware Image-to-Video Generation with Multimodal Transformers</b></summary>

* **Authors:** Andrew Marmon, Grant Schindler, José Lezama, Dan Kondratyuk, Bryan Seybold, Irfan Essa
* **arXiv ID:** 2405.13195
* **One-liner:** Integrated 3D camera motion as a conditioning signal for video generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2405.13195) | [[PDF]](https://arxiv.org/pdf/2405.13195)

> **Core Innovation**
> Extended multimodal transformers to include 3D camera controls, enabling accurate camera path generation from a single frame and camera signal.

<details>
    <summary>Abstract</summary>
    We extend multimodal transformers to include 3D camera motion as a conditioning signal for the task of video generation. Generative video models are becoming increasingly powerful, thus focusing research efforts on methods of controlling the output of such models. We propose to add virtual 3D camera controls to generative video methods by conditioning generated video on an encoding of three-dimensional camera movement over the course of the generated video. Results demonstrate that we are (1) able to successfully control the camera during video generation, starting from a single frame and a camera signal, and (2) we demonstrate the accuracy of the generated 3D camera paths using traditional computer vision methods.
</details>

<details>
    <summary>Key points</summary>
    * Conditioned video generation on 3D camera movement encoding.
    * Used traditional computer vision methods for accuracy validation.
    * Controlled camera during generation from a single frame.
    * Demonstrated accurate 3D camera paths.
</details>
</details>

---


<details>
<summary><b> CamCo: Camera-Controllable 3D-Consistent Image-to-Video Generation</b></summary>

* **Authors:** Dejia Xu, Weili Nie, Chao Liu, Sifei Liu, Jan Kautz, Zhangyang Wang, Arash Vahdat
* **arXiv ID:** 2406.02509
* **One-liner:** Achieved fine-grained camera pose control for image-to-video generation with improved 3D consistency.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2406.02509) | [[PDF]](https://arxiv.org/pdf/2406.02509)

> **Core Innovation**
> Proposed CamCo, using Plücker coordinates for camera pose input and epipolar attention to enforce 3D constraints, fine-tuned on real-world videos.

<details>
    <summary>Abstract</summary>
    Recently video diffusion models have emerged as expressive generative tools for high-quality video content creation readily available to general users. However, these models often do not offer precise control over camera poses for video generation, limiting the expression of cinematic language and user control. To address this issue, we introduce CamCo, which allows fine-grained Camera pose Control for image-to-video generation. We equip a pre-trained image-to-video generator with accurately parameterized camera pose input using Plücker coordinates. To enhance 3D consistency in the videos produced, we integrate an epipolar attention module in each attention block that enforces epipolar constraints to the feature maps. Additionally, we fine-tune CamCo on real-world videos with camera poses estimated through structure-from-motion algorithms to better synthesize object motion. Our experiments show that CamCo significantly improves 3D consistency and camera control capabilities compared to previous models while effectively generating plausible object motion. Project page: <a href="https://ir1d.github.io/CamCo/" rel="external noopener nofollow" class="link-external link-https">this https URL</a>
</details>

<details>
    <summary>Key points</summary>
    * Used Plücker coordinates for camera pose parameterization.
    * Integrated epipolar attention module for 3D consistency.
    * Fine-tuned on real-world videos with estimated camera poses.
    * Enhanced object motion synthesis and camera control.
</details>
</details>

---


<details>
<summary><b> JVID: Joint Video-Image Diffusion for Visual-Quality and Temporal-Consistency in Video Generation</b></summary>

* **Authors:** Hadrien Reynaud, Matthew Baugh, Mischa Dombrowski, Sarah Cechnicka, Qingjie Meng, Bernhard Kainz
* **arXiv ID:** 2409.14149
* **One-liner:** Introduced JVID for high-quality, temporally coherent video generation by integrating image and video diffusion models.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2409.14149) | [[PDF]](https://arxiv.org/pdf/2409.14149)

> **Core Innovation**
> Combines Latent Image Diffusion Model (LIDM) and Latent Video Diffusion Model (LVDM) in reverse diffusion to handle spatio-temporal dynamics.

<details>
    <summary>Abstract</summary>
    We introduce the Joint Video-Image Diffusion model (JVID), a novel approach to generating high-quality and temporally coherent videos. We achieve this by integrating two diffusion models: a Latent Image Diffusion Model (LIDM) trained on images and a Latent Video Diffusion Model (LVDM) trained on video data. Our method combines these models in the reverse diffusion process, where the LIDM enhances image quality and the LVDM ensures temporal consistency. This unique combination allows us to effectively handle the complex spatio-temporal dynamics in video generation. Our results demonstrate quantitative and qualitative improvements in producing realistic and coherent videos.
</details>

<details>
    <summary>Key points</summary>
    * Integration of LIDM and LVDM
    * Reverse diffusion process for enhanced image quality and temporal consistency
    * Quantitative and qualitative improvements in video realism and coherence
</details>
</details>

---


<details>
<summary><b> FrameBridge: Improving Image-to-Video Generation with Bridge Models</b></summary>

* **Authors:** Yuji Wang, Zehua Chen, Xiaoyu Chen, Yixiang Wei, Jun Zhu, Jianfei Chen
* **arXiv ID:** 2410.15371
* **One-liner:** Proposed FrameBridge for improved image-to-video generation by modeling frame-to-frames as a data-to-data process.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2410.15371) | [[PDF]](https://arxiv.org/pdf/2410.15371)

> **Core Innovation**
> Uses bridge model to align generation process with I2V task, exploiting given image information for better consistency.

<details>
    <summary>Abstract</summary>
    Diffusion models have achieved remarkable progress on image-to-video (I2V) generation, while their noise-to-data generation process is inherently mismatched with this task, which may lead to suboptimal synthesis quality. In this work, we present FrameBridge. By modeling the frame-to-frames generation process with a bridge model based data-to-data generative process, we are able to fully exploit the information contained in the given image and improve the consistency between the generation process and I2V task. Moreover, we propose two novel techniques toward the two popular settings of training I2V models, respectively. Firstly, we propose SNR-Aligned Fine-tuning (SAF), making the first attempt to fine-tune a diffusion model to a bridge model and, therefore, allowing us to utilize the pre-trained diffusion-based text-to-video (T2V) models. Secondly, we propose neural prior, further improving the synthesis quality of FrameBridge when training from scratch. Experiments conducted on WebVid-2M and UCF-101 demonstrate the superior quality of FrameBridge in comparison with the diffusion counterpart (zero-shot FVD 95 vs. 192 on MSR-VTT and non-zero-shot FVD 122 vs. 171 on UCF-101), and the advantages of our proposed SAF and neural prior for bridge-based I2V models. The project page: <a href="https://framebridge-icml.github.io/" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Frame-to-frames bridge model
    * SNR-Aligned Fine-tuning (SAF) for fine-tuning diffusion models
    * Neural prior for training from scratch
    * Superior performance on WebVid-2M and UCF-101
</details>
</details>

---


<details>
<summary><b> TIP-I2V: A Million-Scale Real Text and Image Prompt Dataset for Image-to-Video Generation</b></summary>

* **Authors:** Wenhao Wang, Yi Yang
* **arXiv ID:** 2411.04709
* **One-liner:** Introduced TIP-I2V, the first large-scale dataset for image-to-video prompts with over 1.70 million entries.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2411.04709) | [[PDF]](https://arxiv.org/pdf/2411.04709)

> **Core Innovation**
> Provides user-provided text and image prompts and generated videos to advance I2V research and model evaluation.

<details>
    <summary>Abstract</summary>
    Video generation models are revolutionizing content creation, with image-to-video models drawing increasing attention due to their enhanced controllability, visual consistency, and practical applications. However, despite their popularity, these models rely on user-provided text and image prompts, and there is currently no dedicated dataset for studying these prompts. In this paper, we introduce TIP-I2V, the first large-scale dataset of over 1.70 million unique user-provided Text and Image Prompts specifically for Image-to-Video generation. Additionally, we provide the corresponding generated videos from five state-of-the-art image-to-video models. We begin by outlining the time-consuming and costly process of curating this large-scale dataset. Next, we compare TIP-I2V to two popular prompt datasets, VidProM (text-to-video) and DiffusionDB (text-to-image), highlighting differences in both basic and semantic information. This dataset enables advancements in image-to-video research. For instance, to develop better models, researchers can use the prompts in TIP-I2V to analyze user preferences and evaluate the multi-dimensional performance of their trained models; and to enhance model safety, they may focus on addressing the misinformation issue caused by image-to-video models. The new research inspired by TIP-I2V and the differences with existing datasets emphasize the importance of a specialized image-to-video prompt dataset. The project is available at <a href="https://tip-i2v.github.io" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Curated large-scale dataset with unique prompts
    * Comparison with VidProM and DiffusionDB
    * Enables analysis of user preferences and model safety
</details>
</details>

---


<details>
<summary><b> SG-I2V: Self-Guided Trajectory Control in Image-to-Video Generation</b></summary>

* **Authors:** Koichi Namekata, Sherwin Bahmani, Ziyi Wu, Yash Kant, Igor Gilitschenski, David B. Lindell
* **arXiv ID:** 2411.04989
* **One-liner:** Developed SG-I2V for zero-shot controllable image-to-video generation without fine-tuning.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2411.04989) | [[PDF]](https://arxiv.org/pdf/2411.04989)

> **Core Innovation**
> Relies on pre-trained model knowledge for self-guided control, improving visual quality and motion fidelity.

<details>
    <summary>Abstract</summary>
    Methods for image-to-video generation have achieved impressive, photo-realistic quality. However, adjusting specific elements in generated videos, such as object motion or camera movement, is often a tedious process of trial and error, e.g., involving re-generating videos with different random seeds. Recent techniques address this issue by fine-tuning a pre-trained model to follow conditioning signals, such as bounding boxes or point trajectories. Yet, this fine-tuning procedure can be computationally expensive, and it requires datasets with annotated object motion, which can be difficult to procure. In this work, we introduce SG-I2V, a framework for controllable image-to-video generation that is self-guided$\unicode{x2013}$offering zero-shot control by relying solely on the knowledge present in a pre-trained image-to-video diffusion model without the need for fine-tuning or external knowledge. Our zero-shot method outperforms unsupervised baselines while significantly narrowing down the performance gap with supervised models in terms of visual quality and motion fidelity. Additional details and video results are available on our project page: <a href="https://kmcode1.github.io/Projects/SG-I2V" rel="external noopener nofollow" class="link-external link-https">this https URL</a>
</details>

<details>
    <summary>Key points</summary>
    * Zero-shot control framework
    * No fine-tuning or external knowledge required
    * Outperforms unsupervised baselines and narrows gap with supervised models
</details>
</details>

---


<details>
<summary><b> OmniDrag: Enabling Motion Control for Omnidirectional Image-to-Video Generation</b></summary>

* **Authors:** Weiqi Li, Shijie Zhao, Chong Mou, Xuhan Sheng, Zhenyu Zhang, Qian Wang, Junlin Li, Li Zhang, Jian Zhang
* **arXiv ID:** 2412.09623
* **One-liner:** Proposed OmniDrag for accurate, high-quality omnidirectional image-to-video generation with motion control.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2412.09623) | [[PDF]](https://arxiv.org/pdf/2412.09623)

> **Core Innovation**
> Introduces omnidirectional control module and spherical motion estimator for handling complex spherical motions.

<details>
    <summary>Abstract</summary>
    As virtual reality gains popularity, the demand for controllable creation of immersive and dynamic omnidirectional videos (ODVs) is increasing. While previous text-to-ODV generation methods achieve impressive results, they struggle with content inaccuracies and inconsistencies due to reliance solely on textual inputs. Although recent motion control techniques provide fine-grained control for video generation, directly applying these methods to ODVs often results in spatial distortion and unsatisfactory performance, especially with complex spherical motions. To tackle these challenges, we propose OmniDrag, the first approach enabling both scene- and object-level motion control for accurate, high-quality omnidirectional image-to-video generation. Building on pretrained video diffusion models, we introduce an omnidirectional control module, which is jointly fine-tuned with temporal attention layers to effectively handle complex spherical motion. In addition, we develop a novel spherical motion estimator that accurately extracts motion-control signals and allows users to perform drag-style ODV generation by simply drawing handle and target points. We also present a new dataset, named Move360, addressing the scarcity of ODV data with large scene and object motions. Experiments demonstrate the significant superiority of OmniDrag in achieving holistic scene-level and fine-grained object-level control for ODV generation. The project page is available at <a href="https://lwq20020127.github.io/OmniDrag" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Omnidirectional control module
    * Spherical motion estimator for drag-style generation
    * Move360 dataset for ODV data
    * Joint fine-tuning with temporal attention
</details>
</details>

---


<details>
<summary><b> TIV-Diffusion: Towards Object-Centric Movement for Text-driven Image to Video Generation</b></summary>

* **Authors:** Xingrui Wang, Xin Li, Yaosi Hu, Hanxin Zhu, Chen Hou, Cuiling Lan, Zhibo Chen
* **arXiv ID:** 2412.10275
* **One-liner:** Introduced TIV-Diffusion for precise control and high-quality video generation via object-centric textual-visual alignment.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2412.10275) | [[PDF]](https://arxiv.org/pdf/2412.10275)

> **Core Innovation**
> Incorporates fused textual and visual knowledge and alignment module to ensure object consistency and motion accuracy.

<details>
    <summary>Abstract</summary>
    Text-driven Image to Video Generation (TI2V) aims to generate controllable video given the first frame and corresponding textual description. The primary challenges of this task lie in two parts: (i) how to identify the target objects and ensure the consistency between the movement trajectory and the textual description. (ii) how to improve the subjective quality of generated videos. To tackle the above challenges, we propose a new diffusion-based TI2V framework, termed TIV-Diffusion, via object-centric textual-visual alignment, intending to achieve precise control and high-quality video generation based on textual-described motion for different objects. Concretely, we enable our TIV-Diffuion model to perceive the textual-described objects and their motion trajectory by incorporating the fused textual and visual knowledge through scale-offset modulation. Moreover, to mitigate the problems of object disappearance and misaligned objects and motion, we introduce an object-centric textual-visual alignment module, which reduces the risk of misaligned objects/motion by decoupling the objects in the reference image and aligning textual features with each object individually. Based on the above innovations, our TIV-Diffusion achieves state-of-the-art high-quality video generation compared with existing TI2V methods.
</details>

<details>
    <summary>Key points</summary>
    * Object-centric textual-visual alignment
    * Scale-offset modulation for perception
    * Decoupling and aligning objects individually
    * State-of-the-art video generation quality
</details>
</details>

---


<details>
<summary><b> Through-The-Mask: Mask-based Motion Trajectories for Image-to-Video Generation</b></summary>

* **Authors:** Guy Yariv, Yuval Kirstain, Amit Zohar, Shelly Sheynin, Yaniv Taigman, Yossi Adi, Sagie Benaim, Adam Polyak
* **arXiv ID:** 2501.03059
* **One-liner:** Proposed a two-stage compositional framework for I2V generation using mask-based motion trajectory as intermediate representation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2501.03059) | [[PDF]](https://arxiv.org/pdf/2501.03059)

> **Core Innovation**
> Decomposes generation into explicit representation and video stages with object-level attention for coherence and realism.

<details>
    <summary>Abstract</summary>
    We consider the task of Image-to-Video (I2V) generation, which involves transforming static images into realistic video sequences based on a textual description. While recent advancements produce photorealistic outputs, they frequently struggle to create videos with accurate and consistent object motion, especially in multi-object scenarios. To address these limitations, we propose a two-stage compositional framework that decomposes I2V generation into: (i) An explicit intermediate representation generation stage, followed by (ii) A video generation stage that is conditioned on this representation. Our key innovation is the introduction of a mask-based motion trajectory as an intermediate representation, that captures both semantic object information and motion, enabling an expressive but compact representation of motion and semantics. To incorporate the learned representation in the second stage, we utilize object-level attention objectives. Specifically, we consider a spatial, per-object, masked-cross attention objective, integrating object-specific prompts into corresponding latent space regions and a masked spatio-temporal self-attention objective, ensuring frame-to-frame consistency for each object. We evaluate our method on challenging benchmarks with multi-object and high-motion scenarios and empirically demonstrate that the proposed method achieves state-of-the-art results in temporal coherence, motion realism, and text-prompt faithfulness. Additionally, we introduce \benchmark, a new challenging benchmark for single-object and multi-object I2V generation, and demonstrate our method&#39;s superiority on this benchmark. Project page is available at <a href="https://guyyariv.github.io/TTM/" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Two-stage framework with intermediate representation
    * Mask-based motion trajectory
    * Object-level attention objectives
    * Evaluation on multi-object and high-motion benchmarks
</details>
</details>

---


<details>
<summary><b> VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video Generation</b></summary>

* **Authors:** Sixiao Zheng, Zimian Peng, Yanpeng Zhou, Yi Zhu, Hang Xu, Xiangru Huang, Yanwei Fu
* **arXiv ID:** 2502.07531
* **One-liner:** Presented VidCRAFT3 for unified control over camera motion, object motion, and lighting direction in I2V generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2502.07531) | [[PDF]](https://arxiv.org/pdf/2502.07531)

> **Core Innovation**
> Integrates Image2Cloud, ObjMotionNet, and Spatial Triple-Attention Transformer with three-stage training strategy.

<details>
    <summary>Abstract</summary>
    Controllable image-to-video (I2V) generation transforms a reference image into a coherent video guided by user-specified control signals. In content creation workflows, precise and simultaneous control over camera motion, object motion, and lighting direction enhances both accuracy and flexibility. However, existing approaches typically treat these control signals separately, largely due to the scarcity of datasets with high-quality joint annotations and mismatched control spaces across modalities. We present VidCRAFT3, a unified and flexible I2V framework that supports both independent and joint control over camera motion, object motion, and lighting direction by integrating three core components. Image2Cloud reconstructs a 3D point cloud from the reference image to enable precise camera motion control. ObjMotionNet encodes sparse object trajectories into multi-scale optical flow features to guide object motion. The Spatial Triple-Attention Transformer integrates lighting direction embeddings via parallel cross-attention. To address the scarcity of jointly annotated data, we curate the VideoLightingDirection (VLD) dataset of synthetic static-scene video clips with per-frame lighting-direction labels, and adopt a three-stage training strategy that enables robust learning without fully joint annotations. Extensive experiments show that VidCRAFT3 outperforms existing methods in control precision and visual coherence. Code and data will be released. Project page: <a href="https://sixiaozheng.github.io/VidCRAFT3/" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Unified framework with three core components
    * Image2Cloud for 3D reconstruction
    * ObjMotionNet for object motion encoding
    * Spatial Triple-Attention for lighting control
    * VLD dataset for training
</details>
</details>

---


<details>
<summary><b> RealCam-I2V: Real-World Image-to-Video Generation with Interactive Complex Camera Control</b></summary>

* **Authors:** Teng Li, Guangcong Zheng, Rui Jiang, Shuigen Zhan, Tao Wu, Yehao Lu, Yining Lin, Chuanyun Deng, Yepan Xiong, Min Chen, Lin Cheng, Xi Li
* **arXiv ID:** 2502.10059
* **One-liner:** Developed RealCam-I2V for precise camera control in video generation using monocular depth estimation and 3D scene reconstruction.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2502.10059) | [[PDF]](https://arxiv.org/pdf/2502.10059)

> **Core Innovation**
> Enables intuitive camera trajectory drawing and scene-constrained noise shaping for improved controllability and quality.

<details>
    <summary>Abstract</summary>
    Recent advancements in camera-trajectory-guided image-to-video generation offer higher precision and better support for complex camera control compared to text-based approaches. However, they also introduce significant usability challenges, as users often struggle to provide precise camera parameters when working with arbitrary real-world images without knowledge of their depth nor scene scale. To address these real-world application issues, we propose RealCam-I2V, a novel diffusion-based video generation framework that integrates monocular metric depth estimation to establish 3D scene reconstruction in a preprocessing step. During training, the reconstructed 3D scene enables scaling camera parameters from relative to metric scales, ensuring compatibility and scale consistency across diverse real-world images. In inference, RealCam-I2V offers an intuitive interface where users can precisely draw camera trajectories by dragging within the 3D scene. To further enhance precise camera control and scene consistency, we propose scene-constrained noise shaping, which shapes high-level noise and also allows the framework to maintain dynamic and coherent video generation in lower noise stages. RealCam-I2V achieves significant improvements in controllability and video quality on the RealEstate10K and out-of-domain images. We further enables applications like camera-controlled looping video generation and generative frame interpolation. Project page: <a href="https://zgctroy.github.io/RealCam-I2V" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Integration of monocular metric depth estimation
    * 3D scene reconstruction for scale consistency
    * Scene-constrained noise shaping
    * Applications in looping video and frame interpolation
</details>
</details>

---


<details>
<summary><b> TextOCVP: Object-Centric Video Prediction with Language Guidance</b></summary>

* **Authors:** Angel Villar-Corrales, Gjergj Plepi, Sven Behnke
* **arXiv ID:** 2502.11655
* **One-liner:** Proposed TextOCVP for object-centric video prediction guided by textual descriptions, improving controllability and robustness.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2502.11655) | [[PDF]](https://arxiv.org/pdf/2502.11655)

> **Core Innovation**
> Uses object slots and text-conditioned transformer for forecasting future states with structured latent space.

<details>
    <summary>Abstract</summary>
    Understanding and forecasting future scene states is critical for autonomous agents to plan and act effectively in complex environments. Object-centric models, with structured latent spaces, have shown promise in modeling object dynamics and predicting future scene states, but often struggle to scale beyond simple synthetic datasets and to integrate external guidance, limiting their applicability in robotics. To address these limitations, we propose TextOCVP, an object-centric model for video prediction guided by textual descriptions. TextOCVP parses an observed scene into object representations, called slots, and utilizes a text-conditioned transformer predictor to forecast future object states and video frames. Our approach jointly models object dynamics and interactions while incorporating textual guidance, enabling accurate and controllable predictions. TextOCVP&#39;s structured latent space offers a more precise control of the forecasting process, outperforming several video prediction baselines on two datasets. Additionally, we show that structured object-centric representations provide superior robustness to novel scene configurations, as well as improved controllability and interpretability, enabling more precise and understandable predictions. Videos and code are available at <a href="https://play-slot.github.io/TextOCVP" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Object-centric model with slot representations
    * Text-conditioned transformer predictor
    * Structured latent space for precise control
    * Superior robustness and interpretability in predictions
</details>
</details>

---


<details>
<summary><b> Dynamic-I2V: Exploring Image-to-Video Generation Models via Multimodal LLM</b></summary>

* **Authors:** Peng Liu, Xiaoming Ren, Fengkai Liu, Qingsong Xie, Quanlong Zheng, Yanhao Zhang, Haonan Lu, Yujiu Yang
* **arXiv ID:** 2505.19901
* **One-liner:** Proposed Dynamic-I2V framework with MLLM integration for enhanced motion control and temporal coherence in I2V generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2505.19901) | [[PDF]](https://arxiv.org/pdf/2505.19901)

> **Core Innovation**
> Integrates Multimodal Large Language Models (MLLMs) to encode visual and textual conditions for a diffusion transformer, improving motion controllability and temporal coherence, and introduces DIVE benchmark for dynamic quality assessment.

<details>
    <summary>Abstract</summary>
    Recent advancements in image-to-video (I2V) generation have shown promising performance in conventional scenarios. However, these methods still encounter significant challenges when dealing with complex scenes that require a deep understanding of nuanced motion and intricate object-action relationships. To address these challenges, we present Dynamic-I2V, an innovative framework that integrates Multimodal Large Language Models (MLLMs) to jointly encode visual and textual conditions for a diffusion transformer (DiT) architecture. By leveraging the advanced multimodal understanding capabilities of MLLMs, our model significantly improves motion controllability and temporal coherence in synthesized videos. The inherent multimodality of Dynamic-I2V further enables flexible support for diverse conditional inputs, extending its applicability to various downstream generation tasks. Through systematic analysis, we identify a critical limitation in current I2V benchmarks: a significant bias towards favoring low-dynamic videos, stemming from an inadequate balance between motion complexity and visual quality metrics. To resolve this evaluation gap, we propose DIVE - a novel assessment benchmark specifically designed for comprehensive dynamic quality measurement in I2V generation. In conclusion, extensive quantitative and qualitative experiments confirm that Dynamic-I2V attains state-of-the-art performance in image-to-video generation, particularly revealing significant improvements of 42.5%, 7.9%, and 11.8% in dynamic range, controllability, and quality, respectively, as assessed by the DIVE metric in comparison to existing methods.
</details>

<details>
    <summary>Key points</summary>
    * Joint encoding of visual and textual conditions using MLLMs
    * Use of diffusion transformer (DiT) architecture
    * Introduction of DIVE benchmark for evaluation
    * Achieves significant improvements in dynamic range, controllability, and quality
</details>
</details>

---


<details>
<summary><b> MotionPro: A Precise Motion Controller for Image-to-Video Generation</b></summary>

* **Authors:** Zhongwei Zhang, Fuchen Long, Zhaofan Qiu, Yingwei Pan, Wu Liu, Ting Yao, Tao Mei
* **arXiv ID:** 2505.20287
* **One-liner:** Introduced MotionPro for precise motion control in I2V generation using region-wise trajectories and motion masks.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2505.20287) | [[PDF]](https://arxiv.org/pdf/2505.20287)

> **Core Innovation**
> Leverages region-wise trajectories and motion masks to regulate fine-grained motion synthesis and distinguish object vs. camera movement, enhancing control and realism.

<details>
    <summary>Abstract</summary>
    Animating images with interactive motion control has garnered popularity for image-to-video (I2V) generation. Modern approaches typically rely on large Gaussian kernels to extend motion trajectories as condition without explicitly defining movement region, leading to coarse motion control and failing to disentangle object and camera moving. To alleviate these, we present MotionPro, a precise motion controller that novelly leverages region-wise trajectory and motion mask to regulate fine-grained motion synthesis and identify target motion category (i.e., object or camera moving), respectively. Technically, MotionPro first estimates the flow maps on each training video via a tracking model, and then samples the region-wise trajectories to simulate inference scenario. Instead of extending flow through large Gaussian kernels, our region-wise trajectory approach enables more precise control by directly utilizing trajectories within local regions, thereby effectively characterizing fine-grained movements. A motion mask is simultaneously derived from the predicted flow maps to capture the holistic motion dynamics of the movement regions. To pursue natural motion control, MotionPro further strengthens video denoising by incorporating both region-wise trajectories and motion mask through feature modulation. More remarkably, we meticulously construct a benchmark, i.e., MC-Bench, with 1.1K user-annotated image-trajectory pairs, for the evaluation of both fine-grained and object-level I2V motion control. Extensive experiments conducted on WebVid-10M and MC-Bench demonstrate the effectiveness of MotionPro. Please refer to our project page for more results: <a href="https://zhw-zhang.github.io/MotionPro-page/" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Estimation of flow maps via tracking model
    * Use of region-wise trajectories for precise control
    * Derivation of motion masks for holistic dynamics
    * Feature modulation with trajectories and masks for denoising
</details>
</details>

---


<details>
<summary><b> Consistent Video Editing as Flow-Driven Image-to-Video Generation</b></summary>

* **Authors:** Ge Wang, Songlin Fan, Hangxu Liu, Quanjian Song, Hewei Wang, Jinfeng Xu
* **arXiv ID:** 2506.07713
* **One-liner:** Presented FlowV2V for video editing by re-investigating it as flow-driven I2V generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2506.07713) | [[PDF]](https://arxiv.org/pdf/2506.07713)

> **Core Innovation**
> Decomposes video editing into first-frame editing and conditional I2V generation, using pseudo flow sequences to ensure temporal consistency and handle shape deformation.

<details>
    <summary>Abstract</summary>
    With the prosper of video diffusion models, down-stream applications like video editing have been significantly promoted without consuming much computational cost. One particular challenge in this task lies at the motion transfer process from the source video to the edited one, where it requires the consideration of the shape deformation in between, meanwhile maintaining the temporal consistency in the generated video sequence. However, existing methods fail to model complicated motion patterns for video editing, and are fundamentally limited to object replacement, where tasks with non-rigid object motions like multi-object and portrait editing are largely neglected. In this paper, we observe that optical flows offer a promising alternative in complex motion modeling, and present FlowV2V to re-investigate video editing as a task of flow-driven Image-to-Video (I2V) generation. Specifically, FlowV2V decomposes the entire pipeline into first-frame editing and conditional I2V generation, and simulates pseudo flow sequence that aligns with the deformed shape, thus ensuring the consistency during editing. Experimental results on DAVIS-EDIT with improvements of 13.67% and 50.66% on DOVER and warping error illustrate the superior temporal consistency and sample quality of FlowV2V compared to existing state-of-the-art ones. Furthermore, we conduct comprehensive ablation studies to analyze the internal functionalities of the first-frame paradigm and flow alignment in the proposed method.
</details>

<details>
    <summary>Key points</summary>
    * Decomposition into first-frame editing and I2V generation
    * Simulation of pseudo flow sequences aligned with shape deformation
    * Use of optical flows for complex motion modeling
    * Improvements in temporal consistency and sample quality
</details>
</details>

---


<details>
<summary><b> Versatile Transition Generation with Image-to-Video Diffusion</b></summary>

* **Authors:** Zuhao Yang, Jiahui Zhang, Yingchen Yu, Shijian Lu, Song Bai
* **arXiv ID:** 2508.01698
* **One-liner:** Developed VTG framework for smooth and coherent transition video generation between frames.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2508.01698) | [[PDF]](https://arxiv.org/pdf/2508.01698)

> **Core Innovation**
> Introduces interpolation-based initialization, dual-directional motion fine-tuning, and representation alignment regularization to enhance motion smoothness and fidelity in transitions.

<details>
    <summary>Abstract</summary>
    Leveraging text, images, structure maps, or motion trajectories as conditional guidance, diffusion models have achieved great success in automated and high-quality video generation. However, generating smooth and rational transition videos given the first and last video frames as well as descriptive text prompts is far underexplored. We present VTG, a Versatile Transition video Generation framework that can generate smooth, high-fidelity, and semantically coherent video transitions. VTG introduces interpolation-based initialization that helps preserve object identity and handle abrupt content changes effectively. In addition, it incorporates dual-directional motion fine-tuning and representation alignment regularization to mitigate the limitations of pre-trained image-to-video diffusion models in motion smoothness and generation fidelity, respectively. To evaluate VTG and facilitate future studies on unified transition generation, we collected TransitBench, a comprehensive benchmark for transition generation covering two representative transition tasks: concept blending and scene transition. Extensive experiments show that VTG achieves superior transition performance consistently across all four tasks.
</details>

<details>
    <summary>Key points</summary>
    * Interpolation-based initialization for identity preservation
    * Dual-directional motion fine-tuning
    * Representation alignment regularization
    * Introduction of TransitBench for evaluation
</details>
</details>

---


<details>
<summary><b> Zero-shot 3D-Aware Trajectory-Guided image-to-video generation via Test-Time Training</b></summary>

* **Authors:** Ruicheng Zhang, Jun Zhou, Zunnan Xu, Zihao Liu, Jiehui Huang, Mingyang Zhang, Yu Sun, Xiu Li
* **arXiv ID:** 2509.06723
* **One-liner:** Proposed Zo3T, a zero-shot test-time-training framework for trajectory-guided I2V generation with 3D awareness.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2509.06723) | [[PDF]](https://arxiv.org/pdf/2509.06723)

> **Core Innovation**
> Incorporates 3D-Aware Kinematic Projection, Trajectory-Guided Test-Time LoRA, and Guidance Field Rectification to enhance motion accuracy and realism without fine-tuning.

<details>
    <summary>Abstract</summary>
    Trajectory-Guided image-to-video (I2V) generation aims to synthesize videos that adhere to user-specified motion instructions. Existing methods typically rely on computationally expensive fine-tuning on scarce annotated datasets. Although some zero-shot methods attempt to trajectory control in the latent space, they may yield unrealistic motion by neglecting 3D perspective and creating a misalignment between the manipulated latents and the network&#39;s noise predictions. To address these challenges, we introduce Zo3T, a novel zero-shot test-time-training framework for trajectory-guided generation with three core innovations: First, we incorporate a 3D-Aware Kinematic Projection, leveraging inferring scene depth to derive perspective-correct affine transformations for target regions. Second, we introduce Trajectory-Guided Test-Time LoRA, a mechanism that dynamically injects and optimizes ephemeral LoRA adapters into the denoising network alongside the latent state. Driven by a regional feature consistency loss, this co-adaptation effectively enforces motion constraints while allowing the pre-trained model to locally adapt its internal representations to the manipulated latent, thereby ensuring generative fidelity and on-manifold adherence. Finally, we develop Guidance Field Rectification, which refines the denoising evolutionary path by optimizing the conditional guidance field through a one-step lookahead strategy, ensuring efficient generative progression towards the target trajectory. Zo3T significantly enhances 3D realism and motion accuracy in trajectory-controlled I2V generation, demonstrating superior performance over existing training-based and zero-shot approaches.
</details>

<details>
    <summary>Key points</summary>
    * 3D-Aware Kinematic Projection for perspective correction
    * Trajectory-Guided Test-Time LoRA for dynamic adaptation
    * Guidance Field Rectification for efficient progression
    * Zero-shot approach with regional feature consistency
</details>
</details>

---


<details>
<summary><b> Vid-Freeze: Protecting Images from Malicious Image-to-Video Generation via Temporal Freezing</b></summary>

* **Authors:** Rohit Chowdhury, Aniruddha Bala, Rohan Jaiswal, Siddharth Roheda
* **arXiv ID:** 2509.23279
* **One-liner:** Introduced Vid-Freeze, an adversarial attack to block motion synthesis in I2V models while preserving image semantics.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2509.23279) | [[PDF]](https://arxiv.org/pdf/2509.23279)

> **Core Innovation**
> Uses attention-suppressing adversarial perturbations to disrupt motion synthesis, generating static videos to prevent malicious use of I2V generation.

<details>
    <summary>Abstract</summary>
    The rapid progress of image-to-video (I2V) generation models has introduced significant risks, enabling video synthesis from static images and facilitating deceptive or malicious content creation. While prior defenses such as I2VGuard attempt to immunize images, effective and principled protection to block motion remains underexplored. In this work, we introduce Vid-Freeze - a novel attention-suppressing adversarial attack that adds carefully crafted adversarial perturbations to images. Our method explicitly targets the attention mechanism of I2V models, completely disrupting motion synthesis while preserving semantic fidelity of the input image. The resulting immunized images generate stand-still or near-static videos, effectively blocking malicious content creation. Our experiments demonstrate the impressive protection provided by the proposed approach, highlighting the importance of attention attacks as a promising direction for robust and proactive defenses against misuse of I2V generation models.
</details>

<details>
    <summary>Key points</summary>
    * Targeting attention mechanism of I2V models
    * Addition of adversarial perturbations
    * Preservation of semantic fidelity
    * Evaluation showing effective protection
</details>
</details>

---


<details>
<summary><b> MotionRAG: Motion Retrieval-Augmented Image-to-Video Generation</b></summary>

* **Authors:** Chenhui Zhu, Yilu Wu, Shuai Wang, Gangshan Wu, Limin Wang
* **arXiv ID:** 2509.26391
* **One-liner:** Proposed MotionRAG, a retrieval-augmented framework for enhancing motion realism in video generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2509.26391) | [[PDF]](https://arxiv.org/pdf/2509.26391)

> **Core Innovation**
> Adapts motion priors from reference videos using Context-Aware Motion Adaptation (CAMA), with retrieval-based pipeline and attention-based injection for zero-shot generalization.

<details>
    <summary>Abstract</summary>
    Image-to-video generation has made remarkable progress with the advancements in diffusion models, yet generating videos with realistic motion remains highly challenging. This difficulty arises from the complexity of accurately modeling motion, which involves capturing physical constraints, object interactions, and domain-specific dynamics that are not easily generalized across diverse scenarios. To address this, we propose MotionRAG, a retrieval-augmented framework that enhances motion realism by adapting motion priors from relevant reference videos through Context-Aware Motion Adaptation (CAMA). The key technical innovations include: (i) a retrieval-based pipeline extracting high-level motion features using video encoder and specialized resamplers to distill semantic motion representations; (ii) an in-context learning approach for motion adaptation implemented through a causal transformer architecture; (iii) an attention-based motion injection adapter that seamlessly integrates transferred motion features into pretrained video diffusion models. Extensive experiments demonstrate that our method achieves significant improvements across multiple domains and various base models, all with negligible computational overhead during inference. Furthermore, our modular design enables zero-shot generalization to new domains by simply updating the retrieval database without retraining any components. This research enhances the core capability of video generation systems by enabling the effective retrieval and transfer of motion priors, facilitating the synthesis of realistic motion dynamics.
</details>

<details>
    <summary>Key points</summary>
    * Retrieval-based pipeline for motion features
    * Context-Aware Motion Adaptation (CAMA) with causal transformer
    * Attention-based motion injection adapter
    * Zero-shot generalization with modular design
</details>
</details>

---


<details>
<summary><b> Moonshot: Towards Controllable Video Generation and Editing with Multimodal Conditions</b></summary>

* **Authors:** David Junhao Zhang, Dongxu Li, Hung Le, Mike Zheng Shou, Caiming Xiong, Doyen Sahoo
* **arXiv ID:** 2401.01827
* **One-liner:** Presented Moonshot, a video generation model with multimodal conditioning on image and text inputs.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2401.01827) | [[PDF]](https://arxiv.org/pdf/2401.01827)

> **Core Innovation**
> Uses multimodal video blocks (MVB) with decoupled cross-attention for appearance conditioning and optional integration with ControlNet for geometry, improving visual quality and consistency.

<details>
    <summary>Abstract</summary>
    Most existing video diffusion models (VDMs) are limited to mere text conditions. Thereby, they are usually lacking in control over visual appearance and geometry structure of the generated videos. This work presents Moonshot, a new video generation model that conditions simultaneously on multimodal inputs of image and text. The model builts upon a core module, called multimodal video block (MVB), which consists of conventional spatialtemporal layers for representing video features, and a decoupled cross-attention layer to address image and text inputs for appearance conditioning. In addition, we carefully design the model architecture such that it can optionally integrate with pre-trained image ControlNet modules for geometry visual conditions, without needing of extra training overhead as opposed to prior methods. Experiments show that with versatile multimodal conditioning mechanisms, Moonshot demonstrates significant improvement on visual quality and temporal consistency compared to existing models. In addition, the model can be easily repurposed for a variety of generative applications, such as personalized video generation, image animation and video editing, unveiling its potential to serve as a fundamental architecture for controllable video generation. Models will be made public on <a href="https://github.com/salesforce/LAVIS" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Multimodal video block (MVB) with spatial-temporal layers
    * Decoupled cross-attention for image and text inputs
    * Optional ControlNet integration for geometry
    * Versatile applications in personalized generation and editing
</details>
</details>

---


<details>
<summary><b> LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video Editing</b></summary>

* **Authors:** Bryan Wang, Yuliang Li, Zhaoyang Lv, Haijun Xia, Yan Xu, Raj Sodhi
* **arXiv ID:** 2402.10294
* **One-liner:** Developed LAVE, an LLM-powered system to assist in video editing for beginners.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2402.10294) | [[PDF]](https://arxiv.org/pdf/2402.10294)

> **Core Innovation**
> Automatically generates language descriptions for footage, enabling LLM agents to plan and execute editing tasks based on user objectives, with flexibility for manual refinement.

<details>
    <summary>Abstract</summary>
    Video creation has become increasingly popular, yet the expertise and effort required for editing often pose barriers to beginners. In this paper, we explore the integration of large language models (LLMs) into the video editing workflow to reduce these barriers. Our design vision is embodied in LAVE, a novel system that provides LLM-powered agent assistance and language-augmented editing features. LAVE automatically generates language descriptions for the user&#39;s footage, serving as the foundation for enabling the LLM to process videos and assist in editing tasks. When the user provides editing objectives, the agent plans and executes relevant actions to fulfill them. Moreover, LAVE allows users to edit videos through either the agent or direct UI manipulation, providing flexibility and enabling manual refinement of agent actions. Our user study, which included eight participants ranging from novices to proficient editors, demonstrated LAVE&#39;s effectiveness. The results also shed light on user perceptions of the proposed LLM-assisted editing paradigm and its impact on users&#39; creativity and sense of co-creation. Based on these findings, we propose design implications to inform the future development of agent-assisted content editing.
</details>

<details>
    <summary>Key points</summary>
    * Automatic generation of language descriptions for videos
    * LLM agent for planning and executing edits
    * Support for both agent and direct UI manipulation
    * User study demonstrating effectiveness and co-creation
</details>
</details>

---


<details>
<summary><b> ExpressEdit: Video Editing with Natural Language and Sketching</b></summary>

* **Authors:** Bekzat Tilekbay, Saelyne Yang, Michal Lewkowicz, Alex Suryapranata, Juho Kim
* **arXiv ID:** 2403.17693
* **One-liner:** Introduced ExpressEdit, a multimodal system for video editing using natural language and sketching.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2403.17693) | [[PDF]](https://arxiv.org/pdf/2403.17693)

> **Core Innovation**
> Interprets temporal, spatial, and operational references from NL commands and sketching, implementing edits iteratively to enhance expression and efficiency for novices.

<details>
    <summary>Abstract</summary>
    Informational videos serve as a crucial source for explaining conceptual and procedural knowledge to novices and experts alike. When producing informational videos, editors edit videos by overlaying text/images or trimming footage to enhance the video quality and make it more engaging. However, video editing can be difficult and time-consuming, especially for novice video editors who often struggle with expressing and implementing their editing ideas. To address this challenge, we first explored how multimodality$-$natural language (NL) and sketching, which are natural modalities humans use for expression$-$can be utilized to support video editors in expressing video editing ideas. We gathered 176 multimodal expressions of editing commands from 10 video editors, which revealed the patterns of use of NL and sketching in describing edit intents. Based on the findings, we present ExpressEdit, a system that enables editing videos via NL text and sketching on the video frame. Powered by LLM and vision models, the system interprets (1) temporal, (2) spatial, and (3) operational references in an NL command and spatial references from sketching. The system implements the interpreted edits, which then the user can iterate on. An observational study (N=10) showed that ExpressEdit enhanced the ability of novice video editors to express and implement their edit ideas. The system allowed participants to perform edits more efficiently and generate more ideas by generating edits based on user&#39;s multimodal edit commands and supporting iterations on the editing commands. This work offers insights into the design of future multimodal interfaces and AI-based pipelines for video editing.
</details>

<details>
    <summary>Key points</summary>
    * Interpretation of NL and sketching for edit commands
    * Handling of temporal, spatial, and operational references
    * Iterative implementation of edits
    * Observational study showing improved efficiency and idea generation
</details>
</details>

---


<details>
<summary><b> GenVideo: One-shot Target-image and Shape Aware Video Editing using T2I Diffusion Models</b></summary>

* **Authors:** Sai Sree Harsha, Ambareesh Revanur, Dhwanit Agarwal, Shradha Agrawal
* **arXiv ID:** 2404.12541
* **One-liner:** Proposed GenVideo for precise video editing using target images, handling objects of varying shapes and sizes.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2404.12541) | [[PDF]](https://arxiv.org/pdf/2404.12541)

> **Core Innovation**
> Leverages target-image aware T2I models with novel InvEdit masks and latent noise correction for temporal consistency.

<details>
    <summary>Abstract</summary>
    Video editing methods based on diffusion models that rely solely on a text prompt for the edit are hindered by the limited expressive power of text prompts. Thus, incorporating a reference target image as a visual guide becomes desirable for precise control over edit. Also, most existing methods struggle to accurately edit a video when the shape and size of the object in the target image differ from the source object. To address these challenges, we propose &#34;GenVideo&#34; for editing videos leveraging target-image aware T2I models. Our approach handles edits with target objects of varying shapes and sizes while maintaining the temporal consistency of the edit using our novel target and shape aware InvEdit masks. Further, we propose a novel target-image aware latent noise correction strategy during inference to improve the temporal consistency of the edits. Experimental analyses indicate that GenVideo can effectively handle edits with objects of varying shapes, where existing approaches fail.
</details>

<details>
    <summary>Key points</summary>
    * Uses target-image aware T2I models
    * Employs target and shape aware InvEdit masks
    * Implements latent noise correction strategy
</details>
</details>

---


<details>
<summary><b> Edit-Your-Motion: Space-Time Diffusion Decoupling Learning for Video Motion Editing</b></summary>

* **Authors:** Yi Zuo, Lingling Li, Licheng Jiao, Fang Liu, Xu Liu, Wenping Ma, Shuyuan Yang, Yuwei Guo
* **arXiv ID:** 2405.04496
* **One-liner:** Introduced Edit-Your-Motion for human motion editing with reduced ghosting and distortion in unseen cases.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2405.04496) | [[PDF]](https://arxiv.org/pdf/2405.04496)

> **Core Innovation**
> Utilizes one-shot fine-tuning, DDIM inversion, motion attention adapter, and spatio-temporal learning strategy.

<details>
    <summary>Abstract</summary>
    Existing diffusion-based methods have achieved impressive results in human motion editing. However, these methods often exhibit significant ghosting and body distortion in unseen in-the-wild cases. In this paper, we introduce Edit-Your-Motion, a video motion editing method that tackles these challenges through one-shot fine-tuning on unseen cases. Specifically, firstly, we utilized DDIM inversion to initialize the noise, preserving the appearance of the source video and designed a lightweight motion attention adapter module to enhance motion fidelity. DDIM inversion aims to obtain the implicit representations by estimating the prediction noise from the source video, which serves as a starting point for the sampling process, ensuring the appearance consistency between the source and edited videos. The Motion Attention Module (MA) enhances the model&#39;s motion editing ability by resolving the conflict between the skeleton features and the appearance features. Secondly, to effectively decouple motion and appearance of source video, we design a spatio-temporal two-stage learning strategy (STL). In the first stage, we focus on learning temporal features of human motion and propose recurrent causal attention (RCA) to ensure consistency between video frames. In the second stage, we shift focus on learning the appearance features of the source video. With Edit-Your-Motion, users can edit the motion of humans in the source video, creating more engaging and diverse content. Extensive qualitative and quantitative experiments, along with user preference studies, show that Edit-Your-Motion outperforms other methods.
</details>

<details>
    <summary>Key points</summary>
    * Employs DDIM inversion for noise initialization
    * Uses motion attention adapter module
    * Implements spatio-temporal two-stage learning
</details>
</details>

---


<details>
<summary><b> Learning Action and Reasoning-Centric Image Editing from Videos and Simulations</b></summary>

* **Authors:** Benno Krojer, Dheeraj Vattikonda, Luis Lara, Varun Jampani, Eva Portelance, Christopher Pal, Siva Reddy
* **arXiv ID:** 2407.03471
* **One-liner:** Curated AURORA Dataset and developed a model for diverse image edits, excelling in action and reasoning tasks.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2407.03471) | [[PDF]](https://arxiv.org/pdf/2407.03471)

> **Core Innovation**
> Focuses on high-quality data with minimal changes and proposes a new automatic metric for evaluation.

<details>
    <summary>Abstract</summary>
    An image editing model should be able to perform diverse edits, ranging from object replacement, changing attributes or style, to performing actions or movement, which require many forms of reasoning. Current general instruction-guided editing models have significant shortcomings with action and reasoning-centric edits. Object, attribute or stylistic changes can be learned from visually static datasets. On the other hand, high-quality data for action and reasoning-centric edits is scarce and has to come from entirely different sources that cover e.g. physical dynamics, temporality and spatial reasoning. To this end, we meticulously curate the AURORA Dataset (Action-Reasoning-Object-Attribute), a collection of high-quality training data, human-annotated and curated from videos and simulation engines. We focus on a key aspect of quality training data: triplets (source image, prompt, target image) contain a single meaningful visual change described by the prompt, i.e., truly minimal changes between source and target images. To demonstrate the value of our dataset, we evaluate an AURORA-finetuned model on a new expert-curated benchmark (AURORA-Bench) covering 8 diverse editing tasks. Our model significantly outperforms previous editing models as judged by human raters. For automatic evaluations, we find important flaws in previous metrics and caution their use for semantically hard editing tasks. Instead, we propose a new automatic metric that focuses on discriminative understanding. We hope that our efforts : (1) curating a quality training dataset and an evaluation benchmark, (2) developing critical evaluations, and (3) releasing a state-of-the-art model, will fuel further progress on general image editing.
</details>

<details>
    <summary>Key points</summary>
    * Curates AURORA Dataset with human-annotated triplets
    * Finetunes model on AURORA for diverse edits
    * Proposes discriminative automatic metric
</details>
</details>

---


<details>
<summary><b> A Reinforcement Learning-Based Automatic Video Editing Method Using Pre-trained Vision-Language Model</b></summary>

* **Authors:** Panwen Hu, Nan Xiao, Feifei Li, Yongquan Chen, Rui Huang
* **arXiv ID:** 2411.04942
* **One-liner:** Proposed a two-stage scheme for general video editing using VLM-based context and RL-based framework.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2411.04942) | [[PDF]](https://arxiv.org/pdf/2411.04942)

> **Core Innovation**
> Leverages pre-trained VLM for editing context and RL for sequential decision-making in general scenes.

<details>
    <summary>Abstract</summary>
    In this era of videos, automatic video editing techniques attract more and more attention from industry and academia since they can reduce workloads and lower the requirements for human editors. Existing automatic editing systems are mainly scene- or event-specific, e.g., soccer game broadcasting, yet the automatic systems for general editing, e.g., movie or vlog editing which covers various scenes and events, were rarely studied before, and converting the event-driven editing method to a general scene is nontrivial. In this paper, we propose a two-stage scheme for general editing. Firstly, unlike previous works that extract scene-specific features, we leverage the pre-trained Vision-Language Model (VLM) to extract the editing-relevant representations as editing context. Moreover, to close the gap between the professional-looking videos and the automatic productions generated with simple guidelines, we propose a Reinforcement Learning (RL)-based editing framework to formulate the editing problem and train the virtual editor to make better sequential editing decisions. Finally, we evaluate the proposed method on a more general editing task with a real movie dataset. Experimental results demonstrate the effectiveness and benefits of the proposed context representation and the learning ability of our RL-based editing framework.
</details>

<details>
    <summary>Key points</summary>
    * Uses Vision-Language Model for context extraction
    * Implements Reinforcement Learning framework
    * Evaluates on real movie dataset
</details>
</details>

---


<details>
<summary><b> VideoDirector: Precise Video Editing via Text-to-Video Models</b></summary>

* **Authors:** Yukun Wang, Longguang Wang, Zhiyuan Ma, Qibin Hu, Kai Xu, Yulan Guo
* **arXiv ID:** 2411.17592
* **One-liner:** Developed VideoDirector to harness T2V models for video editing with improved temporal coherence.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2411.17592) | [[PDF]](https://arxiv.org/pdf/2411.17592)

> **Core Innovation**
> Introduces spatial-temporal decoupled guidance and multi-frame null-text optimization for precise inversion.

<details>
    <summary>Abstract</summary>
    Despite the typical inversion-then-editing paradigm using text-to-image (T2I) models has demonstrated promising results, directly extending it to text-to-video (T2V) models still suffers severe artifacts such as color flickering and content distortion. Consequently, current video editing methods primarily rely on T2I models, which inherently lack temporal-coherence generative ability, often resulting in inferior editing results. In this paper, we attribute the failure of the typical editing paradigm to: 1) Tightly Spatial-temporal Coupling. The vanilla pivotal-based inversion strategy struggles to disentangle spatial-temporal information in the video diffusion model; 2) Complicated Spatial-temporal Layout. The vanilla cross-attention control is deficient in preserving the unedited content. To address these limitations, we propose a spatial-temporal decoupled guidance (STDG) and multi-frame null-text optimization strategy to provide pivotal temporal cues for more precise pivotal inversion. Furthermore, we introduce a self-attention control strategy to maintain higher fidelity for precise partial content editing. Experimental results demonstrate that our method (termed VideoDirector) effectively harnesses the powerful temporal generation capabilities of T2V models, producing edited videos with state-of-the-art performance in accuracy, motion smoothness, realism, and fidelity to unedited content.
</details>

<details>
    <summary>Key points</summary>
    * Proposes spatial-temporal decoupled guidance
    * Uses multi-frame null-text optimization
    * Implements self-attention control
</details>
</details>

---


<details>
<summary><b> SPAgent: Adaptive Task Decomposition and Model Selection for General Video Generation and Editing</b></summary>

* **Authors:** Rong-Cheng Tu, Wenhao Sun, Zhao Jin, Jingyi Liao, Jiaxing Huang, Dacheng Tao
* **arXiv ID:** 2411.18983
* **One-liner:** Created SPAgent system for coordinating multiple models in video generation and editing tasks.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2411.18983) | [[PDF]](https://arxiv.org/pdf/2411.18983)

> **Core Innovation**
> Assembles tool library with automatic coordination via intent recognition, route planning, and model selection.

<details>
    <summary>Abstract</summary>
    While open-source video generation and editing models have made significant progress, individual models are typically limited to specific tasks, failing to meet the diverse needs of users. Effectively coordinating these models can unlock a wide range of video generation and editing capabilities. However, manual coordination is complex and time-consuming, requiring users to deeply understand task requirements and possess comprehensive knowledge of each model&#39;s performance, applicability, and limitations, thereby increasing the barrier to entry. To address these challenges, we propose a novel video generation and editing system powered by our Semantic Planning Agent (SPAgent). SPAgent bridges the gap between diverse user intents and the effective utilization of existing generative models, enhancing the adaptability, efficiency, and overall quality of video generation and editing. Specifically, the SPAgent assembles a tool library integrating state-of-the-art open-source image and video generation and editing models as tools. After fine-tuning on our manually annotated dataset, SPAgent can automatically coordinate the tools for video generation and editing, through our novelly designed three-step framework: (1) decoupled intent recognition, (2) principle-guided route planning, and (3) capability-based execution model selection. Additionally, we enhance the SPAgent&#39;s video quality evaluation capability, enabling it to autonomously assess and incorporate new video generation and editing models into its tool library without human intervention. Experimental results demonstrate that the SPAgent effectively coordinates models to generate or edit videos, highlighting its versatility and adaptability across various video tasks.
</details>

<details>
    <summary>Key points</summary>
    * Integrates models into tool library
    * Uses three-step framework for coordination
    * Enhances video quality evaluation capability
</details>
</details>

---


<details>
<summary><b> DIVE: Taming DINO for Subject-Driven Video Editing</b></summary>

* **Authors:** Yi Huang, Wei Xiong, He Zhang, Chaoqi Chen, Jianzhuang Liu, Mingfu Yan, Shifeng Chen
* **arXiv ID:** 2412.03347
* **One-liner:** Proposed DIVE for subject-driven video editing using DINOv2 features for motion consistency and identity registration.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2412.03347) | [[PDF]](https://arxiv.org/pdf/2412.03347)

> **Core Innovation**
> Leverages DINO features for motion alignment and LoRAs for subject identity learning.

<details>
    <summary>Abstract</summary>
    Building on the success of diffusion models in image generation and editing, video editing has recently gained substantial attention. However, maintaining temporal consistency and motion alignment still remains challenging. To address these issues, this paper proposes DINO-guided Video Editing (DIVE), a framework designed to facilitate subject-driven editing in source videos conditioned on either target text prompts or reference images with specific identities. The core of DIVE lies in leveraging the powerful semantic features extracted from a pretrained DINOv2 model as implicit correspondences to guide the editing process. Specifically, to ensure temporal motion consistency, DIVE employs DINO features to align with the motion trajectory of the source video. For precise subject editing, DIVE incorporates the DINO features of reference images into a pretrained text-to-image model to learn Low-Rank Adaptations (LoRAs), effectively registering the target subject&#39;s identity. Extensive experiments on diverse real-world videos demonstrate that our framework can achieve high-quality editing results with robust motion consistency, highlighting the potential of DINO to contribute to video editing. Project page: <a href="https://dino-video-editing.github.io" rel="external noopener nofollow" class="link-external link-https">this https URL</a>
</details>

<details>
    <summary>Key points</summary>
    * Uses DINOv2 features for semantic guidance
    * Employs LoRAs for identity registration
    * Ensures temporal motion consistency
</details>
</details>

---


<details>
<summary><b> Re-Attentional Controllable Video Diffusion Editing</b></summary>

* **Authors:** Yuanzhi Wang, Yong Li, Mengyi Liu, Xiaoya Zhang, Xin Liu, Zhen Cui, Antoni B. Chan
* **arXiv ID:** 2412.11710
* **One-liner:** Introduced ReAtCo method for controllable video editing with improved spatial alignment and artifact reduction.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2412.11710) | [[PDF]](https://arxiv.org/pdf/2412.11710)

> **Core Innovation**
> Uses Re-Attentional Diffusion and Invariant Region-guided Joint Sampling for fidelity and consistency.

<details>
    <summary>Abstract</summary>
    Editing videos with textual guidance has garnered popularity due to its streamlined process which mandates users to solely edit the text prompt corresponding to the source video. Recent studies have explored and exploited large-scale text-to-image diffusion models for text-guided video editing, resulting in remarkable video editing capabilities. However, they may still suffer from some limitations such as mislocated objects, incorrect number of objects. Therefore, the controllability of video editing remains a formidable challenge. In this paper, we aim to challenge the above limitations by proposing a Re-Attentional Controllable Video Diffusion Editing (ReAtCo) method. Specially, to align the spatial placement of the target objects with the edited text prompt in a training-free manner, we propose a Re-Attentional Diffusion (RAD) to refocus the cross-attention activation responses between the edited text prompt and the target video during the denoising stage, resulting in a spatially location-aligned and semantically high-fidelity manipulated video. In particular, to faithfully preserve the invariant region content with less border artifacts, we propose an Invariant Region-guided Joint Sampling (IRJS) strategy to mitigate the intrinsic sampling errors w.r.t the invariant regions at each denoising timestep and constrain the generated content to be harmonized with the invariant region content. Experimental results verify that ReAtCo consistently improves the controllability of video diffusion editing and achieves superior video editing performance.
</details>

<details>
    <summary>Key points</summary>
    * Implements Re-Attentional Diffusion
    * Uses Invariant Region-guided Joint Sampling
    * Refocuses cross-attention activations
</details>
</details>

---


<details>
<summary><b> Edit as You See: Image-guided Video Editing via Masked Motion Modeling</b></summary>

* **Authors:** Zhi-Lin Huang, Yixuan Liu, Chujun Qin, Zhongdao Wang, Dong Zhou, Dong Li, Emad Barsoum
* **arXiv ID:** 2501.04325
* **One-liner:** Developed IVEDiff for image-guided video editing with learnable motion modules and optical flow guidance.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2501.04325) | [[PDF]](https://arxiv.org/pdf/2501.04325)

> **Core Innovation**
> Incorporates masked motion modeling and motion reference network for temporal consistency.

<details>
    <summary>Abstract</summary>
    Recent advancements in diffusion models have significantly facilitated text-guided video editing. However, there is a relative scarcity of research on image-guided video editing, a method that empowers users to edit videos by merely indicating a target object in the initial frame and providing an RGB image as reference, without relying on the text prompts. In this paper, we propose a novel Image-guided Video Editing Diffusion model, termed IVEDiff for the image-guided video editing. IVEDiff is built on top of image editing models, and is equipped with learnable motion modules to maintain the temporal consistency of edited video. Inspired by self-supervised learning concepts, we introduce a masked motion modeling fine-tuning strategy that empowers the motion module&#39;s capabilities for capturing inter-frame motion dynamics, while preserving the capabilities for intra-frame semantic correlations modeling of the base image editing model. Moreover, an optical-flow-guided motion reference network is proposed to ensure the accurate propagation of information between edited video frames, alleviating the misleading effects of invalid information. We also construct a benchmark to facilitate further research. The comprehensive experiments demonstrate that our method is able to generate temporally smooth edited videos while robustly dealing with various editing objects with high quality.
</details>

<details>
    <summary>Key points</summary>
    * Builds on image editing models with motion modules
    * Uses masked motion modeling fine-tuning
    * Implements optical-flow-guided motion reference
</details>
</details>

---


<details>
<summary><b> VideoPainter: Any-length Video Inpainting and Editing with Plug-and-Play Context Control</b></summary>

* **Authors:** Yuxuan Bian, Zhaoyang Zhang, Xuan Ju, Mingdeng Cao, Liangbin Xie, Ying Shan, Qiang Xu
* **arXiv ID:** 2503.05639
* **One-liner:** Proposed VideoPainter for video inpainting with dual-stream paradigm and any-length capability.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2503.05639) | [[PDF]](https://arxiv.org/pdf/2503.05639)

> **Core Innovation**
> Uses context encoder and target region ID resampling, and establishes VPData and VPBench.

<details>
    <summary>Abstract</summary>
    Video inpainting, which aims to restore corrupted video content, has experienced substantial progress. Despite these advances, existing methods, whether propagating unmasked region pixels through optical flow and receptive field priors, or extending image-inpainting models temporally, face challenges in generating fully masked objects or balancing the competing objectives of background context preservation and foreground generation in one model, respectively. To address these limitations, we propose a novel dual-stream paradigm VideoPainter that incorporates an efficient context encoder (comprising only 6% of the backbone parameters) to process masked videos and inject backbone-aware background contextual cues to any pre-trained video DiT, producing semantically consistent content in a plug-and-play manner. This architectural separation significantly reduces the model&#39;s learning complexity while enabling nuanced integration of crucial background context. We also introduce a novel target region ID resampling technique that enables any-length video inpainting, greatly enhancing our practical applicability. Additionally, we establish a scalable dataset pipeline leveraging current vision understanding models, contributing VPData and VPBench to facilitate segmentation-based inpainting training and assessment, the largest video inpainting dataset and benchmark to date with over 390K diverse clips. Using inpainting as a pipeline basis, we also explore downstream applications including video editing and video editing pair data generation, demonstrating competitive performance and significant practical potential. Extensive experiments demonstrate VideoPainter&#39;s superior performance in both any-length video inpainting and editing, across eight key metrics, including video quality, mask region preservation, and textual coherence.
</details>

<details>
    <summary>Key points</summary>
    * Implements dual-stream paradigm with context encoder
    * Uses target region ID resampling
    * Establishes VPData and VPBench datasets
</details>
</details>

---


<details>
<summary><b> VACE: All-in-One Video Creation and Editing</b></summary>

* **Authors:** Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, Yu Liu
* **arXiv ID:** 2503.07598
* **One-liner:** VACE enables unified video creation and editing in an all-in-one framework.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2503.07598) | [[PDF]](https://arxiv.org/pdf/2503.07598)

> **Core Innovation**
> Developed a unified model for various video tasks using a Video Condition Unit and Context Adapter.

<details>
    <summary>Abstract</summary>
    Diffusion Transformer has demonstrated powerful capability and scalability in generating high-quality images and videos. Further pursuing the unification of generation and editing tasks has yielded significant progress in the domain of image content creation. However, due to the intrinsic demands for consistency across both temporal and spatial dynamics, achieving a unified approach for video synthesis remains challenging. We introduce VACE, which enables users to perform Video tasks within an All-in-one framework for Creation and Editing. These tasks include reference-to-video generation, video-to-video editing, and masked video-to-video editing. Specifically, we effectively integrate the requirements of various tasks by organizing video task inputs, such as editing, reference, and masking, into a unified interface referred to as the Video Condition Unit (VCU). Furthermore, by utilizing a Context Adapter structure, we inject different task concepts into the model using formalized representations of temporal and spatial dimensions, allowing it to handle arbitrary video synthesis tasks flexibly. Extensive experiments demonstrate that the unified model of VACE achieves performance on par with task-specific models across various subtasks. Simultaneously, it enables diverse applications through versatile task combinations. Project page: <a href="https://ali-vilab.github.io/VACE-Page/" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Organized video task inputs into a unified interface (VCU)
    * Used Context Adapter to inject task concepts with temporal and spatial representations
    * Achieved performance comparable to task-specific models
</details>
</details>

---


<details>
<summary><b> VEGGIE: Instructional Editing and Reasoning Video Concepts with Grounded Generation</b></summary>

* **Authors:** Shoubin Yu, Difan Liu, Ziqiao Ma, Yicong Hong, Yang Zhou, Hao Tan, Joyce Chai, Mohit Bansal
* **arXiv ID:** 2503.14350
* **One-liner:** VEGGIE unifies video editing, grounding, and reasoning based on user instructions.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2503.14350) | [[PDF]](https://arxiv.org/pdf/2503.14350)

> **Core Innovation**
> Introduced an end-to-end framework using MLLM for intention interpretation and diffusion for video rendering.

<details>
    <summary>Abstract</summary>
    Recent video diffusion models have enhanced video editing, but it remains challenging to handle instructional editing and diverse tasks (e.g., adding, removing, changing) within a unified framework. In this paper, we introduce VEGGIE, a Video Editor with Grounded Generation from Instructions, a simple end-to-end framework that unifies video concept editing, grounding, and reasoning based on diverse user instructions. Specifically, given a video and text query, VEGGIE first utilizes an MLLM to interpret user intentions in instructions and ground them to the video contexts, generating frame-specific grounded task queries for pixel-space responses. A diffusion model then renders these plans and generates edited videos that align with user intent. To support diverse tasks and complex instructions, we employ a curriculum learning strategy: first aligning the MLLM and video diffusion model with large-scale instructional image editing data, followed by end-to-end fine-tuning on high-quality multitask video data. Additionally, we introduce a novel data synthesis pipeline to generate paired instructional video editing data for model training. It transforms static image data into diverse, high-quality video editing samples by leveraging Image-to-Video models to inject dynamics. VEGGIE shows strong performance in instructional video editing with different editing skills, outperforming the best instructional baseline as a versatile model, while other models struggle with multi-tasking. VEGGIE also excels in video object grounding and reasoning segmentation, where other baselines fail. We further reveal how the multiple tasks help each other and highlight promising applications like zero-shot multimodal instructional and in-context video editing.
</details>

<details>
    <summary>Key points</summary>
    * Employed MLLM to interpret instructions and ground them to video contexts
    * Used curriculum learning with image editing data and video fine-tuning
    * Developed a data synthesis pipeline for instructional video editing data
</details>
</details>

---


<details>
<summary><b> InstructVEdit: A Holistic Approach for Instructional Video Editing</b></summary>

* **Authors:** Chi Zhang, Chengjian Feng, Feng Yan, Qiming Zhang, Mingjin Zhang, Yujie Zhong, Jing Zhang, Lin Ma
* **arXiv ID:** 2503.17641
* **One-liner:** InstructVEdit provides a full-cycle approach for instructional video editing with improved data and model strategies.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2503.17641) | [[PDF]](https://arxiv.org/pdf/2503.17641)

> **Core Innovation**
> Established a dataset curation workflow and model improvements for better edit quality and temporal consistency.

<details>
    <summary>Abstract</summary>
    Video editing according to instructions is a highly challenging task due to the difficulty in collecting large-scale, high-quality edited video pair data. This scarcity not only limits the availability of training data but also hinders the systematic exploration of model architectures and training strategies. While prior work has improved specific aspects of video editing (e.g., synthesizing a video dataset using image editing techniques or decomposed video editing training), a holistic framework addressing the above challenges remains underexplored. In this study, we introduce InstructVEdit, a full-cycle instructional video editing approach that: (1) establishes a reliable dataset curation workflow to initialize training, (2) incorporates two model architectural improvements to enhance edit quality while preserving temporal consistency, and (3) proposes an iterative refinement strategy leveraging real-world data to enhance generalization and minimize train-test discrepancies. Extensive experiments show that InstructVEdit achieves state-of-the-art performance in instruction-based video editing, demonstrating robust adaptability to diverse real-world scenarios. Project page: <a href="https://o937-blip.github.io/InstructVEdit" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Created a reliable dataset curation workflow
    * Incorporated architectural improvements for edit quality and temporal consistency
    * Proposed iterative refinement strategy with real-world data
</details>
</details>

---


<details>
<summary><b> OmniV2V: Versatile Video Generation and Editing via Dynamic Content Manipulation</b></summary>

* **Authors:** Sen Liang, Zhentao Yu, Zhengguang Zhou, Teng Hu, Hongmei Wang, Yi Chen, Qin Lin, Yuan Zhou, Xin Li, Qinglin Lu, Zhibo Chen
* **arXiv ID:** 2506.01801
* **One-liner:** OmniV2V enables diverse video generation and editing across multiple scenarios.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2506.01801) | [[PDF]](https://arxiv.org/pdf/2506.01801)

> **Core Innovation**
> Designed a unified model with dynamic content manipulation and visual-text instruction modules.

<details>
    <summary>Abstract</summary>
    The emergence of Diffusion Transformers (DiT) has brought significant advancements to video generation, especially in text-to-video and image-to-video tasks. Although video generation is widely applied in various fields, most existing models are limited to single scenarios and cannot perform diverse video generation and editing through dynamic content manipulation. We propose OmniV2V, a video model capable of generating and editing videos across different scenarios based on various operations, including: object movement, object addition, mask-guided video edit, try-on, inpainting, outpainting, human animation, and controllable character video synthesis. We explore a unified dynamic content manipulation injection module, which effectively integrates the requirements of the above tasks. In addition, we design a visual-text instruction module based on LLaVA, enabling the model to effectively understand the correspondence between visual content and instructions. Furthermore, we build a comprehensive multi-task data processing system. Since there is data overlap among various tasks, this system can efficiently provide data augmentation. Using this system, we construct a multi-type, multi-scenario OmniV2V dataset and its corresponding OmniV2V-Test benchmark. Extensive experiments show that OmniV2V works as well as, and sometimes better than, the best existing open-source and commercial models for many video generation and editing tasks.
</details>

<details>
    <summary>Key points</summary>
    * Developed a unified dynamic content manipulation injection module
    * Used LLaVA-based visual-text instruction module for understanding
    * Built a multi-task data processing system for data augmentation
</details>
</details>

---


<details>
<summary><b> Yan: Foundational Interactive Video Generation</b></summary>

* **Authors:** Deheng Ye, Fangyun Zhou, Jiacheng Lv, Jianqi Ma, Jun Zhang, Junyan Lv, Junyou Li, Minwen Deng, Mingyu Yang, Qiang Fu, Wei Yang, Wenkai Lv, Yangbin Yu, Yewen Wang, Yonghang Guan, Zhihao Hu, Zhongbin Fang, Zhongqian Sun
* **arXiv ID:** 2508.08601
* **One-liner:** Yan integrates simulation, generation, and editing for interactive video creation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2508.08601) | [[PDF]](https://arxiv.org/pdf/2508.08601)

> **Core Innovation**
> Combined real-time simulation, multi-modal generation, and multi-granularity editing in a foundational framework.

<details>
    <summary>Abstract</summary>
    We present Yan, a foundational framework for interactive video generation, covering the entire pipeline from simulation and generation to editing. Specifically, Yan comprises three core modules. AAA-level Simulation: We design a highly-compressed, low-latency 3D-VAE coupled with a KV-cache-based shift-window denoising inference process, achieving real-time 1080P/60FPS interactive simulation. Multi-Modal Generation: We introduce a hierarchical autoregressive caption method that injects game-specific knowledge into open-domain multi-modal video diffusion models (VDMs), then transforming the VDM into a frame-wise, action-controllable, real-time infinite interactive video generator. Notably, when the textual and visual prompts are sourced from different domains, the model demonstrates strong generalization, allowing it to blend and compose the style and mechanics across domains flexibly according to user prompts. Multi-Granularity Editing: We propose a hybrid model that explicitly disentangles interactive mechanics simulation from visual rendering, enabling multi-granularity video content editing during interaction through text. Collectively, Yan offers an integration of these modules, pushing interactive video generation beyond isolated capabilities toward a comprehensive AI-driven interactive creation paradigm, paving the way for the next generation of creative tools, media, and entertainment. The project page is: <a href="https://greatx3.github.io/Yan/" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Implemented AAA-level simulation with 3D-VAE and KV-cache denoising
    * Introduced hierarchical autoregressive caption for multi-modal generation
    * Proposed hybrid model for disentangling mechanics and rendering in editing
</details>
</details>

---


<details>
<summary><b> EditDuet: A Multi-Agent System for Video Non-Linear Editing</b></summary>

* **Authors:** Marcelo Sandoval-Castaneda, Bryan Russell, Josef Sivic, Gregory Shakhnarovich, Fabian Caba Heilbron
* **arXiv ID:** 2509.10761
* **One-liner:** Automated video editing via multi-agent sequential decision making.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2509.10761) | [[PDF]](https://arxiv.org/pdf/2509.10761)

> **Core Innovation**
> Formulated video editing as a sequential process with Editor and Critic agents.

<details>
    <summary>Abstract</summary>
    Automated tools for video editing and assembly have applications ranging from filmmaking and advertisement to content creation for social media. Previous video editing work has mainly focused on either retrieval or user interfaces, leaving actual editing to the user. In contrast, we propose to automate the core task of video editing, formulating it as sequential decision making process. Ours is a multi-agent approach. We design an Editor agent and a Critic agent. The Editor takes as input a collection of video clips together with natural language instructions and uses tools commonly found in video editing software to produce an edited sequence. On the other hand, the Critic gives natural language feedback to the editor based on the produced sequence or renders it if it is satisfactory. We introduce a learning-based approach for enabling effective communication across specialized agents to address the language-driven video editing task. Finally, we explore an LLM-as-a-judge metric for evaluating the quality of video editing system and compare it with general human preference. We evaluate our system&#39;s output video sequences qualitatively and quantitatively through a user study and find that our system vastly outperforms existing approaches in terms of coverage, time constraint satisfaction, and human preference.
</details>

<details>
    <summary>Key points</summary>
    * Designed Editor agent using tools for video editing
    * Used Critic agent for feedback and rendering
    * Employed learning-based approach for agent communication
</details>
</details>

---


<details>
<summary><b> Prompt-Driven Agentic Video Editing System: Autonomous Comprehension of Long-Form, Story-Driven Media</b></summary>

* **Authors:** Zihan Ding, Xinyi Wang, Junlong Chen, Per Ola Kristensson, Junxiao Shen
* **arXiv ID:** 2509.16811
* **One-liner:** Modular system for prompt-driven editing of long-form narrative videos.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2509.16811) | [[PDF]](https://arxiv.org/pdf/2509.16811)

> **Core Innovation**
> Developed a semantic indexing pipeline for global narrative and transparent editing.

<details>
    <summary>Abstract</summary>
    Creators struggle to edit long-form, narrative-rich videos not because of UI complexity, but due to the cognitive demands of searching, storyboarding, and sequencing hours of footage. Existing transcript- or embedding-based methods fall short for creative workflows, as models struggle to track characters, infer motivations, and connect dispersed events. We present a prompt-driven, modular editing system that helps creators restructure multi-hour content through free-form prompts rather than timelines. At its core is a semantic indexing pipeline that builds a global narrative via temporal segmentation, guided memory compression, and cross-granularity fusion, producing interpretable traces of plot, dialogue, emotion, and context. Users receive cinematic edits while optionally refining transparent intermediate outputs. Evaluated on 400+ videos with expert ratings, QA, and preference studies, our system scales prompt-driven editing, preserves narrative coherence, and balances automation with creator control.
</details>

<details>
    <summary>Key points</summary>
    * Built semantic indexing with temporal segmentation and memory compression
    * Enabled cross-granularity fusion for plot and context traces
    * Allowed user refinement of intermediate outputs
</details>
</details>

---


<details>
<summary><b> EditVerse: Unifying Image and Video Editing and Generation with In-Context Learning</b></summary>

* **Authors:** Xuan Ju, Tianyu Wang, Yuqian Zhou, He Zhang, Qing Liu, Nanxuan Zhao, Zhifei Zhang, Yijun Li, Yuanhao Cai, Shaoteng Liu, Daniil Pakhomov, Zhe Lin, Soo Ye Kim, Qiang Xu
* **arXiv ID:** 2509.20360
* **One-liner:** EditVerse unifies image and video generation and editing in a single model.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2509.20360) | [[PDF]](https://arxiv.org/pdf/2509.20360)

> **Core Innovation**
> Represented modalities as unified token sequences for cross-modal tasks.

<details>
    <summary>Abstract</summary>
    Recent advances in foundation models highlight a clear trend toward unification and scaling, showing emergent capabilities across diverse domains. While image generation and editing have rapidly transitioned from task-specific to unified frameworks, video generation and editing remain fragmented due to architectural limitations and data scarcity. In this work, we introduce EditVerse, a unified framework for image and video generation and editing within a single model. By representing all modalities, i.e., text, image, and video, as a unified token sequence, EditVerse leverages self-attention to achieve robust in-context learning, natural cross-modal knowledge transfer, and flexible handling of inputs and outputs with arbitrary resolutions and durations. To address the lack of video editing training data, we design a scalable data pipeline that curates 232K video editing samples and combines them with large-scale image and video datasets for joint training. Furthermore, we present EditVerseBench, the first benchmark for instruction-based video editing covering diverse tasks and resolutions. Extensive experiments and user studies demonstrate that EditVerse achieves state-of-the-art performance, surpassing existing open-source and commercial models, while exhibiting emergent editing and generation abilities across modalities.
</details>

<details>
    <summary>Key points</summary>
    * Used unified token sequence representation for all modalities
    * Designed scalable data pipeline for video editing samples
    * Created EditVerseBench benchmark for video editing
</details>
</details>

---


<details>
<summary><b> ERNIE-ViLG: Unified Generative Pre-training for Bidirectional Vision-Language Generation</b></summary>

* **Authors:** Han Zhang, Weichong Yin, Yewei Fang, Lanxin Li, Boqiang Duan, Zhihua Wu, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang
* **arXiv ID:** 2112.15283
* **One-liner:** ERNIE-ViLG enables bidirectional image-text generation with unified pre-training.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2112.15283) | [[PDF]](https://arxiv.org/pdf/2112.15283)

> **Core Innovation**
> Formulated image and text generation as autoregressive tasks for semantic alignment.

<details>
    <summary>Abstract</summary>
    Conventional methods for the image-text generation tasks mainly tackle the naturally bidirectional generation tasks separately, focusing on designing task-specific frameworks to improve the quality and fidelity of the generated samples. Recently, Vision-Language Pre-training models have greatly improved the performance of the image-to-text generation tasks, but large-scale pre-training models for text-to-image synthesis task are still under-developed. In this paper, we propose ERNIE-ViLG, a unified generative pre-training framework for bidirectional image-text generation with transformer model. Based on the image quantization models, we formulate both image generation and text generation as autoregressive generative tasks conditioned on the text/image input. The bidirectional image-text generative modeling eases the semantic alignments across vision and language. For the text-to-image generation process, we further propose an end-to-end training method to jointly learn the visual sequence generator and the image reconstructor. To explore the landscape of large-scale pre-training for bidirectional text-image generation, we train a 10-billion parameter ERNIE-ViLG model on a large-scale dataset of 145 million (Chinese) image-text pairs which achieves state-of-the-art performance for both text-to-image and image-to-text tasks, obtaining an FID of 7.9 on MS-COCO for text-to-image synthesis and best results on COCO-CN and AIC-ICC for image captioning.
</details>

<details>
    <summary>Key points</summary>
    * Modeled bidirectional generation as autoregressive tasks
    * Proposed end-to-end training for text-to-image synthesis
    * Trained on large-scale dataset for state-of-the-art performance
</details>
</details>

---


<details>
<summary><b> Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks</b></summary>

* **Authors:** Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, Aniruddha Kembhavi
* **arXiv ID:** 2206.08916
* **One-liner:** Unified-IO performs diverse AI tasks across vision and language with a single model.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2206.08916) | [[PDF]](https://arxiv.org/pdf/2206.08916)

> **Core Innovation**
> Homogenized inputs and outputs into token sequences for unified transformer training.

<details>
    <summary>Abstract</summary>
    We propose Unified-IO, a model that performs a large variety of AI tasks spanning classical computer vision tasks, including pose estimation, object detection, depth estimation and image generation, vision-and-language tasks such as region captioning and referring expression, to natural language processing tasks such as question answering and paraphrasing. Developing a single unified model for such a large variety of tasks poses unique challenges due to the heterogeneous inputs and outputs pertaining to each task, including RGB images, per-pixel maps, binary masks, bounding boxes, and language. We achieve this unification by homogenizing every supported input and output into a sequence of discrete vocabulary tokens. This common representation across all tasks allows us to train a single transformer-based architecture, jointly on over 90 diverse datasets in the vision and language fields. Unified-IO is the first model capable of performing all 7 tasks on the GRIT benchmark and produces strong results across 16 diverse benchmarks like NYUv2-Depth, ImageNet, VQA2.0, OK-VQA, Swig, VizWizGround, BoolQ, and SciTail, with no task-specific fine-tuning. Code and demos for Unified-IO are available at: <a href="https://unified-io.allenai.org" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Represented all inputs and outputs as discrete token sequences
    * Trained on over 90 diverse datasets
    * Achieved strong results on multiple benchmarks without fine-tuning
</details>
</details>

---


<details>
<summary><b> Grounding Language Models to Images for Multimodal Inputs and Outputs</b></summary>

* **Authors:** Jing Yu Koh, Ruslan Salakhutdinov, Daniel Fried
* **arXiv ID:** 2301.13823
* **One-liner:** Enabled text-only language models to process and generate interleaved image-text data.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2301.13823) | [[PDF]](https://arxiv.org/pdf/2301.13823)

> **Core Innovation**
> Grounded pretrained text-only language models to the visual domain by finetuning input/output linear layers while keeping the language model frozen.

<details>
    <summary>Abstract</summary>
    We propose an efficient method to ground pretrained text-only language models to the visual domain, enabling them to process arbitrarily interleaved image-and-text data, and generate text interleaved with retrieved images. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and finetune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text interleaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contextual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained language models in visually grounded settings.
</details>

<details>
    <summary>Key points</summary>
    * Leveraged in-context learning and free-form text generation from large-scale text pretraining
    * Froze the language model and finetuned linear layers for cross-modality interactions
    * Achieved zero-shot performance on grounded tasks like contextual image retrieval and multimodal dialogue
</details>
</details>

---


<details>
<summary><b> Any-to-Any Generation via Composable Diffusion</b></summary>

* **Authors:** Zineng Tang, Ziyi Yang, Chenguang Zhu, Michael Zeng, Mohit Bansal
* **arXiv ID:** 2305.11846
* **One-liner:** Developed a generative model that can generate any combination of output modalities from any input modalities.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2305.11846) | [[PDF]](https://arxiv.org/pdf/2305.11846)

> **Core Innovation**
> Introduced Composable Diffusion (CoDi) for parallel generation of multiple modalities by aligning them in input and output spaces.

<details>
    <summary>Abstract</summary>
    We present Composable Diffusion (CoDi), a novel generative model capable of generating any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities. Unlike existing generative AI systems, CoDi can generate multiple modalities in parallel and its input is not limited to a subset of modalities like text or image. Despite the absence of training datasets for many combinations of modalities, we propose to align modalities in both the input and output space. This allows CoDi to freely condition on any input combination and generate any group of modalities, even if they are not present in the training data. CoDi employs a novel composable generation strategy which involves building a shared multimodal space by bridging alignment in the diffusion process, enabling the synchronized generation of intertwined modalities, such as temporally aligned video and audio. Highly customizable and flexible, CoDi achieves strong joint-modality generation quality, and outperforms or is on par with the unimodal state-of-the-art for single-modality synthesis. The project page with demonstrations and code is at <a href="https://codi-gen.github.io" rel="external noopener nofollow" class="link-external link-https">this https URL</a>
</details>

<details>
    <summary>Key points</summary>
    * Aligned modalities in input and output spaces to handle unseen combinations
    * Used a composable generation strategy with a shared multimodal space
    * Achieved strong joint-modality generation quality and competitive single-modality synthesis
</details>
</details>

---


<details>
<summary><b> Planting a SEED of Vision in Large Language Model</b></summary>

* **Authors:** Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, Ying Shan
* **arXiv ID:** 2307.08041
* **One-liner:** Empowered LLMs with the ability to see and draw using a discrete image tokenizer.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2307.08041) | [[PDF]](https://arxiv.org/pdf/2307.08041)

> **Core Innovation**
> Designed SEED, an image tokenizer with 1D causal dependency and high-level semantics alignment for unified multimodal training.

<details>
    <summary>Abstract</summary>
    We present SEED, an elaborate image tokenizer that empowers Large Language Models (LLMs) with the emergent ability to SEE and Draw at the same time. Research on image tokenizers has previously reached an impasse, as frameworks employing quantized visual tokens have lost prominence due to subpar performance and convergence in multimodal comprehension (compared to BLIP-2, etc.) or generation (compared to Stable Diffusion, etc.). Despite the limitations, we remain confident in its natural capacity to unify visual and textual representations, facilitating scalable multimodal training with LLM&#39;s original recipe. In this study, we identify two crucial principles for the architecture and training of SEED that effectively ease subsequent alignment with LLMs. (1) Image tokens should be independent of 2D physical patch positions and instead be produced with a 1D causal dependency, exhibiting intrinsic interdependence that aligns with the left-to-right autoregressive prediction mechanism in LLMs. (2) Image tokens should capture high-level semantics consistent with the degree of semantic abstraction in words, and be optimized for both discriminativeness and reconstruction during the tokenizer training phase. As a result, the off-the-shelf LLM is able to perform both image-to-text and text-to-image generation by incorporating our SEED through efficient LoRA tuning. Comprehensive multimodal pretraining and instruction tuning, which may yield improved results, are reserved for future investigation. This version of SEED was trained in 5.7 days using only 64 V100 GPUs and 5M publicly available image-text pairs. Our preliminary study emphasizes the great potential of discrete visual tokens in versatile multimodal LLMs and the importance of proper image tokenizers in broader research.
</details>

<details>
    <summary>Key points</summary>
    * Made image tokens independent of 2D positions with 1D causal dependency
    * Optimized tokens for discriminativeness and reconstruction
    * Enabled image-to-text and text-to-image generation via efficient LoRA tuning
</details>
</details>

---


<details>
<summary><b> Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization</b></summary>

* **Authors:** Yang Jin, Kun Xu, Kun Xu, Liwei Chen, Chao Liao, Jianchao Tan, Quzhe Huang, Bin Chen, Chenyi Lei, An Liu, Chengru Song, Xiaoqiang Lei, Di Zhang, Wenwu Ou, Kun Gai, Yadong Mu
* **arXiv ID:** 2309.04669
* **One-liner:** Unified vision and language representation for generative multimodal tasks.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2309.04669) | [[PDF]](https://arxiv.org/pdf/2309.04669)

> **Core Innovation**
> Introduced LaVIT, a foundation model that handles image and text indiscriminately under a generative learning paradigm.

<details>
    <summary>Abstract</summary>
    Recently, the remarkable advance of the Large Language Model (LLM) has inspired researchers to transfer its extraordinary reasoning capability to both vision and language data. However, the prevailing approaches primarily regard the visual input as a prompt and focus exclusively on optimizing the text generation process conditioned upon vision content by a frozen LLM. Such an inequitable treatment of vision and language heavily constrains the model&#39;s potential. In this paper, we break through this limitation by representing both vision and language in a unified form. Specifically, we introduce a well-designed visual tokenizer to translate the non-linguistic image into a sequence of discrete tokens like a foreign language that LLM can read. The resulting visual tokens encompass high-level semantics worthy of a word and also support dynamic sequence length varying from the image. Coped with this tokenizer, the presented foundation model called LaVIT can handle both image and text indiscriminately under the same generative learning paradigm. This unification empowers LaVIT to serve as an impressive generalist interface to understand and generate multi-modal content simultaneously. Extensive experiments further showcase that it outperforms the existing models by a large margin on massive vision-language tasks. Our code and models are available at <a href="https://github.com/jy0205/LaVIT" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Used a visual tokenizer to convert images into discrete tokens like a foreign language
    * Enabled unified generative learning for both modalities
    * Achieved superior performance on various vision-language tasks
</details>
</details>

---


<details>
<summary><b> VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation</b></summary>

* **Authors:** Jinguo Zhu, Xiaohan Ding, Yixiao Ge, Yuying Ge, Sijie Zhao, Hengshuang Zhao, Xiaohua Wang, Ying Shan
* **arXiv ID:** 2312.09251
* **One-liner:** Created a transformer model for concurrent perception and generation of visual and linguistic data.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2312.09251) | [[PDF]](https://arxiv.org/pdf/2312.09251)

> **Core Innovation**
> Developed VL-GPT with a unified auto-regressive objective for multimodal pre-training using an image tokenizer-detokenizer framework.

<details>
    <summary>Abstract</summary>
    In this work, we introduce Vision-Language Generative Pre-trained Transformer (VL-GPT), a transformer model proficient at concurrently perceiving and generating visual and linguistic data. VL-GPT achieves a unified pre-training approach for both image and text modalities by employing a straightforward auto-regressive objective, thereby enabling the model to process image and text as seamlessly as a language model processes text. To accomplish this, we initially propose a novel image tokenizer-detokenizer framework for visual data, specifically designed to transform raw images into a sequence of continuous embeddings and reconstruct them accordingly. In combination with the existing text tokenizer and detokenizer, this framework allows for the encoding of interleaved image-text data into a multimodal sequence, which can subsequently be fed into the transformer model. Consequently, VL-GPT can perform large-scale pre-training on multimodal corpora utilizing a unified auto-regressive objective (i.e., next-token prediction). Upon completion of pre-training, VL-GPT exhibits remarkable zero-shot and few-shot performance across a diverse range of vision and language understanding and generation tasks, including image captioning, visual question answering, text-to-image generation, and more. Additionally, the pre-trained model retrains in-context learning capabilities when provided with multimodal prompts. We further conduct instruction tuning on our VL-GPT, highlighting its exceptional potential for multimodal assistance. The source code and model weights shall be released.
</details>

<details>
    <summary>Key points</summary>
    * Proposed an image tokenizer-detokenizer for continuous embeddings
    * Used unified auto-regressive pre-training on multimodal corpora
    * Demonstrated zero-shot and few-shot capabilities across vision-language tasks
</details>
</details>

---


<details>
<summary><b> Unifying Generation and Compression: Ultra-low bitrate Image Coding Via Multi-stage Transformer</b></summary>

* **Authors:** Naifu Xue, Qi Mao, Zijian Wang, Yuan Zhang, Siwei Ma
* **arXiv ID:** 2403.03736
* **One-liner:** Merged image generation and compression for ultra-low bitrate scenarios.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2403.03736) | [[PDF]](https://arxiv.org/pdf/2403.03736)

> **Core Innovation**
> Introduced the Unified Image Generation-Compression (UIGC) paradigm using VQ tokenization and multi-stage transformers.

<details>
    <summary>Abstract</summary>
    Recent progress in generative compression technology has significantly improved the perceptual quality of compressed data. However, these advancements primarily focus on producing high-frequency details, often overlooking the ability of generative models to capture the prior distribution of image content, thus impeding further bitrate reduction in extreme compression scenarios (&lt;0.05 bpp). Motivated by the capabilities of predictive language models for lossless compression, this paper introduces a novel Unified Image Generation-Compression (UIGC) paradigm, merging the processes of generation and compression. A key feature of the UIGC framework is the adoption of vector-quantized (VQ) image models for tokenization, alongside a multi-stage transformer designed to exploit spatial contextual information for modeling the prior distribution. As such, the dual-purpose framework effectively utilizes the learned prior for entropy estimation and assists in the regeneration of lost tokens. Extensive experiments demonstrate the superiority of the proposed UIGC framework over existing codecs in perceptual quality and human perception, particularly in ultra-low bitrate scenarios (&lt;=0.03 bpp), pioneering a new direction in generative compression.
</details>

<details>
    <summary>Key points</summary>
    * Adopted vector-quantized image models for tokenization
    * Used multi-stage transformers to model prior distribution
    * Achieved superior perceptual quality in ultra-low bitrate compression
</details>
</details>

---


<details>
<summary><b> In-Context Translation: Towards Unifying Image Recognition, Processing, and Generation</b></summary>

* **Authors:** Han Xue, Qianru Sun, Li Song, Wenjun Zhang, Zhiwu Huang
* **arXiv ID:** 2404.09633
* **One-liner:** Unified diverse vision tasks into a single framework using in-context learning.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2404.09633) | [[PDF]](https://arxiv.org/pdf/2404.09633)

> **Core Innovation**
> Proposed In-Context Translation (ICT) to standardize input-output as RGB image pairs and train with in-context learning.

<details>
    <summary>Abstract</summary>
    We propose In-Context Translation (ICT), a general learning framework to unify visual recognition (e.g., semantic segmentation), low-level image processing (e.g., denoising), and conditional image generation (e.g., edge-to-image synthesis). Thanks to unification, ICT significantly reduces the inherent inductive bias that comes with designing models for specific tasks, and it maximizes mutual enhancement across similar tasks. However, the unification across a large number of tasks is non-trivial due to various data formats and training pipelines. To this end, ICT introduces two designs. Firstly, it standardizes input-output data of different tasks into RGB image pairs, e.g., semantic segmentation data pairs an RGB image with its segmentation mask in the same RGB format. This turns different tasks into a general translation task between two RGB images. Secondly, it standardizes the training of different tasks into a general in-context learning, where &#34;in-context&#34; means the input comprises an example input-output pair of the target task and a query image. The learning objective is to generate the &#34;missing&#34; data paired with the query. The implicit translation process is thus between the query and the generated image. In experiments, ICT unifies ten vision tasks and showcases impressive performance on their respective benchmarks. Notably, ICT performs well across three major categories of computer vision tasks, while its two competitors (Painter and PromptDiffusion) are only effective in at most two of these task categories. In addition, compared to its competitors, ICT trained on only 4 RTX 3090 GPUs is shown to be more efficient and less costly in training.
</details>

<details>
    <summary>Key points</summary>
    * Standardized tasks into RGB image pairs for general translation
    * Used in-context learning with example pairs and query images
    * Unified ten vision tasks and showed efficient training
</details>
</details>

---


<details>
<summary><b> SEED-X: Multimodal Models with Unified Multi-granularity Comprehension and Generation</b></summary>

* **Authors:** Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, Ying Shan
* **arXiv ID:** 2404.14396
* **One-liner:** Enhanced multimodal foundation model for real-world applicability with arbitrary image sizes and multi-granularity generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2404.14396) | [[PDF]](https://arxiv.org/pdf/2404.14396)

> **Core Innovation**
> Developed SEED-X to model multi-granularity visual semantics for comprehension and generation tasks.

<details>
    <summary>Abstract</summary>
    The rapid evolution of multimodal foundation model has demonstrated significant progresses in vision-language understanding and generation, e.g., our previous work SEED-LLaMA. However, there remains a gap between its capability and the real-world applicability, primarily due to the model&#39;s limited capacity to effectively respond to various user instructions and interact with diverse visual data. In this work, we focus on bridging this gap through integrating two enhanced features: (1) comprehending images of arbitrary sizes and ratios, and (2) enabling multi-granularity image generation. We present a unified and versatile foundation model, namely, SEED-X, which is able to model multi-granularity visual semantics for comprehension and generation tasks. Besides the competitive results on public benchmarks, SEED-X demonstrates its effectiveness in handling real-world applications across various domains after instruction tuning. We hope that our work will inspire future research into what can be achieved by versatile multimodal foundation models in real-world applications. The models, codes, and datasets are released in <a href="https://github.com/AILab-CVC/SEED-X" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Enabled comprehension of arbitrary image sizes and ratios
    * Supported multi-granularity image generation
    * Achieved effectiveness in real-world applications after instruction tuning
</details>
</details>

---


<details>
<summary><b> Chameleon: Mixed-Modal Early-Fusion Foundation Models</b></summary>

* **Authors:** Chameleon Team
* **arXiv ID:** 2405.09818
* **One-liner:** Built a mixed-modal model capable of understanding and generating images and text in any sequence.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2405.09818) | [[PDF]](https://arxiv.org/pdf/2405.09818)

> **Core Innovation**
> Introduced Chameleon with early-fusion token-based architecture and stable training for unified multimodal modeling.

<details>
    <summary>Abstract</summary>
    We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model. It also matches or exceeds the performance of much larger models, including Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal generation evaluation, where either the prompt or outputs contain mixed sequences of both images and text. Chameleon marks a significant step forward in a unified modeling of full multimodal documents.
</details>

<details>
    <summary>Key points</summary>
    * Used early-fusion token-based architecture for mixed modalities
    * Developed stable training and alignment recipes
    * Demonstrated state-of-the-art performance in various tasks including long-form mixed-modal generation
</details>
</details>

---


<details>
<summary><b> Libra: Building Decoupled Vision System on Large Language Models</b></summary>

* **Authors:** Yifan Xu, Xiaoshan Yang, Yaguang Song, Changsheng Xu
* **arXiv ID:** 2405.10140
* **One-liner:** Designed a decoupled vision system for effective multimodal comprehension with LLMs.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2405.10140) | [[PDF]](https://arxiv.org/pdf/2405.10140)

> **Core Innovation**
> Created Libra with a routed visual expert and cross-modal bridge module for discrete auto-regressive modeling.

<details>
    <summary>Abstract</summary>
    In this work, we introduce Libra, a prototype model with a decoupled vision system on a large language model (LLM). The decoupled vision system decouples inner-modal modeling and cross-modal interaction, yielding unique visual information modeling and effective cross-modal comprehension. Libra is trained through discrete auto-regressive modeling on both vision and language inputs. Specifically, we incorporate a routed visual expert with a cross-modal bridge module into a pretrained LLM to route the vision and language flows during attention computing to enable different attention patterns in inner-modal modeling and cross-modal interaction scenarios. Experimental results demonstrate that the dedicated design of Libra achieves a strong MLLM baseline that rivals existing works in the image-to-text scenario with merely 50 million training data, providing a new perspective for future multimodal foundation models. Code is available at <a href="https://github.com/YifanXu74/Libra" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Decoupled inner-modal modeling and cross-modal interaction
    * Used routed visual expert with cross-modal bridge in attention
    * Achieved strong baseline with minimal training data
</details>
</details>

---


<details>
<summary><b> GenArtist: Multimodal LLM as an Agent for Unified Image Generation and Editing</b></summary>

* **Authors:** Zhenyu Wang, Aoxue Li, Zhenguo Li, Xihui Liu
* **arXiv ID:** 2407.05600
* **One-liner:** Proposed GenArtist, a unified image generation and editing system coordinated by an MLLM agent.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2407.05600) | [[PDF]](https://arxiv.org/pdf/2407.05600)

> **Core Innovation**
> Achieved state-of-the-art performance in various generation and editing tasks by decomposing complex problems and using a tool library with step-by-step verification.

<details>
    <summary>Abstract</summary>
    Despite the success achieved by existing image generation and editing methods, current models still struggle with complex problems including intricate text prompts, and the absence of verification and self-correction mechanisms makes the generated images unreliable. Meanwhile, a single model tends to specialize in particular tasks and possess the corresponding capabilities, making it inadequate for fulfilling all user requirements. We propose GenArtist, a unified image generation and editing system, coordinated by a multimodal large language model (MLLM) agent. We integrate a comprehensive range of existing models into the tool library and utilize the agent for tool selection and execution. For a complex problem, the MLLM agent decomposes it into simpler sub-problems and constructs a tree structure to systematically plan the procedure of generation, editing, and self-correction with step-by-step verification. By automatically generating missing position-related inputs and incorporating position information, the appropriate tool can be effectively employed to address each sub-problem. Experiments demonstrate that GenArtist can perform various generation and editing tasks, achieving state-of-the-art performance and surpassing existing models such as SDXL and DALL-E 3, as can be seen in Fig. 1. Project page is <a href="https://zhenyuw16.github.io/GenArtist_page" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Decomposes complex problems into simpler sub-problems
    * Utilizes an MLLM agent for tool selection and execution
    * Incorporates self-correction and verification mechanisms
    * Automatically generates missing position-related inputs
</details>
</details>

---


<details>
<summary><b> ANOLE: An Open, Autoregressive, Native Large Multimodal Models for Interleaved Image-Text Generation</b></summary>

* **Authors:** Ethan Chern, Jiadi Su, Yan Ma, Pengfei Liu
* **arXiv ID:** 2407.06135
* **One-liner:** Introduced Anole, an open, autoregressive, native large multimodal model for interleaved image-text generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2407.06135) | [[PDF]](https://arxiv.org/pdf/2407.06135)

> **Core Innovation**
> Built from Chameleon with efficient fine-tuning, enabling high-quality, coherent multimodal generation without separate diffusion models.

<details>
    <summary>Abstract</summary>
    Previous open-source large multimodal models (LMMs) have faced several limitations: (1) they often lack native integration, requiring adapters to align visual representations with pre-trained large language models (LLMs); (2) many are restricted to single-modal generation; (3) while some support multimodal generation, they rely on separate diffusion models for visual modeling and generation. To mitigate these limitations, we present Anole, an open, autoregressive, native large multimodal model for interleaved image-text generation. We build Anole from Meta AI&#39;s Chameleon, adopting an innovative fine-tuning strategy that is both data-efficient and parameter-efficient. Anole demonstrates high-quality, coherent multimodal generation capabilities. We have open-sourced our model, training framework, and instruction tuning data.
</details>

<details>
    <summary>Key points</summary>
    * Adopts an innovative data-efficient and parameter-efficient fine-tuning strategy
    * Supports interleaved image-text generation
    * Open-sourced model, training framework, and instruction tuning data
</details>
</details>

---


<details>
<summary><b> Show-o: One Single Transformer to Unify Multimodal Understanding and Generation</b></summary>

* **Authors:** Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, Mike Zheng Shou
* **arXiv ID:** 2408.12528
* **One-liner:** Developed Show-o, a unified transformer for multimodal understanding and generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2408.12528) | [[PDF]](https://arxiv.org/pdf/2408.12528)

> **Core Innovation**
> Combines autoregressive and discrete diffusion modeling to handle mixed modalities, achieving comparable or superior performance to specialized models.

<details>
    <summary>Abstract</summary>
    We present a unified transformer, i.e., Show-o, that unifies multimodal understanding and generation. Unlike fully autoregressive models, Show-o unifies autoregressive and (discrete) diffusion modeling to adaptively handle inputs and outputs of various and mixed modalities. The unified model flexibly supports a wide range of vision-language tasks including visual question-answering, text-to-image generation, text-guided inpainting/extrapolation, and mixed-modality generation. Across various benchmarks, it demonstrates comparable or superior performance to existing individual models with an equivalent or larger number of parameters tailored for understanding or generation. This significantly highlights its potential as a next-generation foundation model. Code and models are released at <a href="https://github.com/showlab/Show-o" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Unifies autoregressive and discrete diffusion modeling
    * Supports various vision-language tasks
    * Adaptively handles inputs and outputs of mixed modalities
</details>
</details>

---


<details>
<summary><b> PUMA: Empowering Unified MLLM with Multi-granular Visual Generation</b></summary>

* **Authors:** Rongyao Fang, Chengqi Duan, Kun Wang, Hao Li, Hao Tian, Xingyu Zeng, Rui Zhao, Jifeng Dai, Hongsheng Li, Xihui Liu
* **arXiv ID:** 2410.13861
* **One-liner:** Proposed PUMA, a unified MLLM with multi-granular visual generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2410.13861) | [[PDF]](https://arxiv.org/pdf/2410.13861)

> **Core Innovation**
> Addresses varying granularity demands in image generation tasks within a single framework, demonstrating proficiency in multimodal tasks.

<details>
    <summary>Abstract</summary>
    Recent advancements in multimodal foundation models have yielded significant progress in vision-language understanding. Initial attempts have also explored the potential of multimodal large language models (MLLMs) for visual content generation. However, existing works have insufficiently addressed the varying granularity demands of different image generation tasks within a unified MLLM paradigm - from the diversity required in text-to-image generation to the precise controllability needed in image manipulation. In this work, we propose PUMA, emPowering Unified MLLM with Multi-grAnular visual generation. PUMA unifies multi-granular visual features as both inputs and outputs of MLLMs, elegantly addressing the different granularity requirements of various image generation tasks within a unified MLLM framework. Following multimodal pretraining and task-specific instruction tuning, PUMA demonstrates proficiency in a wide range of multimodal tasks. This work represents a significant step towards a truly unified MLLM capable of adapting to the granularity demands of various visual tasks. The code and model will be released in <a href="https://github.com/rongyaofang/PUMA" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Unifies multi-granular visual features as inputs and outputs
    * Uses multimodal pretraining and task-specific instruction tuning
    * Elegantly handles different granularity requirements
</details>
</details>

---


<details>
<summary><b> VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation</b></summary>

* **Authors:** Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, Song Han, Yao Lu
* **arXiv ID:** 2409.04429
* **One-liner:** Presented VILA-U, a unified foundation model for video, image, language understanding and generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2409.04429) | [[PDF]](https://arxiv.org/pdf/2409.04429)

> **Core Innovation**
> Uses a single autoregressive next-token prediction framework, simplifying the model and achieving near state-of-the-art performance.

<details>
    <summary>Abstract</summary>
    VILA-U is a Unified foundation model that integrates Video, Image, Language understanding and generation. Traditional visual language models (VLMs) use separate modules for understanding and generating visual content, which can lead to misalignment and increased complexity. In contrast, VILA-U employs a single autoregressive next-token prediction framework for both tasks, eliminating the need for additional components like diffusion models. This approach not only simplifies the model but also achieves near state-of-the-art performance in visual language understanding and generation. The success of VILA-U is attributed to two main factors: the unified vision tower that aligns discrete visual tokens with textual inputs during pretraining, which enhances visual perception, and autoregressive image generation can achieve similar quality as diffusion models with high-quality dataset. This allows VILA-U to perform comparably to more complex models using a fully token-based autoregressive framework.
</details>

<details>
    <summary>Key points</summary>
    * Employs a unified vision tower for aligning visual tokens with text
    * Uses autoregressive image generation without diffusion models
    * Eliminates need for additional components
</details>
</details>

---


<details>
<summary><b> UniMuMo: Unified Text, Music and Motion Generation</b></summary>

* **Authors:** Han Yang, Kun Su, Yutong Zhang, Jiaben Chen, Kaizhi Qian, Gaowen Liu, Chuang Gan
* **arXiv ID:** 2410.04534
* **One-liner:** Introduced UniMuMo, a unified multimodal model for text, music, and motion generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2410.04534) | [[PDF]](https://arxiv.org/pdf/2410.04534)

> **Core Innovation**
> Aligns unpaired music and motion data and uses a unified encoder-decoder transformer, achieving competitive results across modalities.

<details>
    <summary>Abstract</summary>
    We introduce UniMuMo, a unified multimodal model capable of taking arbitrary text, music, and motion data as input conditions to generate outputs across all three modalities. To address the lack of time-synchronized data, we align unpaired music and motion data based on rhythmic patterns to leverage existing large-scale music-only and motion-only datasets. By converting music, motion, and text into token-based representation, our model bridges these modalities through a unified encoder-decoder transformer architecture. To support multiple generation tasks within a single framework, we introduce several architectural improvements. We propose encoding motion with a music codebook, mapping motion into the same feature space as music. We introduce a music-motion parallel generation scheme that unifies all music and motion generation tasks into a single transformer decoder architecture with a single training task of music-motion joint generation. Moreover, the model is designed by fine-tuning existing pre-trained single-modality models, significantly reducing computational demands. Extensive experiments demonstrate that UniMuMo achieves competitive results on all unidirectional generation benchmarks across music, motion, and text modalities. Quantitative results are available in the \href{<a href="https://hanyangclarence.github.io/unimumo_demo/" rel="external noopener nofollow" class="link-external link-https">this https URL</a>}{project page}.
</details>

<details>
    <summary>Key points</summary>
    * Aligns unpaired music and motion data based on rhythmic patterns
    * Uses token-based representation and a unified transformer architecture
    * Proposes music-motion parallel generation scheme
</details>
</details>

---


<details>
<summary><b> Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation</b></summary>

* **Authors:** Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, Ping Luo
* **arXiv ID:** 2410.13848
* **One-liner:** Developed Janus, an autoregressive framework that unifies multimodal understanding and generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2410.13848) | [[PDF]](https://arxiv.org/pdf/2410.13848)

> **Core Innovation**
> Decouples visual encoding into separate pathways to enhance flexibility and performance, surpassing previous unified models.

<details>
    <summary>Abstract</summary>
    In this paper, we introduce Janus, an autoregressive framework that unifies multimodal understanding and generation. Prior research often relies on a single visual encoder for both tasks, such as Chameleon. However, due to the differing levels of information granularity required by multimodal understanding and generation, this approach can lead to suboptimal performance, particularly in multimodal understanding. To address this issue, we decouple visual encoding into separate pathways, while still leveraging a single, unified transformer architecture for processing. The decoupling not only alleviates the conflict between the visual encoder&#39;s roles in understanding and generation, but also enhances the framework&#39;s flexibility. For instance, both the multimodal understanding and generation components can independently select their most suitable encoding methods. Experiments show that Janus surpasses previous unified model and matches or exceeds the performance of task-specific models. The simplicity, high flexibility, and effectiveness of Janus make it a strong candidate for next-generation unified multimodal models.
</details>

<details>
    <summary>Key points</summary>
    * Decouples visual encoding into separate pathways
    * Uses a single unified transformer architecture
    * Allows independent selection of encoding methods for understanding and generation
</details>
</details>

---


<details>
<summary><b> Orthus: Autoregressive Interleaved Image-Text Generation with Modality-Specific Heads</b></summary>

* **Authors:** Siqi Kou, Jiachun Jin, Zhihong Liu, Chang Liu, Ye Ma, Jian Jia, Quan Chen, Peng Jiang, Zhijie Deng
* **arXiv ID:** 2412.00127
* **One-liner:** Introduced Orthus, an autoregressive transformer for multimodal tasks with continuous image features.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2412.00127) | [[PDF]](https://arxiv.org/pdf/2412.00127)

> **Core Innovation**
> Combines discrete text tokens and continuous image features under AR modeling, achieving high scores on benchmarks with efficient training.

<details>
    <summary>Abstract</summary>
    We introduce Orthus, an autoregressive (AR) transformer that excels in generating images given textual prompts, answering questions based on visual inputs, and even crafting lengthy image-text interleaved contents. Unlike prior arts on unified multimodal modeling, Orthus simultaneously copes with discrete text tokens and continuous image features under the AR modeling principle. The continuous treatment of visual signals minimizes the information loss for both image understanding and generation while the fully AR formulation renders the characterization of the correlation between modalities straightforward. The key mechanism enabling Orthus to leverage these advantages lies in its modality-specific heads -- one regular language modeling (LM) head predicts discrete text tokens and one diffusion head generates continuous image features conditioning on the output of the backbone. We devise an efficient strategy for building Orthus -- by substituting the Vector Quantization (VQ) operation in the existing unified AR model with a soft alternative, introducing a diffusion head, and tuning the added modules to reconstruct images, we can create an Orthus-base model effortlessly (e.g., within mere 72 A100 GPU hours). Orthus-base can further embrace post-training to better model interleaved images and texts. Empirically, Orthus surpasses competing baselines including Show-o and Chameleon across standard benchmarks, achieving a GenEval score of 0.58 and an MME-P score of 1265.8 using 7B parameters. Orthus also shows exceptional mixed-modality generation capabilities, reflecting the potential for handling intricate practical generation tasks.
</details>

<details>
    <summary>Key points</summary>
    * Uses modality-specific heads: LM head for text and diffusion head for images
    * Substitutes VQ with a soft alternative for efficient model building
    * Supports mixed-modality generation and post-training
</details>
</details>

---


<details>
<summary><b> MUSE-VL: Modeling Unified VLM through Semantic Discrete Encoding</b></summary>

* **Authors:** Rongchang Xie, Chen Du, Ping Song, Chang Liu
* **arXiv ID:** 2411.17762
* **One-liner:** Proposed MUSE-VL, a unified vision-language model using Semantic Discrete Encoding.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2411.17762) | [[PDF]](https://arxiv.org/pdf/2411.17762)

> **Core Innovation**
> Improves alignment of visual and language tokens with semantic constraints, reducing training data needs and enhancing performance.

<details>
    <summary>Abstract</summary>
    We introduce MUSE-VL, a Unified Vision-Language Model through Semantic discrete Encoding for multimodal understanding and generation. Recently, the research community has begun exploring unified models for visual generation and understanding. However, existing vision tokenizers (e.g., VQGAN) only consider low-level information, which makes it difficult to align with language tokens. This results in high training complexity and necessitates a large amount of training data to achieve optimal performance. Additionally, their performance is still far from dedicated understanding models. This paper proposes Semantic Discrete Encoding (SDE), which effectively aligns the information of visual tokens and language tokens by adding semantic constraints to the visual tokenizer. This greatly reduces the amount of training data and improves the performance of the unified model. With the same LLM size, our method improved the understanding performance by 4.8% compared to the previous SOTA Emu3 and surpassed the dedicated understanding model LLaVA-NeXT 34B by 3.7%. Our model also surpasses the existing unified models on visual generation benchmarks.
</details>

<details>
    <summary>Key points</summary>
    * Introduces Semantic Discrete Encoding (SDE) for better token alignment
    * Reduces training complexity and data requirements
    * Improves understanding and generation performance compared to SOTA
</details>
</details>

---


<details>
<summary><b> Liquid: Language Models are Scalable and Unified Multi-modal Generators</b></summary>

* **Authors:** Junfeng Wu, Yi Jiang, Chuofan Ma, Yuliang Liu, Hengshuang Zhao, Zehuan Yuan, Song Bai, Xiang Bai
* **arXiv ID:** 2412.04332
* **One-liner:** Presented Liquid, an auto-regressive paradigm for visual comprehension and generation using a single LLM.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2412.04332) | [[PDF]](https://arxiv.org/pdf/2412.04332)

> **Core Innovation**
> Uncovers a scaling law for unified training, enabling mutual enhancement of tasks and outperforming models like Chameleon and SD-XL.

<details>
    <summary>Abstract</summary>
    We present Liquid, an auto-regressive generation paradigm that seamlessly integrates visual comprehension and generation by tokenizing images into discrete codes and learning these code embeddings alongside text tokens within a shared feature space for both vision and language. Unlike previous multimodal large language model (MLLM), Liquid achieves this integration using a single large language model (LLM), eliminating the need for external pretrained visual embeddings such as CLIP. For the first time, Liquid uncovers a scaling law that performance drop unavoidably brought by the unified training of visual and language tasks diminishes as the model size increases. Furthermore, the unified token space enables visual generation and comprehension tasks to mutually enhance each other, effectively removing the typical interference seen in earlier models. We show that existing LLMs can serve as strong foundations for Liquid, saving 100x in training costs while outperforming Chameleon in multimodal capabilities and maintaining language performance comparable to mainstream LLMs like LLAMA2. Liquid also outperforms models like SD v2.1 and SD-XL (FID of 5.47 on MJHQ-30K), excelling in both vision-language and text-only tasks. This work demonstrates that LLMs such as Qwen2.5 and GEMMA2 are powerful multimodal generators, offering a scalable solution for enhancing both vision-language understanding and generation. The code and models will be released at <a href="https://github.com/FoundationVision/Liquid" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Tokenizes images into discrete codes in a shared feature space
    * Uses a single LLM without external visual embeddings
    * Demonstrates scaling law for performance improvement with model size
</details>
</details>

---


<details>
<summary><b> SILMM: Self-Improving Large Multimodal Models for Compositional Text-to-Image Generation</b></summary>

* **Authors:** Leigang Qu, Haochuan Li, Wenjie Wang, Xiang Liu, Juncheng Li, Liqiang Nie, Tat-Seng Chua
* **arXiv ID:** 2412.05818
* **One-liner:** Introduced SILMM, a model-agnostic iterative self-improvement framework for LMMs.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2412.05818) | [[PDF]](https://arxiv.org/pdf/2412.05818)

> **Core Innovation**
> Enables LMMs to provide self-feedback and optimize text-image alignment via DPO, with adaptations for continuous visual features.

<details>
    <summary>Abstract</summary>
    Large Multimodal Models (LMMs) have demonstrated impressive capabilities in multimodal understanding and generation, pushing forward advancements in text-to-image generation. However, achieving accurate text-image alignment for LMMs, particularly in compositional scenarios, remains challenging. Existing approaches, such as layout planning for multi-step generation and learning from human feedback or AI feedback, depend heavily on prompt engineering, costly human annotations, and continual upgrading, limiting flexibility and scalability. In this work, we introduce a model-agnostic iterative self-improvement framework (SILMM) that can enable LMMs to provide helpful and scalable self-feedback and optimize text-image alignment via Direct Preference Optimization (DPO). DPO can readily applied to LMMs that use discrete visual tokens as intermediate image representations; while it is less suitable for LMMs with continuous visual features, as obtaining generation probabilities is challenging. To adapt SILMM to LMMs with continuous features, we propose a diversity mechanism to obtain diverse representations and a kernel-based continuous DPO for alignment. Extensive experiments on three compositional text-to-image generation benchmarks validate the effectiveness and superiority of SILMM, showing improvements exceeding 30% on T2I-CompBench++ and around 20% on DPG-Bench.
</details>

<details>
    <summary>Key points</summary>
    * Iterative self-improvement framework (SILMM)
    * Direct Preference Optimization (DPO) for alignment
    * Diversity mechanism for continuous features
    * Kernel-based continuous DPO
</details>
</details>

---


<details>
<summary><b> ILLUME: Illuminating Your LLMs to See, Draw, and Self-Enhance</b></summary>

* **Authors:** Chunwei Wang, Guansong Lu, Junwei Yang, Runhui Huang, Jianhua Han, Lu Hou, Wei Zhang, Hang Xu
* **arXiv ID:** 2412.06673
* **One-liner:** Developed ILLUME, a unified MLLM with integrated understanding and generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2412.06673) | [[PDF]](https://arxiv.org/pdf/2412.06673)

> **Core Innovation**
> Achieves data efficiency and synergistic enhancement through a vision tokenizer and self-enhancing alignment scheme.

<details>
    <summary>Abstract</summary>
    In this paper, we introduce ILLUME, a unified multimodal large language model (MLLM) that seamlessly integrates multimodal understanding and generation capabilities within a single large language model through a unified next-token prediction formulation. To address the large dataset size typically required for image-text alignment, we propose to enhance data efficiency through the design of a vision tokenizer that incorporates semantic information and a progressive multi-stage training procedure. This approach reduces the dataset size to just 15M for pretraining -- over four times fewer than what is typically needed -- while achieving competitive or even superior performance with existing unified MLLMs, such as Janus. Additionally, to promote synergistic enhancement between understanding and generation capabilities, which is under-explored in previous works, we introduce a novel self-enhancing multimodal alignment scheme. This scheme supervises the MLLM to self-assess the consistency between text descriptions and self-generated images, facilitating the model to interpret images more accurately and avoid unrealistic and incorrect predictions caused by misalignment in image generation. Based on extensive experiments, our proposed ILLUME stands out and competes with state-of-the-art unified MLLMs and specialized models across various benchmarks for multimodal understanding, generation, and editing.
</details>

<details>
    <summary>Key points</summary>
    * Unified next-token prediction formulation
    * Vision tokenizer with semantic information
    * Progressive multi-stage training
    * Self-enhancing multimodal alignment scheme
</details>
</details>

---


<details>
<summary><b> Visual Lexicon: Rich Image Features in Language Space</b></summary>

* **Authors:** XuDong Wang, Xingyi Zhou, Alireza Fathi, Trevor Darrell, Cordelia Schmid
* **arXiv ID:** 2412.06774
* **One-liner:** Created Visual Lexicon (ViLex), a visual language encoding image information into text tokens.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2412.06774) | [[PDF]](https://arxiv.org/pdf/2412.06774)

> **Core Innovation**
> Enables high-quality image generation and understanding by capturing semantics and details in a self-supervised manner.

<details>
    <summary>Abstract</summary>
    We present Visual Lexicon, a novel visual language that encodes rich image information into the text space of vocabulary tokens while retaining intricate visual details that are often challenging to convey in natural language. Unlike traditional methods that prioritize either high-level semantics (e.g., CLIP) or pixel-level reconstruction (e.g., VAE), ViLex simultaneously captures rich semantic content and fine visual details, enabling high-quality image generation and comprehensive visual scene understanding. Through a self-supervised learning pipeline, ViLex generates tokens optimized for reconstructing input images using a frozen text-to-image (T2I) diffusion model, preserving the detailed information necessary for high-fidelity semantic-level reconstruction. As an image embedding in the language space, ViLex tokens leverage the compositionality of natural languages, allowing them to be used independently as &#34;text tokens&#34; or combined with natural language tokens to prompt pretrained T2I models with both visual and textual inputs, mirroring how we interact with vision-language models (VLMs). Experiments demonstrate that ViLex achieves higher fidelity in image reconstruction compared to text embeddings--even with a single ViLex token. Moreover, ViLex successfully performs various DreamBooth tasks in a zero-shot, unsupervised manner without fine-tuning T2I models. Additionally, ViLex serves as a powerful vision encoder, consistently improving vision-language model performance across 15 benchmarks relative to a strong SigLIP baseline.
</details>

<details>
    <summary>Key points</summary>
    * Self-supervised learning pipeline
    * Token generation for image reconstruction
    * Use as text tokens with T2I models
    * Zero-shot DreamBooth tasks
</details>
</details>

---


<details>
<summary><b> SynerGen-VL: Towards Synergistic Image Understanding and Generation with Vision Experts and Token Folding</b></summary>

* **Authors:** Hao Li, Changyao Tian, Jie Shao, Xizhou Zhu, Zhaokai Wang, Jinguo Zhu, Wenhan Dou, Xiaogang Wang, Hongsheng Li, Lewei Lu, Jifeng Dai
* **arXiv ID:** 2412.09604
* **One-liner:** Proposed SynerGen-VL, an encoder-free MLLM for image understanding and generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2412.09604) | [[PDF]](https://arxiv.org/pdf/2412.09604)

> **Core Innovation**
> Simplifies model design with token folding and progressive alignment pretraining, achieving competitive performance.

<details>
    <summary>Abstract</summary>
    The remarkable success of Large Language Models (LLMs) has extended to the multimodal domain, achieving outstanding performance in image understanding and generation. Recent efforts to develop unified Multimodal Large Language Models (MLLMs) that integrate these capabilities have shown promising results. However, existing approaches often involve complex designs in model architecture or training pipeline, increasing the difficulty of model training and scaling. In this paper, we propose SynerGen-VL, a simple yet powerful encoder-free MLLM capable of both image understanding and generation. To address challenges identified in existing encoder-free unified MLLMs, we introduce the token folding mechanism and the vision-expert-based progressive alignment pretraining strategy, which effectively support high-resolution image understanding while reducing training complexity. After being trained on large-scale mixed image-text data with a unified next-token prediction objective, SynerGen-VL achieves or surpasses the performance of existing encoder-free unified MLLMs with comparable or smaller parameter sizes, and narrows the gap with task-specific state-of-the-art models, highlighting a promising path toward future unified MLLMs. Our code and models shall be released.
</details>

<details>
    <summary>Key points</summary>
    * Encoder-free architecture
    * Token folding mechanism
    * Vision-expert-based progressive alignment pretraining
    * Unified next-token prediction objective
</details>
</details>

---


<details>
<summary><b> MetaMorph: Multimodal Understanding and Generation via Instruction Tuning</b></summary>

* **Authors:** Shengbang Tong, David Fan, Jiachen Zhu, Yunyang Xiong, Xinlei Chen, Koustuv Sinha, Michael Rabbat, Yann LeCun, Saining Xie, Zhuang Liu
* **arXiv ID:** 2412.14164
* **One-liner:** Introduced VPiT for instruction tuning to enable LLMs to generate text and visual tokens.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2412.14164) | [[PDF]](https://arxiv.org/pdf/2412.14164)

> **Core Innovation**
> Unlocks visual generation as a byproduct of improved understanding with efficient data usage.

<details>
    <summary>Abstract</summary>
    In this work, we propose Visual-Predictive Instruction Tuning (VPiT) - a simple and effective extension to visual instruction tuning that enables a pretrained LLM to quickly morph into an unified autoregressive model capable of generating both text and visual tokens. VPiT teaches an LLM to predict discrete text tokens and continuous visual tokens from any input sequence of image and text data curated in an instruction-following format. Our empirical investigation reveals several intriguing properties of VPiT: (1) visual generation ability emerges as a natural byproduct of improved visual understanding, and can be unlocked efficiently with a small amount of generation data; (2) while we find understanding and generation to be mutually beneficial, understanding data contributes to both capabilities more effectively than generation data. Building upon these findings, we train our MetaMorph model and achieve competitive performance on both visual understanding and generation. In visual generation, MetaMorph can leverage the world knowledge and reasoning abilities gained from LLM pretraining, and overcome common failure modes exhibited by other generation models. Our results suggest that LLMs may have strong &#34;prior&#34; vision capabilities that can be efficiently adapted to both visual understanding and generation with a relatively simple instruction tuning process.
</details>

<details>
    <summary>Key points</summary>
    * Visual-Predictive Instruction Tuning (VPiT)
    * Prediction of discrete text and continuous visual tokens
    * Mutual benefit of understanding and generation
    * Efficient adaptation with small generation data
</details>
</details>

---


<details>
<summary><b> LMFusion: Adapting Pretrained Language Models for Multimodal Generation</b></summary>

* **Authors:** Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke Zettlemoyer, Lili Yu
* **arXiv ID:** 2412.15188
* **One-liner:** Developed LMFusion, a framework to add multimodal capabilities to text-only LLMs.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2412.15188) | [[PDF]](https://arxiv.org/pdf/2412.15188)

> **Core Innovation**
> Preserves language capabilities while enabling visual understanding and generation with parallel modules.

<details>
    <summary>Abstract</summary>
    We present LMFusion, a framework for empowering pretrained text-only large language models (LLMs) with multimodal generative capabilities, enabling them to understand and generate both text and images in arbitrary sequences. LMFusion leverages existing Llama-3&#39;s weights for processing texts autoregressively while introducing additional and parallel transformer modules for processing images with diffusion. During training, the data from each modality is routed to its dedicated modules: modality-specific feedforward layers, query-key-value projections, and normalization layers process each modality independently, while the shared self-attention layers allow interactions across text and image features. By freezing the text-specific modules and only training the image-specific modules, LMFusion preserves the language capabilities of text-only LLMs while developing strong visual understanding and generation abilities. Compared to methods that pretrain multimodal generative models from scratch, our experiments demonstrate that, LMFusion improves image understanding by 20% and image generation by 3.6% using only 50% of the FLOPs while maintaining Llama-3&#39;s language capabilities. We also demonstrate that this framework can adapt existing vision-language models with multimodal generation ability. Overall, this framework not only leverages existing computational investments in text-only LLMs but also enables the parallel development of language and vision capabilities, presenting a promising direction for efficient multimodal model development.
</details>

<details>
    <summary>Key points</summary>
    * Modality-specific modules with shared self-attention
    * Freezing text modules, training image modules
    * Efficient use of existing LLM weights
    * Parallel development of language and vision
</details>
</details>

---


<details>
<summary><b> Dual Diffusion for Unified Image Generation and Understanding</b></summary>

* **Authors:** Zijie Li, Henry Li, Yichun Shi, Amir Barati Farimani, Yuval Kluger, Linjie Yang, Peng Wang
* **arXiv ID:** 2501.00289
* **One-liner:** Proposed a large-scale end-to-end diffusion model for multimodal understanding and generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2501.00289) | [[PDF]](https://arxiv.org/pdf/2501.00289)

> **Core Innovation**
> First diffusion model to support full vision-language capabilities with a cross-modal likelihood framework.

<details>
    <summary>Abstract</summary>
    Diffusion models have gained tremendous success in text-to-image generation, yet still lag behind with visual understanding tasks, an area dominated by autoregressive vision-language models. We propose a large-scale and fully end-to-end diffusion model for multi-modal understanding and generation that significantly improves on existing diffusion-based multimodal models, and is the first of its kind to support the full suite of vision-language modeling capabilities. Inspired by the multimodal diffusion transformer (MM-DiT) and recent advances in discrete diffusion language modeling, we leverage a cross-modal maximum likelihood estimation framework that simultaneously trains the conditional likelihoods of both images and text jointly under a single loss function, which is back-propagated through both branches of the diffusion transformer. The resulting model is highly flexible and capable of a wide range of tasks including image generation, captioning, and visual question answering. Our model attained competitive performance compared to recent unified image understanding and generation models, demonstrating the potential of multimodal diffusion modeling as a promising alternative to autoregressive next-token prediction models.
</details>

<details>
    <summary>Key points</summary>
    * Cross-modal maximum likelihood estimation
    * Joint training of image and text branches
    * Diffusion transformer architecture
    * Support for generation, captioning, and VQA
</details>
</details>

---


<details>
<summary><b> Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling</b></summary>

* **Authors:** Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan
* **arXiv ID:** 2501.17811
* **One-liner:** Enhanced Janus to Janus-Pro with optimized training, data, and scaling.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2501.17811) | [[PDF]](https://arxiv.org/pdf/2501.17811)

> **Core Innovation**
> Achieves advancements in multimodal understanding and text-to-image generation with improved stability.

<details>
    <summary>Abstract</summary>
    In this work, we introduce Janus-Pro, an advanced version of the previous work Janus. Specifically, Janus-Pro incorporates (1) an optimized training strategy, (2) expanded training data, and (3) scaling to larger model size. With these improvements, Janus-Pro achieves significant advancements in both multimodal understanding and text-to-image instruction-following capabilities, while also enhancing the stability of text-to-image generation. We hope this work will inspire further exploration in the field. Code and models are publicly available.
</details>

<details>
    <summary>Key points</summary>
    * Optimized training strategy
    * Expanded training data
    * Larger model size scaling
    * Improved stability in generation
</details>
</details>

---


<details>
<summary><b> QLIP: Text-Aligned Visual Tokenization Unifies Auto-Regressive Multimodal Understanding and Generation</b></summary>

* **Authors:** Yue Zhao, Fuzhao Xue, Scott Reed, Linxi Fan, Yuke Zhu, Jan Kautz, Zhiding Yu, Philipp Krähenbühl, De-An Huang
* **arXiv ID:** 2502.05178
* **One-liner:** Introduced QLIP, a visual tokenization method for multimodal understanding and generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2502.05178) | [[PDF]](https://arxiv.org/pdf/2502.05178)

> **Core Innovation**
> Balances reconstruction and alignment objectives, enabling unified models with efficient training.

<details>
    <summary>Abstract</summary>
    We introduce Quantized Language-Image Pretraining (QLIP), a visual tokenization method that combines state-of-the-art reconstruction quality with state-of-the-art zero-shot image understanding. QLIP trains a binary-spherical-quantization-based autoencoder with reconstruction and language-image alignment objectives. We are the first to show that the two objectives do not need to be at odds. We balance the two loss terms dynamically during training and show that a two-stage training pipeline effectively mixes the large-batch requirements of image-language pre-training with the memory bottleneck imposed by the reconstruction objective. We validate the effectiveness of QLIP for multimodal understanding and text-conditioned image generation with a single model. Specifically, QLIP serves as a drop-in replacement for the visual encoder for LLaVA and the image tokenizer for LlamaGen with comparable or even better performance. Finally, we demonstrate that QLIP enables a unified mixed-modality auto-regressive model for understanding and generation.
</details>

<details>
    <summary>Key points</summary>
    * Binary-spherical-quantization-based autoencoder
    * Dynamic balancing of loss terms
    * Two-stage training pipeline
    * Drop-in replacement for encoders and tokenizers
</details>
</details>

---


<details>
<summary><b> UniCMs: A Unified Consistency Model For Efficient Multimodal Generation and Understanding</b></summary>

* **Authors:** Chenkai Xu, Xu Wang, Zhenyi Liao, Yishun Li, Tianqi Hou, Zhijie Deng
* **arXiv ID:** 2502.05415
* **One-liner:** Developed UniCMs, a unified consistency model for efficient multimodal generation and understanding.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2502.05415) | [[PDF]](https://arxiv.org/pdf/2502.05415)

> **Core Innovation**
> Uses discrete tokens and parallel decoding for text, outperforming existing models in speed and performance.

<details>
    <summary>Abstract</summary>
    Consistency models (CMs) have shown promise in the efficient generation of both image and text. This raises the natural question of whether we can learn a unified CM for efficient multimodal generation (e.g., text-to-image) and understanding (e.g., image-to-text). Intuitively, such a model could be acquired by applying the consistency distillation (CD) to existing unified multimodal models. However, the key challenge is establishing a unified denoising perspective for both image and text generation, which is essential for establishing the consistency mapping. To tackle this, at the representation level, we advocate for discrete tokens for both modalities to best preserve language modeling capabilities. Critically, instead of defining the text denoising trajectory via recent discrete diffusion language modeling principles, we specify it using the parallel decoding trace of an autoregressive language model, benefiting from the latter&#39;s superior performance in general text generation tasks. The denoising trajectory of image tokens adheres to standard discrete diffusion. We train our unified consistency models (UniCMs) on these combined multimodal trajectories simultaneously with a unified objective. We introduce a trajectory segmentation strategy to further improve the training convergence. Empirically, in text-to-image generation, UniCMs outperform SD3 on GenEval, Image Reward, and CLIP Score metrics, while requiring only approximately ${1}/{8}$ of the sampling time. Meanwhile, in image-to-text generation, UniCMs surpass Show-o on the MMMU benchmark while being $1.5 \times$ faster at long-sequence generating speed. The code is available at <a href="https://github.com/zhijie-group/UniCMs" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Consistency distillation for unified models
    * Discrete tokens for both modalities
    * Parallel decoding trace for text denoising
    * Trajectory segmentation strategy
</details>
</details>

---


<details>
<summary><b> UniTok: A Unified Tokenizer for Visual Generation and Understanding</b></summary>

* **Authors:** Chuofan Ma, Yi Jiang, Junfeng Wu, Jihan Yang, Xin Yu, Zehuan Yuan, Bingyue Peng, Xiaojuan Qi
* **arXiv ID:** 2502.20321
* **One-liner:** Introduced UniTok, a unified tokenizer that sets new performance records in image generation and understanding.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2502.20321) | [[PDF]](https://arxiv.org/pdf/2502.20321)

> **Core Innovation**
> Unified reconstruction and semantic supervision by scaling up vocabulary and bottleneck dimension with multi-codebook quantization.

<details>
    <summary>Abstract</summary>
    Visual generative and understanding models typically rely on distinct tokenizers to process images, presenting a key challenge for unifying them within a single framework. Recent studies attempt to address this by connecting the training of VQVAE (for autoregressive generation) and CLIP (for understanding) to build a unified tokenizer. However, directly combining these training objectives has been observed to cause severe loss conflicts. In this paper, we show that reconstruction and semantic supervision do not inherently conflict. Instead, the underlying bottleneck stems from limited representational capacity of discrete token space. Building on these insights, we introduce UniTok, a unified tokenizer featuring a novel multi-codebook quantization mechanism that effectively scales up the vocabulary size and bottleneck dimension. In terms of final performance, UniTok sets a new record of 0.38 rFID and 78.6% zero-shot accuracy on ImageNet. Besides, UniTok can be seamlessly integrated into MLLMs to unlock native visual generation capability, without compromising the understanding performance. Additionally, we show that UniTok favors cfg-free generation, reducing gFID from 14.6 to 2.5 on ImageNet 256$\times$256 benchmark. GitHub: <a href="https://github.com/FoundationVision/UniTok" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Multi-codebook quantization mechanism
    * Scales up vocabulary size and bottleneck dimension
    * Seamless integration into MLLMs for native visual generation
    * Favors cfg-free generation
</details>
</details>

---


<details>
<summary><b> MMGen: Unified Multi-modal Image Generation and Understanding in One Go</b></summary>

* **Authors:** Jiepeng Wang, Zhaoqing Wang, Hao Pan, Yuan Liu, Dongdong Yu, Changhu Wang, Wenping Wang
* **arXiv ID:** 2503.20644v1
* **One-liner:** Introduced MMGen, a unified diffusion framework for multi-modal generation and understanding.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2503.20644v1) | [[PDF]](https://arxiv.org/pdf/2503.20644v1)

> **Core Innovation**
> Integrated multiple generative tasks into a single diffusion model with a novel diffusion transformer and modality-decoupling strategy.

<details>
    <summary>Abstract</summary>
    A unified diffusion framework for multi-modal generation and understanding has the transformative potential to achieve seamless and controllable image diffusion and other cross-modal tasks. In this paper, we introduce MMGen, a unified framework that integrates multiple generative tasks into a single diffusion model. This includes: (1) multi-modal category-conditioned generation, where multi-modal outputs are generated simultaneously through a single inference process, given category information; (2) multi-modal visual understanding, which accurately predicts depth, surface normals, and segmentation maps from RGB images; and (3) multi-modal conditioned generation, which produces corresponding RGB images based on specific modality conditions and other aligned modalities. Our approach develops a novel diffusion transformer that flexibly supports multi-modal output, along with a simple modality-decoupling strategy to unify various tasks. Extensive experiments and applications demonstrate the effectiveness and superiority of MMGen across diverse tasks and conditions, highlighting its potential for applications that require simultaneous generation and understanding.
</details>

<details>
    <summary>Key points</summary>
    * Novel diffusion transformer for multi-modal output
    * Modality-decoupling strategy
    * Multi-modal category-conditioned generation
    * Multi-modal visual understanding
    * Multi-modal conditioned generation
</details>
</details>

---


<details>
<summary><b> Towards Enhanced Image Generation Via Multi-modal Chain of Thought in Unified Generative Models</b></summary>

* **Authors:** Yi Wang, Mushui Liu, Wanggui He, Hanyang Yuan, Longxiang Zhang, Ziwei Huang, Guanghao Zhang, Wenkai Fang, Haoze Jiang, Shengxuming Zhang, Dong She, Jinlong Liu, Weilong Dai, Mingli Song, Hao Jiang, Jie Song
* **arXiv ID:** 2503.01298
* **One-liner:** Introduced FoX with MCoT to enhance complex image generation in unified models.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2503.01298) | [[PDF]](https://arxiv.org/pdf/2503.01298)

> **Core Innovation**
> Addressed complex compositional instructions by integrating Chain of Thought (CoT) with a functionality-oriented expert architecture.

<details>
    <summary>Abstract</summary>
    Unified generative models have shown remarkable performance in text and image generation. For image synthesis tasks, they adopt straightforward text-to-image (T2I) generation. However, direct T2I generation limits the models in handling complex compositional instructions, which frequently occur in real-world scenarios. Although this issue is vital, existing works mainly focus on improving the basic image generation capability of the models. While such improvements help to some extent, they still fail to adequately resolve the problem. Inspired by Chain of Thought (CoT) solving complex problems step by step, this work aims to introduce CoT into unified generative models to address the challenges of complex image generation that direct T2I generation cannot effectively solve, thereby endowing models with enhanced image generation ability. To achieve this, we first propose Functionality-oriented eXperts (FoXperts), an expert-parallel architecture in our model FoX, which assigns experts by function. FoXperts disentangles potential conflicts in mainstream modality-oriented designs and provides a solid foundation for CoT. When introducing CoT, the first question is how to design it for complex image generation. To this end, we emulate a human-like artistic workflow -- planning, acting, reflection, and correction -- and propose the Multimodal Chain of Thought (MCoT) approach, as the data involves both text and image. To address the subsequent challenge -- designing an effective MCoT training paradigm -- we develop a multi-task joint training scheme that equips the model with all capabilities required for each MCoT step in a disentangled manner. This paradigm avoids the difficulty of collecting consistent multi-step data tuples. Extensive experiments show that FoX consistently outperforms existing unified models on various T2I benchmarks, delivering notable improvements in complex image generation.
</details>

<details>
    <summary>Key points</summary>
    * Functionality-oriented eXperts (FoXperts) architecture
    * Multimodal Chain of Thought (MCoT) approach
    * Multi-task joint training scheme
    * Emulates human-like artistic workflow
</details>
</details>

---


<details>
<summary><b> SemHiTok: A Unified Image Tokenizer via Semantic-Guided Hierarchical Codebook for Multimodal Understanding and Generation</b></summary>

* **Authors:** Zisheng Chen, Chunwei Wang, Xiuwei Chen, Hongbin Xu, Runhui Huang, Jun Zhou, Jianhua Han, Hang Xu, Xiaodan Liang
* **arXiv ID:** 2503.06764
* **One-liner:** Introduced SemHiTok, a unified image tokenizer achieving SOTA in reconstruction and understanding.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2503.06764) | [[PDF]](https://arxiv.org/pdf/2503.06764)

> **Core Innovation**
> Decoupled semantic and pixel features using a semantic-guided hierarchical codebook for consistent representations.

<details>
    <summary>Abstract</summary>
    In this paper, we introduce SemHiTok, a unified image Tokenizer via Semantic-Guided Hierarchical codebook that provides consistent discrete representations for multimodal understanding and generation. Recently, unified image tokenizers have sparked exploration within research community, which is designed to capture high-level semantic features for understanding and retaining low-level pixel features for generation. Previous works attempt to train a unified image tokenizer by combining loss for semantic distillation and pixel reconstruction. However, due to the differing levels of features prioritized by multimodal understanding and generation, joint training methods face significant challenges in achieving a good trade-off. SemHiTok addresses this challenge through a novel semantic-guided hierarchical codebook, which builds pixel sub-codebooks on a pretrained semantic codebook. This design decouples semantic and pixel both in terms of structure and training strategy, enabling the tokenizer to capture pixel features while retaining its ability to comprehend high-level semantic information. Our experiments demonstrate that SemHiTok achieves SOTA performance in image reconstruction and multimodal understanding under LLaVA-v1.5 setting. Further, we develop a unified MLLM with SemHiTok, which exhibits superior performance across multimodal understanding and generation tasks. For understanding, SemHiTok achieves impressive performance on most benchmarks. For generation, our model achieves SOTA performance on MJHQ30K in unified MLLMs.
</details>

<details>
    <summary>Key points</summary>
    * Semantic-guided hierarchical codebook
    * Decouples semantic and pixel features in structure and training
    * Builds pixel sub-codebooks on pretrained semantic codebook
    * Superior performance in MLLM understanding and generation
</details>
</details>

---


<details>
<summary><b> WISE: A World Knowledge-Informed Semantic Evaluation for Text-to-Image Generation</b></summary>

* **Authors:** Yuwei Niu, Munan Ning, Mengren Zheng, Weiyang Jin, Bin Lin, Peng Jin, Jiaqi Liao, Chaoran Feng, Kunpeng Ning, Bin Zhu, Li Yuan
* **arXiv ID:** 2503.07265
* **One-liner:** Proposed WISE benchmark and WiScore metric for evaluating world knowledge in T2I models.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2503.07265) | [[PDF]](https://arxiv.org/pdf/2503.07265)

> **Core Innovation**
> Moved beyond traditional metrics by assessing complex semantic understanding and knowledge integration in image generation.

<details>
    <summary>Abstract</summary>
    Text-to-Image (T2I) models are capable of generating high-quality artistic creations and visual content. However, existing research and evaluation standards predominantly focus on image realism and shallow text-image alignment, lacking a comprehensive assessment of complex semantic understanding and world knowledge integration in text to image generation. To address this challenge, we propose $\textbf{WISE}$, the first benchmark specifically designed for $\textbf{W}$orld Knowledge-$\textbf{I}$nformed $\textbf{S}$emantic $\textbf{E}$valuation. WISE moves beyond simple word-pixel mapping by challenging models with 1000 meticulously crafted prompts across 25 sub-domains in cultural common sense, spatio-temporal reasoning, and natural science. To overcome the limitations of traditional CLIP metric, we introduce $\textbf{WiScore}$, a novel quantitative metric for assessing knowledge-image alignment. Through comprehensive testing of 20 models (10 dedicated T2I models and 10 unified multimodal models) using 1,000 structured prompts spanning 25 subdomains, our findings reveal significant limitations in their ability to effectively integrate and apply world knowledge during image generation, highlighting critical pathways for enhancing knowledge incorporation and application in next-generation T2I models. Code and data are available at <a href="https://github.com/PKU-YuanGroup/WISE" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * WISE benchmark with 1000 prompts across 25 sub-domains
    * WiScore metric for knowledge-image alignment
    * Comprehensive testing of 20 models
    * Highlights limitations in knowledge integration
</details>
</details>

---


<details>
<summary><b> DualToken: Towards Unifying Visual Understanding and Generation with Dual Visual Vocabularies</b></summary>

* **Authors:** Wei Song, Yuran Wang, Zijia Song, Yadong Li, Haoze Sun, Weipeng Chen, Zenan Zhou, Jianhua Xu, Jiaqi Wang, Kaicheng Yu
* **arXiv ID:** 2503.14324
* **One-liner:** Introduced DualToken, a unified tokenizer achieving SOTA in both reconstruction and semantic tasks.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2503.14324) | [[PDF]](https://arxiv.org/pdf/2503.14324)

> **Core Innovation**
> Disentangled high and low-level features with separate codebooks to resolve conflicts between reconstruction and semantic objectives.

<details>
    <summary>Abstract</summary>
    The differing representation spaces required for visual understanding and generation pose a challenge in unifying them within the autoregressive paradigm of large language models. A vision tokenizer trained for reconstruction excels at capturing low-level perceptual details, making it well-suited for visual generation but lacking high-level semantic representations for understanding tasks. Conversely, a vision encoder trained via contrastive learning aligns well with language but struggles to decode back into the pixel space for generation tasks. To bridge this gap, we propose DualToken, a method that unifies representations for both understanding and generation within a single tokenizer. However, directly integrating reconstruction and semantic objectives in a single tokenizer creates conflicts, leading to degraded performance in both reconstruction quality and semantic performance. Instead of forcing a single codebook to handle both semantic and perceptual information, DualToken disentangles them by introducing separate codebooks for high and low-level features, effectively transforming their inherent conflict into a synergistic relationship. As a result, DualToken achieves state-of-the-art performance in both reconstruction and semantic tasks while demonstrating remarkable effectiveness in downstream MLLM understanding and generation tasks. Notably, we also show that DualToken, as a unified tokenizer, surpasses the naive combination of two distinct types vision encoders, providing superior performance within a unified MLLM.
</details>

<details>
    <summary>Key points</summary>
    * Separate codebooks for high and low-level features
    * Transforms conflict into synergy
    * Superior performance in MLLM understanding and generation
    * Surpasses naive combination of distinct encoders
</details>
</details>

---


<details>
<summary><b> Unified Multimodal Discrete Diffusion</b></summary>

* **Authors:** Alexander Swerdlow, Mihir Prabhudesai, Siddharth Gandhi, Deepak Pathak, Katerina Fragkiadaki
* **arXiv ID:** 2503.20853
* **One-liner:** Introduced UniDisc, a unified multimodal discrete diffusion model outperforming AR models.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2503.20853) | [[PDF]](https://arxiv.org/pdf/2503.20853)

> **Core Innovation**
> Leveraged discrete diffusion for joint text and image tasks with improved controllability and efficiency.

<details>
    <summary>Abstract</summary>
    Multimodal generative models that can understand and generate across multiple modalities are dominated by autoregressive (AR) approaches, which process tokens sequentially from left to right, or top to bottom. These models jointly handle images, text, video, and audio for various tasks such as image captioning, question answering, and image generation. In this work, we explore discrete diffusion models as a unified generative formulation in the joint text and image domain, building upon their recent success in text generation. Discrete diffusion models offer several advantages over AR models, including improved control over quality versus diversity of generated samples, the ability to perform joint multimodal inpainting (across both text and image domains), and greater controllability in generation through guidance. Leveraging these benefits, we present the first Unified Multimodal Discrete Diffusion (UniDisc) model which is capable of jointly understanding and generating text and images for a variety of downstream tasks. We compare UniDisc to multimodal AR models, performing a scaling analysis and demonstrating that UniDisc outperforms them in terms of both performance and inference-time compute, enhanced controllability, editability, inpainting, and flexible trade-off between inference time and generation quality. Code and additional visualizations are available at <a href="https://unidisc.github.io" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Unified Multimodal Discrete Diffusion (UniDisc) model
    * Advantages over AR models in control and inpainting
    * Joint multimodal inpainting and generation
    * Flexible trade-off between inference time and quality
</details>
</details>

---


<details>
<summary><b> Harmonizing Visual Representations for Unified Multimodal Understanding and Generation</b></summary>

* **Authors:** Size Wu, Wenwei Zhang, Lumin Xu, Sheng Jin, Zhonghua Wu, Qingyi Tao, Wentao Liu, Wei Li, Chen Change Loy
* **arXiv ID:** 2503.21979
* **One-liner:** Introduced Harmon, a unified autoregressive framework harmonizing understanding and generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2503.21979) | [[PDF]](https://arxiv.org/pdf/2503.21979)

> **Core Innovation**
> Used a shared MAR encoder with a three-stage training procedure to achieve SOTA in generation and understanding.

<details>
    <summary>Abstract</summary>
    Unifying visual understanding and generation within a single multimodal framework remains a significant challenge, as the two inherently heterogeneous tasks require representations at different levels of granularity. Current approaches that utilize vector quantization (VQ) or variational autoencoders (VAE) for unified visual representation prioritize intrinsic imagery features over semantics, compromising understanding performance. In this work, we take inspiration from masked image modelling (MIM) that learns rich semantics via a mask-and-reconstruct pre-training and its successful extension to masked autoregressive (MAR) image generation. A preliminary study on the MAR encoder&#39;s representation reveals exceptional linear probing accuracy and precise feature response to visual concepts, which indicates MAR&#39;s potential for visual understanding tasks beyond its original generation role. Based on these insights, we present \emph{Harmon}, a unified autoregressive framework that harmonizes understanding and generation tasks with a shared MAR encoder. Through a three-stage training procedure that progressively optimizes understanding and generation capabilities, Harmon achieves state-of-the-art image generation results on the GenEval, MJHQ30K and WISE benchmarks while matching the performance of methods with dedicated semantic encoders (e.g., Janus) on image understanding benchmarks. Our code and models will be available at <a href="https://github.com/wusize/Harmon" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Shared MAR encoder for understanding and generation
    * Three-stage training procedure
    * State-of-the-art results on GenEval, MJHQ30K, and WISE
    * Matches dedicated semantic encoders in understanding
</details>
</details>

---


<details>
<summary><b> YoChameleon: Personalized Vision and Language Generation</b></summary>

* **Authors:** Thao Nguyen, Krishna Kumar Singh, Jing Shi, Trung Bui, Yong Jae Lee, Yuheng Li
* **arXiv ID:** 2504.20998
* **One-liner:** Introduced Yo'Chameleon, the first personalization method for large multimodal models.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2504.20998) | [[PDF]](https://arxiv.org/pdf/2504.20998)

> **Core Innovation**
> Enabled personalized image generation and understanding using soft-prompt tuning with few-shot examples.

<details>
    <summary>Abstract</summary>
    Large Multimodal Models (e.g., GPT-4, Gemini, Chameleon) have evolved into powerful tools with millions of users. However, they remain generic models and lack personalized knowledge of specific user concepts. Previous work has explored personalization for text generation, yet it remains unclear how these methods can be adapted to new modalities, such as image generation. In this paper, we introduce Yo&#39;Chameleon, the first attempt to study personalization for large multimodal models. Given 3-5 images of a particular concept, Yo&#39;Chameleon leverages soft-prompt tuning to embed subject-specific information to (i) answer questions about the subject and (ii) recreate pixel-level details to produce images of the subject in new contexts. Yo&#39;Chameleon is trained with (i) a self-prompting optimization mechanism to balance performance across multiple modalities, and (ii) a ``soft-positive&#34; image generation approach to enhance image quality in a few-shot setting.
</details>

<details>
    <summary>Key points</summary>
    * Soft-prompt tuning for personalization
    * Self-prompting optimization mechanism
    * Soft-positive image generation approach
    * Handles multiple modalities with few-shot data
</details>
</details>

---


<details>
<summary><b> X-Fusion: Introducing New Modality to Frozen Large Language Models</b></summary>

* **Authors:** Sicheng Mo, Thao Nguyen, Xun Huang, Siddharth Srinivasan Iyer, Yijun Li, Yuchen Liu, Abhishek Tandon, Eli Shechtman, Krishna Kumar Singh, Yong Jae Lee, Bolei Zhou, Yuheng Li
* **arXiv ID:** 2504.20996
* **One-liner:** Introduced X-Fusion, a framework extending LLMs for multimodal tasks while preserving language capabilities.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2504.20996) | [[PDF]](https://arxiv.org/pdf/2504.20996)

> **Core Innovation**
> Employed a dual-tower design with frozen LLM parameters for efficient multimodal integration.

<details>
    <summary>Abstract</summary>
    We propose X-Fusion, a framework that extends pretrained Large Language Models (LLMs) for multimodal tasks while preserving their language capabilities. X-Fusion employs a dual-tower design with modality-specific weights, keeping the LLM&#39;s parameters frozen while integrating vision-specific information for both understanding and generation. Our experiments demonstrate that X-Fusion consistently outperforms alternative architectures on both image-to-text and text-to-image tasks. We find that incorporating understanding-focused data improves generation quality, reducing image data noise enhances overall performance, and feature alignment accelerates convergence for smaller models but has minimal impact on larger ones. Our findings provide valuable insights into building efficient unified multimodal models.
</details>

<details>
    <summary>Key points</summary>
    * Dual-tower design with modality-specific weights
    * Keeps LLM parameters frozen
    * Improves generation with understanding-focused data
    * Feature alignment for convergence acceleration
</details>
</details>

---


<details>
<summary><b> Nexus-Gen: A Unified Model for Image Understanding, Generation, and Editing</b></summary>

* **Authors:** Hong Zhang, Zhongjie Duan, Xingjun Wang, Yingda Chen, Yuze Zhao, Yu Zhang
* **arXiv ID:** 2504.21356v1
* **One-liner:** Developed Nexus-Gen, a unified model combining LLM reasoning and diffusion model synthesis for multimodal tasks.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2504.21356v1) | [[PDF]](https://arxiv.org/pdf/2504.21356v1)

> **Core Innovation**
> Synergized language reasoning and image synthesis through dual-phase alignment training and prefilled autoregression.

<details>
    <summary>Abstract</summary>
    Unified multimodal large language models (MLLMs) aim to integrate multimodal understanding and generation abilities through a single framework. Despite their versatility, existing open-source unified models exhibit performance gaps against domain-specific architectures. To bridge this gap, we present Nexus-Gen, a unified model that synergizes the language reasoning capabilities of LLMs with the image synthesis power of diffusion models. To align the embedding space of the LLM and diffusion model, we conduct a dual-phase alignment training process. (1) The autoregressive LLM learns to predict image embeddings conditioned on multimodal inputs, while (2) the vision decoder is trained to reconstruct high-fidelity images from these embeddings. During training the LLM, we identified a critical discrepancy between the autoregressive paradigm&#39;s training and inference phases, where error accumulation in continuous embedding space severely degrades generation quality. To avoid this issue, we introduce a prefilled autoregression strategy that prefills input sequence with position-embedded special tokens instead of continuous embeddings. Through dual-phase training, Nexus-Gen has developed the integrated capability to comprehensively address the image understanding, generation and editing tasks. All models, datasets, and codes are published at <a href="https://github.com/modelscope/Nexus-Gen.git" rel="external noopener nofollow" class="link-external link-https">this https URL</a> to facilitate further advancements across the field.
</details>

<details>
    <summary>Key points</summary>
    * Dual-phase alignment training for embedding space alignment
    * Prefilled autoregression strategy to prevent error accumulation
    * Integration of LLM and diffusion model for unified capabilities
</details>
</details>

---


<details>
<summary><b> T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level and Token-level CoT</b></summary>

* **Authors:** Dongzhi Jiang, Ziyu Guo, Renrui Zhang, Zhuofan Zong, Hao Li, Le Zhuo, Shilin Yan, Pheng-Ann Heng, Hongsheng Li
* **arXiv ID:** 2505.00703
* **One-liner:** Introduced T2I-R1, a reasoning-enhanced text-to-image model using RL and bi-level CoT.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2505.00703) | [[PDF]](https://arxiv.org/pdf/2505.00703)

> **Core Innovation**
> Enhanced generation with semantic-level and token-level CoT reasoning optimized via BiCoT-GRPO.

<details>
    <summary>Abstract</summary>
    Recent advancements in large language models have demonstrated how chain-of-thought (CoT) and reinforcement learning (RL) can improve performance. However, applying such reasoning strategies to the visual generation domain remains largely unexplored. In this paper, we present T2I-R1, a novel reasoning-enhanced text-to-image generation model, powered by RL with a bi-level CoT reasoning process. Specifically, we identify two levels of CoT that can be utilized to enhance different stages of generation: (1) the semantic-level CoT for high-level planning of the prompt and (2) the token-level CoT for low-level pixel processing during patch-by-patch generation. To better coordinate these two levels of CoT, we introduce BiCoT-GRPO with an ensemble of generation rewards, which seamlessly optimizes both generation CoTs within the same training step. By applying our reasoning strategies to the baseline model, Janus-Pro, we achieve superior performance with 13% improvement on T2I-CompBench and 19% improvement on the WISE benchmark, even surpassing the state-of-the-art model FLUX.1. Code is available at: <a href="https://github.com/CaraJ7/T2I-R1" rel="external noopener nofollow" class="link-external link-https">this https URL</a>
</details>

<details>
    <summary>Key points</summary>
    * Bi-level CoT: semantic-level for prompt planning, token-level for pixel processing
    * BiCoT-GRPO with ensemble rewards for coordinated optimization
    * Application to baseline model Janus-Pro for performance gains
</details>
</details>

---


<details>
<summary><b> Ming-Lite-Uni: Advancements in Unified Architecture for Natural Multimodal Interaction</b></summary>

* **Authors:** Inclusion AI, Biao Gong, Cheng Zou, Dandan Zheng, Hu Yu, Jingdong Chen, Jianxin Sun, Junbo Zhao, Jun Zhou, Kaixiang Ji, Lixiang Ru, Libin Wang, Qingpei Guo, Rui Liu, Weilong Chai, Xinyu Xiao, Ziyuan Huang
* **arXiv ID:** 2505.02471
* **One-liner:** Created Ming-Lite-Uni, an open-source unified multimodal framework with native AR capabilities.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2505.02471) | [[PDF]](https://arxiv.org/pdf/2505.02471)

> **Core Innovation**
> Unified visual generator and multimodal AR model using multi-scale tokens and alignment.

<details>
    <summary>Abstract</summary>
    We introduce Ming-Lite-Uni, an open-source multimodal framework featuring a newly designed unified visual generator and a native multimodal autoregressive model tailored for unifying vision and language. Specifically, this project provides an open-source implementation of the integrated MetaQueries and M2-omni framework, while introducing the novel multi-scale learnable tokens and multi-scale representation alignment strategy. By leveraging a fixed MLLM and a learnable diffusion model, Ming-Lite-Uni enables native multimodal AR models to perform both text-to-image generation and instruction based image editing tasks, expanding their capabilities beyond pure visual understanding. Our experimental results demonstrate the strong performance of Ming-Lite-Uni and illustrate the impressive fluid nature of its interactive process. All code and model weights are open-sourced to foster further exploration within the community. Notably, this work aligns with concurrent multimodal AI milestones - such as ChatGPT-4o with native image generation updated in March 25, 2025 - underscoring the broader significance of unified models like Ming-Lite-Uni on the path toward AGI. Ming-Lite-Uni is in alpha stage and will soon be further refined.
</details>

<details>
    <summary>Key points</summary>
    * Multi-scale learnable tokens and representation alignment
    * Fixed MLLM and learnable diffusion model for generation and editing
    * Open-source implementation supporting interactive processes
</details>
</details>

---


<details>
<summary><b> TokLIP: Marry Visual Tokens to CLIP for Multimodal Comprehension and Generation</b></summary>

* **Authors:** Haokun Lin, Teng Wang, Yixiao Ge, Yuying Ge, Zhichao Lu, Ying Wei, Qingfu Zhang, Zhenan Sun, Ying Shan
* **arXiv ID:** 2505.05422
* **One-liner:** Proposed TokLIP, a visual tokenizer enhancing comprehension and generation with semanticized VQ tokens.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2505.05422) | [[PDF]](https://arxiv.org/pdf/2505.05422)

> **Core Innovation**
> Disentangled training objectives for comprehension and generation, enabling efficient AR training.

<details>
    <summary>Abstract</summary>
    Pioneering token-based works such as Chameleon and Emu3 have established a foundation for multimodal unification but face challenges of high training computational overhead and limited comprehension performance due to a lack of high-level semantics. In this paper, we introduce TokLIP, a visual tokenizer that enhances comprehension by semanticizing vector-quantized (VQ) tokens and incorporating CLIP-level semantics while enabling end-to-end multimodal autoregressive training with standard VQ tokens. TokLIP integrates a low-level discrete VQ tokenizer with a ViT-based token encoder to capture high-level continuous semantics. Unlike previous approaches (e.g., VILA-U) that discretize high-level features, TokLIP disentangles training objectives for comprehension and generation, allowing the direct application of advanced VQ tokenizers without the need for tailored quantization operations. Our empirical results demonstrate that TokLIP achieves exceptional data efficiency, empowering visual tokens with high-level semantic understanding while enhancing low-level generative capacity, making it well-suited for autoregressive Transformers in both comprehension and generation tasks. The code and models are available at <a href="https://github.com/TencentARC/TokLIP" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Semanticization of VQ tokens with CLIP-level semantics
    * ViT-based token encoder for high-level semantics
    * End-to-end training without tailored quantization
</details>
</details>

---


<details>
<summary><b> Selftok: Discrete Visual Tokens of Autoregression, by Diffusion, and for Reasoning</b></summary>

* **Authors:** Bohan Wang, Zhongqi Yue, Fengda Zhang, Shuo Chen, Li&#39;an Bi, Junzhe Zhang, Xue Song, Kennard Yanting Chan, Jiachun Pan, Weijia Wu, Mingze Zhou, Wang Lin, Kaihang Pan, Saining Zhang, Liyu Jia, Wentao Hu, Wei Zhao, Hanwang Zhang
* **arXiv ID:** 2505.07538
* **One-liner:** Introduced Selftok, a discrete visual tokenizer with AR prior for unified VLM and RL support.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2505.07538) | [[PDF]](https://arxiv.org/pdf/2505.07538)

> **Core Innovation**
> Enabled pure AR architecture for VLMs and effective RL in visual generation.

<details>
    <summary>Abstract</summary>
    We completely discard the conventional spatial prior in image representation and introduce a novel discrete visual tokenizer: Self-consistency Tokenizer (Selftok). At its design core, we compose an autoregressive (AR) prior -- mirroring the causal structure of language -- into visual tokens by using the reverse diffusion process of image generation. The AR property makes Selftok fundamentally distinct from traditional spatial tokens in the following two key ways: - Selftok offers an elegant and minimalist approach to unify diffusion and AR for vision-language models (VLMs): By representing images with Selftok tokens, we can train a VLM using a purely discrete autoregressive architecture -- like that in LLMs -- without requiring additional modules or training objectives. - We theoretically show that the AR prior satisfies the Bellman equation, whereas the spatial prior does not. Therefore, Selftok supports reinforcement learning (RL) for visual generation with effectiveness comparable to that achieved in LLMs. Besides the AR property, Selftok is also a SoTA tokenizer that achieves a favorable trade-off between high-quality reconstruction and compression rate. We use Selftok to build a pure AR VLM for both visual comprehension and generation tasks. Impressively, without using any text-image training pairs, a simple policy gradient RL working in the visual tokens can significantly boost the visual generation benchmark, surpassing all the existing models by a large margin. Therefore, we believe that Selftok effectively addresses the long-standing challenge that visual tokens cannot support effective RL. When combined with the well-established strengths of RL in LLMs, this brings us one step closer to realizing a truly multimodal LLM. Project Page: <a href="https://selftok-team.github.io/report/" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * AR prior from reverse diffusion process
    * Theoretical Bellman equation satisfaction for RL
    * High-quality reconstruction and compression trade-off
</details>
</details>

---


<details>
<summary><b> BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture, Training and Dataset</b></summary>

* **Authors:** Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, Le Xue, Caiming Xiong, Ran Xu
* **arXiv ID:** 2505.09568
* **One-liner:** Developed BLIP3-o, a unified model with diffusion transformer and sequential pretraining.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2505.09568) | [[PDF]](https://arxiv.org/pdf/2505.09568)

> **Core Innovation**
> Improved training efficiency and generative quality with curated datasets and innovative design.

<details>
    <summary>Abstract</summary>
    Unifying image understanding and generation has gained growing attention in recent research on multimodal models. Although design choices for image understanding have been extensively studied, the optimal model architecture and training recipe for a unified framework with image generation remain underexplored. Motivated by the strong potential of autoregressive and diffusion models for high-quality generation and scalability, we conduct a comprehensive study of their use in unified multimodal settings, with emphasis on image representations, modeling objectives, and training strategies. Grounded in these investigations, we introduce a novel approach that employs a diffusion transformer to generate semantically rich CLIP image features, in contrast to conventional VAE-based representations. This design yields both higher training efficiency and improved generative quality. Furthermore, we demonstrate that a sequential pretraining strategy for unified models-first training on image understanding and subsequently on image generation-offers practical advantages by preserving image understanding capability while developing strong image generation ability. Finally, we carefully curate a high-quality instruction-tuning dataset BLIP3o-60k for image generation by prompting GPT-4o with a diverse set of captions covering various scenes, objects, human gestures, and more. Building on our innovative model design, training recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art unified multimodal models. BLIP3-o achieves superior performance across most of the popular benchmarks spanning both image understanding and generation tasks. To facilitate future research, we fully open-source our models, including code, model weights, training scripts, and pretraining and instruction tuning datasets.
</details>

<details>
    <summary>Key points</summary>
    * Diffusion transformer for CLIP image features
    * Sequential pretraining: understanding then generation
    * BLIP3o-60k instruction-tuning dataset
</details>
</details>

---


<details>
<summary><b> Exploring the Deep Fusion of Large Language Models and Diffusion Transformers for Text-to-Image Synthesis</b></summary>

* **Authors:** Bingda Tang, Boyang Zheng, Xichen Pan, Sayak Paul, Saining Xie
* **arXiv ID:** 2505.10046
* **One-liner:** Conducted empirical study on LLM-DiT fusion for text-to-image generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2505.10046) | [[PDF]](https://arxiv.org/pdf/2505.10046)

> **Core Innovation**
> Provided controlled comparisons and reproducible training recipes for multimodal generation.

<details>
    <summary>Abstract</summary>
    This paper does not describe a new method; instead, it provides a thorough exploration of an important yet understudied design space related to recent advances in text-to-image synthesis -- specifically, the deep fusion of large language models (LLMs) and diffusion transformers (DiTs) for multi-modal generation. Previous studies mainly focused on overall system performance rather than detailed comparisons with alternative methods, and key design details and training recipes were often left undisclosed. These gaps create uncertainty about the real potential of this approach. To fill these gaps, we conduct an empirical study on text-to-image generation, performing controlled comparisons with established baselines, analyzing important design choices, and providing a clear, reproducible recipe for training at scale. We hope this work offers meaningful data points and practical guidelines for future research in multi-modal generation.
</details>

<details>
    <summary>Key points</summary>
    * Controlled comparisons with established baselines
    * Analysis of design choices and training strategies
    * Clear, scalable training recipe
</details>
</details>

---


<details>
<summary><b> End-to-End Vision Tokenizer Tuning</b></summary>

* **Authors:** Wenxuan Wang, Fan Zhang, Yufeng Cui, Haiwen Diao, Zhuoyan Luo, Huchuan Lu, Jing Liu, Xinlong Wang
* **arXiv ID:** 2505.10562
* **One-liner:** Proposed ETT, an end-to-end vision tokenizer tuning method for joint optimization.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2505.10562) | [[PDF]](https://arxiv.org/pdf/2505.10562)

> **Core Innovation**
> Enabled joint optimization of tokenization and AR tasks, improving performance without architecture changes.

<details>
    <summary>Abstract</summary>
    Existing vision tokenization isolates the optimization of vision tokenizers from downstream training, implicitly assuming the visual tokens can generalize well across various tasks, e.g., image generation and visual question answering. The vision tokenizer optimized for low-level reconstruction is agnostic to downstream tasks requiring varied representations and semantics. This decoupled paradigm introduces a critical misalignment: The loss of the vision tokenization can be the representation bottleneck for target tasks. For example, errors in tokenizing text in a given image lead to poor results when recognizing or generating them. To address this, we propose ETT, an end-to-end vision tokenizer tuning approach that enables joint optimization between vision tokenization and target autoregressive tasks. Unlike prior autoregressive models that use only discrete indices from a frozen vision tokenizer, ETT leverages the visual embeddings of the tokenizer codebook, and optimizes the vision tokenizers end-to-end with both reconstruction and caption objectives. ETT can be seamlessly integrated into existing training pipelines with minimal architecture modifications. Our ETT is simple to implement and integrate, without the need to adjust the original codebooks or architectures of the employed large language models. Extensive experiments demonstrate that our proposed end-to-end vision tokenizer tuning unlocks significant performance gains, i.e., 2-6% for multimodal understanding and visual generation tasks compared to frozen tokenizer baselines, while preserving the original reconstruction capability. We hope this very simple and strong method can empower multimodal foundation models besides image generation and understanding.
</details>

<details>
    <summary>Key points</summary>
    * End-to-end tuning with reconstruction and caption objectives
    * Integration into existing pipelines with minimal modifications
    * Performance gains in understanding and generation
</details>
</details>

---


<details>
<summary><b> UniCTokens: Boosting Personalized Understanding and Generation via Unified Concept Tokens</b></summary>

* **Authors:** Ruichuan An, Sihan Yang, Renrui Zhang, Zijun Shen, Ming Lu, Gaole Dai, Hao Liang, Ziyu Guo, Shilin Yan, Yulin Luo, Bocheng Zou, Chaoqun Yang, Wentao Zhang
* **arXiv ID:** 2505.14671
* **One-liner:** Introduced UniCTokens for unified personalized VLM with attribute-reasoning generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2505.14671) | [[PDF]](https://arxiv.org/pdf/2505.14671)

> **Core Innovation**
> Enhanced mutual benefits between understanding and generation through progressive training.

<details>
    <summary>Abstract</summary>
    Personalized models have demonstrated remarkable success in understanding and generating concepts provided by users. However, existing methods use separate concept tokens for understanding and generation, treating these tasks in isolation. This may result in limitations for generating images with complex prompts. For example, given the concept $\langle bo\rangle$, generating &#34;$\langle bo\rangle$ wearing its hat&#34; without additional textual descriptions of its hat. We call this kind of generation \textit{\textbf{personalized attribute-reasoning generation}}. To address the limitation, we present UniCTokens, a novel framework that effectively integrates personalized information into a unified vision language model (VLM) for understanding and generation. UniCTokens trains a set of unified concept tokens to leverage complementary semantics, boosting two personalized tasks. Moreover, we propose a progressive training strategy with three stages: understanding warm-up, bootstrapping generation from understanding, and deepening understanding from generation to enhance mutual benefits between both tasks. To quantitatively evaluate the unified VLM personalization, we present UnifyBench, the first benchmark for assessing concept understanding, concept generation, and attribute-reasoning generation. Experimental results on UnifyBench indicate that UniCTokens shows competitive performance compared to leading methods in concept understanding, concept generation, and achieving state-of-the-art results in personalized attribute-reasoning generation. Our research demonstrates that enhanced understanding improves generation, and the generation process can yield valuable insights into understanding. Our code and dataset will be released at: \href{<a href="https://github.com/arctanxarc/UniCTokens" rel="external noopener nofollow" class="link-external link-https">this https URL</a>}{<a href="https://github.com/arctanxarc/UniCTokens" rel="external noopener nofollow" class="link-external link-https">this https URL</a>}.
</details>

<details>
    <summary>Key points</summary>
    * Unified concept tokens for complementary semantics
    * Progressive training strategy in three stages
    * UnifyBench benchmark for evaluation
</details>
</details>

---


<details>
<summary><b> UniGen: Enhanced Training &amp; Test-Time Strategies for Unified Multimodal Understanding and Generation</b></summary>

* **Authors:** Rui Tian, Mingfei Gao, Mingze Xu, Jiaming Hu, Jiasen Lu, Zuxuan Wu, Yinfei Yang, Afshin Dehghan
* **arXiv ID:** 2505.14682
* **One-liner:** Developed UniGen, a unified MLLM with CoT-V strategy for test-time scaling.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2505.14682) | [[PDF]](https://arxiv.org/pdf/2505.14682)

> **Core Innovation**
> Achieved SOTA performance through full training pipeline and CoT verification.

<details>
    <summary>Abstract</summary>
    We introduce UniGen, a unified multimodal large language model (MLLM) capable of image understanding and generation. We study the full training pipeline of UniGen from a data-centric perspective, including multi-stage pre-training, supervised fine-tuning, and direct preference optimization. More importantly, we propose a new Chain-of-Thought Verification (CoT-V) strategy for test-time scaling, which significantly boosts UniGen&#39;s image generation quality using a simple Best-of-N test-time strategy. Specifically, CoT-V enables UniGen to act as both image generator and verifier at test time, assessing the semantic alignment between a text prompt and its generated image in a step-by-step CoT manner. Trained entirely on open-source datasets across all stages, UniGen achieves state-of-the-art performance on a range of image understanding and generation benchmarks, with a final score of 0.78 on GenEval and 85.19 on DPG-Bench. Through extensive ablation studies, our work provides actionable insights and addresses key challenges in the full life cycle of building unified MLLMs, contributing meaningful directions to the future research.
</details>

<details>
    <summary>Key points</summary>
    * Chain-of-Thought Verification (CoT-V) for test-time scaling
    * Multi-stage training: pre-training, fine-tuning, DPO
    * Open-source dataset usage and ablation studies
</details>
</details>

---


<details>
<summary><b> OpenUni: A Simple Baseline for Unified Multimodal Understanding and Generation</b></summary>

* **Authors:** Size Wu, Zhonghua Wu, Zerui Gong, Qingyi Tao, Sheng Jin, Qinyue Li, Wei Li, Chen Change Loy
* **arXiv ID:** 2505.23661
* **One-liner:** Developed OpenUni, a lightweight open-source baseline for unified multimodal understanding and generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2505.23661) | [[PDF]](https://arxiv.org/pdf/2505.23661)

> **Core Innovation**
> Achieved high-quality image generation and exceptional benchmark performance with minimal parameters by bridging LLMs and diffusion models via learnable queries and a transformer connector.

<details>
    <summary>Abstract</summary>
    In this report, we present OpenUni, a simple, lightweight, and fully open-source baseline for unifying multimodal understanding and generation. Inspired by prevailing practices in unified model learning, we adopt an efficient training strategy that minimizes the training complexity and overhead by bridging the off-the-shelf multimodal large language models (LLMs) and diffusion models through a set of learnable queries and a light-weight transformer-based connector. With a minimalist choice of architecture, we demonstrate that OpenUni can: 1) generate high-quality and instruction-aligned images, and 2) achieve exceptional performance on standard benchmarks such as GenEval, DPG- Bench, and WISE, with only 1.1B and 3.1B activated parameters. To support open research and community advancement, we release all model weights, training code, and our curated training datasets (including 23M image-text pairs) at <a href="https://github.com/wusize/OpenUni" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Adopted an efficient training strategy with learnable queries
    * Used a lightweight transformer-based connector
    * Minimized training complexity and overhead
    * Released model weights, code, and datasets for open research
</details>
</details>

---


<details>
<summary><b> Muddit: Liberating Generation Beyond Text-to-Image with a Unified Discrete Diffusion Model</b></summary>

* **Authors:** Qingyu Shi, Jinbin Bai, Zhuoran Zhao, Wenhao Chai, Kaidong Yu, Jianzong Wu, Shuangyong Song, Yunhai Tong, Xiangtai Li, Xuelong Li, Shuicheng Yan
* **arXiv ID:** 2505.23606
* **One-liner:** Introduced Muddit, a unified discrete diffusion transformer for fast parallel generation across text and image modalities.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2505.23606) | [[PDF]](https://arxiv.org/pdf/2505.23606)

> **Core Innovation**
> Enabled fast and parallel multimodal generation by integrating strong visual priors from a pretrained backbone with a lightweight text decoder under a unified architecture.

<details>
    <summary>Abstract</summary>
    Unified generation models aim to handle diverse tasks across modalities -- such as text generation, image generation, and vision-language reasoning -- within a single architecture and decoding paradigm. Autoregressive unified models suffer from slow inference due to sequential decoding, and non-autoregressive unified models suffer from weak generalization due to limited pretrained backbones. We introduce Muddit, a unified discrete diffusion transformer that enables fast and parallel generation across both text and image modalities. Unlike prior unified diffusion models trained from scratch, Muddit integrates strong visual priors from a pretrained text-to-image backbone with a lightweight text decoder, enabling flexible and high-quality multimodal generation under a unified architecture. Empirical results show that Muddit achieves competitive or superior performance compared to significantly larger autoregressive models in both quality and efficiency. The work highlights the potential of purely discrete diffusion, when equipped with strong visual priors, as a scalable and effective backbone for unified generation.
</details>

<details>
    <summary>Key points</summary>
    * Utilized discrete diffusion for scalable generation
    * Integrated pretrained text-to-image backbone
    * Employed lightweight text decoder
    * Achieved competitive performance in quality and efficiency
</details>
</details>

---


<details>
<summary><b> UniRL: Self-Improving Unified Multimodal Models via Supervised and Reinforcement Learning</b></summary>

* **Authors:** Weijia Mao, Zhenheng Yang, Mike Zheng Shou
* **arXiv ID:** 2505.23380
* **One-liner:** Proposed UniRL, a self-improving post-training approach for unified multimodal models.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2505.23380) | [[PDF]](https://arxiv.org/pdf/2505.23380)

> **Core Innovation**
> Enhanced model performance by generating and using images as training data in iterations, improving both generation and understanding without external data.

<details>
    <summary>Abstract</summary>
    Unified multimodal large language models such as Show-o and Janus have achieved strong performance across both generation and understanding tasks. However, these models typically rely on large-scale datasets and require substantial computation during the pretraining stage. In addition, several post-training methods have been proposed, but they often depend on external data or are limited to task-specific customization. In this work, we introduce UniRL, a self-improving post-training approach. Our approach enables the model to generate images from prompts and use them as training data in each iteration, without relying on any external image data. Moreover, it enables the two tasks to enhance each other: the generated images are used for understanding, and the understanding results are used to supervise generation. We explore supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO) to optimize the models. UniRL offers three key advantages: (1) it requires no external image data, as all training samples are generated by the model itself during training; (2) it not only improves individual task performance, but also reduces the imbalance between generation and understanding; and (3) it requires only several additional training steps during the post-training stage. We evaluate UniRL on top of Show-o and Janus, achieving a GenEval score of 0.77 for Show-o and 0.65 for Janus. Code and models will be released in <a href="https://github.com/showlab/UniRL" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Used generated images for training without external data
    * Applied supervised fine-tuning and GRPO
    * Enabled mutual enhancement between tasks
    * Required only additional post-training steps
</details>
</details>

---


<details>
<summary><b> Are Unified Vision-Language Models Necessary: Generalization Across Understanding and Generation</b></summary>

* **Authors:** Jihai Zhang, Tianle Li, Linjie Li, Zhengyuan Yang, Yu Cheng
* **arXiv ID:** 2505.23043
* **One-liner:** Systematically investigated generalization across understanding and generation tasks in unified VLMs.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2505.23043) | [[PDF]](https://arxiv.org/pdf/2505.23043)

> **Core Innovation**
> Found mutual benefits between understanding and generation tasks, with better alignment and cross-task knowledge transfer in unified architectures.

<details>
    <summary>Abstract</summary>
    Recent advancements in unified vision-language models (VLMs), which integrate both visual understanding and generation capabilities, have attracted significant attention. The underlying hypothesis is that a unified architecture with mixed training on both understanding and generation tasks can enable mutual enhancement between understanding and generation. However, this hypothesis remains underexplored in prior works on unified VLMs. To address this gap, this paper systematically investigates the generalization across understanding and generation tasks in unified VLMs. Specifically, we design a dataset closely aligned with real-world scenarios to facilitate extensive experiments and quantitative evaluations. We evaluate multiple unified VLM architectures to validate our findings. Our key findings are as follows. First, unified VLMs trained with mixed data exhibit mutual benefits in understanding and generation tasks across various architectures, and this mutual benefits can scale up with increased data. Second, better alignment between multimodal input and output spaces will lead to better generalization. Third, the knowledge acquired during generation tasks can transfer to understanding tasks, and this cross-task generalization occurs within the base language model, beyond modality adapters. Our findings underscore the critical necessity of unifying understanding and generation in VLMs, offering valuable insights for the design and optimization of unified VLMs.
</details>

<details>
    <summary>Key points</summary>
    * Designed a real-world aligned dataset for experiments
    * Evaluated multiple unified VLM architectures
    * Identified mutual benefits and scaling with data
    * Highlighted cross-task generalization within base models
</details>
</details>

---


<details>
<summary><b> UniWorld-V1: High-Resolution Semantic Encoders for Unified Visual Understanding and Generation</b></summary>

* **Authors:** Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, Yatian Pang, Li Yuan
* **arXiv ID:** 2506.03147
* **One-liner:** Developed UniWorld-V1, a unified generative framework for image understanding, generation, manipulation, and perception.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2506.03147) | [[PDF]](https://arxiv.org/pdf/2506.03147)

> **Core Innovation**
> Achieved impressive performance across diverse tasks using semantic features from multimodal LLMs and contrastive encoders with minimal training data.

<details>
    <summary>Abstract</summary>
    Although existing unified models achieve strong performance in vision-language understanding and text-to-image generation, they remain limited in addressing image perception and manipulation -- capabilities increasingly demanded in practical applications. Recently, OpenAI introduced the powerful GPT-4o-Image model, which showcases advanced capabilities in comprehensive image perception and manipulation, sparking widespread interest. Through carefully designed experiments, we observe that GPT-4o-Image likely relies on semantic encoders rather than VAEs for feature extraction, despite VAEs being commonly regarded as crucial for image manipulation tasks. Inspired by this insight, we propose UniWorld-V1, a unified generative framework built upon semantic features extracted from powerful multimodal large language models and contrastive semantic encoders. Using only 2.7M training data, UniWorld-V1 achieves impressive performance across diverse tasks, including image understanding, generation, manipulation, and perception. We fully open-source the UniWorld-V1 framework, including model weights, training and evaluation scripts, and datasets to promote reproducibility and further research.
</details>

<details>
    <summary>Key points</summary>
    * Built on semantic features from multimodal LLMs
    * Used contrastive semantic encoders
    * Trained with only 2.7M data
    * Open-sourced framework for reproducibility
</details>
</details>

---


<details>
<summary><b> LaTtE-Flow: Layerwise Timestep-Expert Flow-based Transformer</b></summary>

* **Authors:** Ying Shen, Zhiyang Xu, Jiuhai Chen, Shizhe Diao, Jiaxin Zhang, Yuguang Yao, Joy Rimchala, Ismini Lourentzou, Lifu Huang
* **arXiv ID:** 2506.06952
* **One-liner:** Proposed LaTtE-Flow, an efficient architecture unifying image understanding and generation with fast inference.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2506.06952) | [[PDF]](https://arxiv.org/pdf/2506.06952)

> **Core Innovation**
> Improved sampling efficiency and inference speed by distributing flow-matching across specialized layer groups and using timestep-conditioned residual attention.

<details>
    <summary>Abstract</summary>
    Recent advances in multimodal foundation models unifying image understanding and generation have opened exciting avenues for tackling a wide range of vision-language tasks within a single framework. Despite progress, existing unified models typically require extensive pretraining and struggle to achieve the same level of performance compared to models dedicated to each task. Additionally, many of these models suffer from slow image generation speeds, limiting their practical deployment in real-time or resource-constrained settings. In this work, we propose Layerwise Timestep-Expert Flow-based Transformer (LaTtE-Flow), a novel and efficient architecture that unifies image understanding and generation within a single multimodal model. LaTtE-Flow builds upon powerful pretrained Vision-Language Models (VLMs) to inherit strong multimodal understanding capabilities, and extends them with a novel Layerwise Timestep Experts flow-based architecture for efficient image generation. LaTtE-Flow distributes the flow-matching process across specialized groups of Transformer layers, each responsible for a distinct subset of timesteps. This design significantly improves sampling efficiency by activating only a small subset of layers at each sampling timestep. To further enhance performance, we propose a Timestep-Conditioned Residual Attention mechanism for efficient information reuse across layers. Experiments demonstrate that LaTtE-Flow achieves strong performance on multimodal understanding tasks, while achieving competitive image generation quality with around 6x faster inference speed compared to recent unified multimodal models.
</details>

<details>
    <summary>Key points</summary>
    * Extended pretrained VLMs with flow-based architecture
    * Distributed flow-matching across layer groups
    * Used timestep-conditioned residual attention
    * Achieved 6x faster inference with competitive quality
</details>
</details>

---


<details>
<summary><b> Symmetrical Flow Matching: Unified Image Generation, Segmentation, and Classification with Score-Based Generative Models</b></summary>

* **Authors:** Francisco Caetano, Christiaan Viviers, Peter H.N. De With, Fons van der Sommen
* **arXiv ID:** 2506.10634
* **One-liner:** Introduced SymmFlow, a symmetrical flow matching framework for unifying semantic segmentation, classification, and image generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2506.10634) | [[PDF]](https://arxiv.org/pdf/2506.10634)

> **Core Innovation**
> Enabled joint modeling of forward and reverse transformations with bi-directional consistency, achieving state-of-the-art performance in semantic image synthesis.

<details>
    <summary>Abstract</summary>
    Flow Matching has emerged as a powerful framework for learning continuous transformations between distributions, enabling high-fidelity generative modeling. This work introduces Symmetrical Flow Matching (SymmFlow), a new formulation that unifies semantic segmentation, classification, and image generation within a single model. Using a symmetric learning objective, SymmFlow models forward and reverse transformations jointly, ensuring bi-directional consistency, while preserving sufficient entropy for generative diversity. A new training objective is introduced to explicitly retain semantic information across flows, featuring efficient sampling while preserving semantic structure, allowing for one-step segmentation and classification without iterative refinement. Unlike previous approaches that impose strict one-to-one mapping between masks and images, SymmFlow generalizes to flexible conditioning, supporting both pixel-level and image-level class labels. Experimental results on various benchmarks demonstrate that SymmFlow achieves state-of-the-art performance on semantic image synthesis, obtaining FID scores of 11.9 on CelebAMask-HQ and 7.0 on COCO-Stuff with only 25 inference steps. Additionally, it delivers competitive results on semantic segmentation and shows promising capabilities in classification tasks. The code will be publicly available.
</details>

<details>
    <summary>Key points</summary>
    * Used symmetric learning objective for bi-directional consistency
    * Introduced semantic retention training objective
    * Supported flexible conditioning with labels
    * Achieved efficient one-step segmentation and classification
</details>
</details>

---


<details>
<summary><b> Scale Your Instructions: Enhance the Instruction-Following Fidelity of Unified Image Generation Model by Self-Adaptive Attention Scaling</b></summary>

* **Authors:** Chao Zhou, Tianyi Wei, Nenghai Yu
* **arXiv ID:** 2507.16240
* **One-liner:** Proposed SaaS, a method to address text instruction neglect in unified image generation models.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2507.16240) | [[PDF]](https://arxiv.org/pdf/2507.16240)

> **Core Innovation**
> Enhanced instruction-following fidelity by dynamically scaling attention activations based on cross-attention consistency between timesteps, without additional training.

<details>
    <summary>Abstract</summary>
    Recent advancements in unified image generation models, such as OmniGen, have enabled the handling of diverse image generation and editing tasks within a single framework, accepting multimodal, interleaved texts and images in free form. This unified architecture eliminates the need for text encoders, greatly reducing model complexity and standardizing various image generation and editing tasks, making it more user-friendly. However, we found that it suffers from text instruction neglect, especially when the text instruction contains multiple sub-instructions. To explore this issue, we performed a perturbation analysis on the input to identify critical steps and layers. By examining the cross-attention maps of these key steps, we observed significant conflicts between neglected sub-instructions and the activations of the input image. In response, we propose Self-Adaptive Attention Scaling (SaaS), a method that leverages the consistency of cross-attention between adjacent timesteps to dynamically scale the attention activation for each sub-instruction. Our SaaS enhances instruction-following fidelity without requiring additional training or test-time optimization. Experimental results on instruction-based image editing and visual conditional image generation validate the effectiveness of our SaaS, showing superior instruction-following fidelity over existing methods. The code is available <a href="https://github.com/zhouchao-ops/SaaS" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Identified key steps and layers via perturbation analysis
    * Leveraged cross-attention consistency for dynamic scaling
    * Applied to image editing and generation tasks
    * Required no extra training or optimization
</details>
</details>

---


<details>
<summary><b> OneReward: Unified Mask-Guided Image Generation via Multi-Task Human Preference Learning</b></summary>

* **Authors:** Yuan Gong, Xionghui Wang, Jie Wu, Shiyin Wang, Yitong Wang, Xinglong Wu
* **arXiv ID:** 2508.21066
* **One-liner:** Developed OneReward, a unified reinforcement learning framework for multi-task generation using a single reward model.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2508.21066) | [[PDF]](https://arxiv.org/pdf/2508.21066)

> **Core Innovation**
> Enabled consistent performance across diverse tasks by employing a vision-language model as a generative reward model, eliminating task-specific fine-tuning.

<details>
    <summary>Abstract</summary>
    In this paper, we introduce OneReward, a unified reinforcement learning framework that enhances the model&#39;s generative capabilities across multiple tasks under different evaluation criteria using only \textit{One Reward} model. By employing a single vision-language model (VLM) as the generative reward model, which can distinguish the winner and loser for a given task and a given evaluation criterion, it can be effectively applied to multi-task generation models, particularly in contexts with varied data and diverse task objectives. We utilize OneReward for mask-guided image generation, which can be further divided into several sub-tasks such as image fill, image extend, object removal, and text rendering, involving a binary mask as the edit area. Although these domain-specific tasks share same conditioning paradigm, they differ significantly in underlying data distributions and evaluation metrics. Existing methods often rely on task-specific supervised fine-tuning (SFT), which limits generalization and training efficiency. Building on OneReward, we develop Seedream 3.0 Fill, a mask-guided generation model trained via multi-task reinforcement learning directly on a pre-trained base model, eliminating the need for task-specific SFT. Experimental results demonstrate that our unified edit model consistently outperforms both commercial and open-source competitors, such as Ideogram, Adobe Photoshop, and FLUX Fill [Pro], across multiple evaluation dimensions. Code and model are available at: <a href="https://one-reward.github.io" rel="external noopener nofollow" class="link-external link-https">this https URL</a>
</details>

<details>
    <summary>Key points</summary>
    * Used single VLM as generative reward model
    * Applied to mask-guided image generation tasks
    * Eliminated need for task-specific SFT
    * Outperformed competitors in evaluations
</details>
</details>

---


<details>
<summary><b> Reconstruction Alignment Improves Unified Multimodal Models</b></summary>

* **Authors:** Ji Xie, Trevor Darrell, Luke Zettlemoyer, XuDong Wang
* **arXiv ID:** 2509.07295
* **One-liner:** Introduced RecA, a resource-efficient post-training method for aligning understanding and generation in unified multimodal models.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2509.07295) | [[PDF]](https://arxiv.org/pdf/2509.07295)

> **Core Innovation**
> Improved generation and editing fidelity by conditioning models on their own visual embeddings and optimizing with self-supervised reconstruction loss.

<details>
    <summary>Abstract</summary>
    Unified multimodal models (UMMs) unify visual understanding and generation within a single architecture. However, conventional training relies on image-text pairs (or sequences) whose captions are typically sparse and miss fine-grained visual details--even when they use hundreds of words to describe a simple image. We introduce Reconstruction Alignment (RecA), a resource-efficient post-training method that leverages visual understanding encoder embeddings as dense &#34;text prompts,&#34; providing rich supervision without captions. Concretely, RecA conditions a UMM on its own visual understanding embeddings and optimizes it to reconstruct the input image with a self-supervised reconstruction loss, thereby realigning understanding and generation. Despite its simplicity, RecA is broadly applicable: across autoregressive, masked-autoregressive, and diffusion-based UMMs, it consistently improves generation and editing fidelity. With only 27 GPU-hours, post-training with RecA substantially improves image generation performance on GenEval (0.73$\rightarrow$0.90) and DPGBench (80.93$\rightarrow$88.15), while also boosting editing benchmarks (ImgEdit 3.38$\rightarrow$3.75, GEdit 6.94$\rightarrow$7.25). Notably, RecA surpasses much larger open-source models and applies broadly across diverse UMM architectures, establishing it as an efficient and general post-training alignment strategy for UMMs
</details>

<details>
    <summary>Key points</summary>
    * Leveraged visual understanding embeddings as dense prompts
    * Used self-supervised reconstruction loss
    * Applied across diverse UMM architectures
    * Achieved significant benchmark improvements with low resource cost
</details>
</details>

---


<details>
<summary><b> GenExam: A Multidisciplinary Text-to-Image Exam</b></summary>

* **Authors:** Zhaokai Wang, Penghao Yin, Xiangyu Zhao, Changyao Tian, Yu Qiao, Wenhai Wang, Jifeng Dai, Gen Luo
* **arXiv ID:** 2509.14232
* **One-liner:** Introduced GenExam, the first benchmark for multidisciplinary text-to-image exams.
* **Published in:** 
* **Links:** [[Paper]](https://www.arxiv.org/abs/2509.14232) | [[PDF]](https://www.arxiv.org/pdf/2509.14232)

> **Core Innovation**
> Established a rigorous evaluation framework for image generation models by framing it as an exam with fine-grained scoring.

<details>
    <summary>Abstract</summary>
    Exams are a fundamental test of expert-level intelligence and require integrated understanding, reasoning, and generation. Existing exam-style benchmarks mainly focus on understanding and reasoning tasks, and current generation benchmarks emphasize the illustration of world knowledge and visual concepts, neglecting the evaluation of rigorous drawing exams. We introduce GenExam, the first benchmark for multidisciplinary text-to-image exams, featuring 1,000 samples across 10 subjects with exam-style prompts organized under a four-level taxonomy. Each problem is equipped with ground-truth images and fine-grained scoring points to enable a precise evaluation of semantic correctness and visual plausibility. Experiments show that even state-of-the-art models such as GPT-Image-1 and Gemini-2.5-Flash-Image achieve less than 15% strict scores, and most models yield almost 0%, suggesting the great challenge of our benchmark. By framing image generation as an exam, GenExam offers a rigorous assessment of models&#39; ability to integrate understanding, reasoning, and generation, providing insights on the path to general AGI. Our benchmark and evaluation code are released at <a href="https://github.com/OpenGVLab/GenExam" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Developed a benchmark with 1,000 samples across 10 subjects
    * Organized prompts under a four-level taxonomy
    * Provided ground-truth images and scoring points for precise evaluation
    * Showed state-of-the-art models achieve low scores, highlighting the challenge
</details>
</details>

---


<details>
<summary><b> MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid Vision Tokenizer</b></summary>

* **Authors:** Yanghao Li, Rui Qian, Bowen Pan, Haotian Zhang, Haoshuo Huang, Bowen Zhang, Jialing Tong, Haoxuan You, Xianzhi Du, Zhe Gan, Hyunjik Kim, Chao Jia, Zhenbang Wang, Yinfei Yang, Mingfei Gao, Zi-Yi Dou, Wenze Hu, Chang Gao, Dongxu Li, Philipp Dufter, Zirui Wang, Guoli Yin, Zhengdong Zhang, Chen Chen, Yang Zhao, Ruoming Pang, Zhifeng Chen
* **arXiv ID:** 2509.16197
* **One-liner:** Presented Manzano, a unified framework reducing performance trade-off between understanding and generation in multimodal LLMs.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2509.16197) | [[PDF]](https://arxiv.org/pdf/2509.16197)

> **Core Innovation**
> Achieved state-of-the-art results among unified models by coupling a hybrid image tokenizer with a curated training recipe.

<details>
    <summary>Abstract</summary>
    Unified multimodal Large Language Models (LLMs) that can both understand and generate visual content hold immense potential. However, existing open-source models often suffer from a performance trade-off between these capabilities. We present Manzano, a simple and scalable unified framework that substantially reduces this tension by coupling a hybrid image tokenizer with a well-curated training recipe. A single shared vision encoder feeds two lightweight adapters that produce continuous embeddings for image-to-text understanding and discrete tokens for text-to-image generation within a common semantic space. A unified autoregressive LLM predicts high-level semantics in the form of text and image tokens, with an auxiliary diffusion decoder subsequently translating the image tokens into pixels. The architecture, together with a unified training recipe over understanding and generation data, enables scalable joint learning of both capabilities. Manzano achieves state-of-the-art results among unified models, and is competitive with specialist models, particularly on text-rich evaluation. Our studies show minimal task conflicts and consistent gains from scaling model size, validating our design choice of a hybrid tokenizer.
</details>

<details>
    <summary>Key points</summary>
    * Used a shared vision encoder with lightweight adapters for continuous and discrete tokens
    * Employed a unified autoregressive LLM for text and image token prediction
    * Integrated an auxiliary diffusion decoder for pixel generation
    * Demonstrated minimal task conflicts and gains from scaling model size
</details>
</details>

---


<details>
<summary><b> EditVerse: Unifying Image and Video Editing and Generation with In-Context Learning</b></summary>

* **Authors:** Xuan Ju, Tianyu Wang, Yuqian Zhou, He Zhang, Qing Liu, Nanxuan Zhao, Zhifei Zhang, Yijun Li, Yuanhao Cai, Shaoteng Liu, Daniil Pakhomov, Zhe Lin, Soo Ye Kim, Qiang Xu
* **arXiv ID:** 2509.20360
* **One-liner:** Introduced EditVerse, a unified framework for image and video generation and editing in a single model.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2509.20360) | [[PDF]](https://arxiv.org/pdf/2509.20360)

> **Core Innovation**
> Enabled robust in-context learning and cross-modal knowledge transfer by representing all modalities as unified token sequences.

<details>
    <summary>Abstract</summary>
    Recent advances in foundation models highlight a clear trend toward unification and scaling, showing emergent capabilities across diverse domains. While image generation and editing have rapidly transitioned from task-specific to unified frameworks, video generation and editing remain fragmented due to architectural limitations and data scarcity. In this work, we introduce EditVerse, a unified framework for image and video generation and editing within a single model. By representing all modalities, i.e., text, image, and video, as a unified token sequence, EditVerse leverages self-attention to achieve robust in-context learning, natural cross-modal knowledge transfer, and flexible handling of inputs and outputs with arbitrary resolutions and durations. To address the lack of video editing training data, we design a scalable data pipeline that curates 232K video editing samples and combines them with large-scale image and video datasets for joint training. Furthermore, we present EditVerseBench, the first benchmark for instruction-based video editing covering diverse tasks and resolutions. Extensive experiments and user studies demonstrate that EditVerse achieves state-of-the-art performance, surpassing existing open-source and commercial models, while exhibiting emergent editing and generation abilities across modalities.
</details>

<details>
    <summary>Key points</summary>
    * Represented text, image, and video as unified token sequences
    * Designed a scalable data pipeline with 232K video editing samples
    * Created EditVerseBench for instruction-based video editing evaluation
    * Achieved state-of-the-art performance and emergent abilities across modalities
</details>
</details>

---


<details>
<summary><b> UniAlignment: Semantic Alignment for Unified Image Generation, Understanding, Manipulation and Perception</b></summary>

* **Authors:** Xinyang Song, Libin Wang, Weining Wang, Shaozhen Liu, Dandan Zheng, Jingdong Chen, Qi Li, Zhenan Sun
* **arXiv ID:** 2509.23760
* **One-liner:** Proposed UniAlignment, a unified multimodal generation framework within a single diffusion transformer.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2509.23760) | [[PDF]](https://arxiv.org/pdf/2509.23760)

> **Core Innovation**
> Enhanced cross-modal consistency and instruction-following robustness through dual-stream diffusion training.

<details>
    <summary>Abstract</summary>
    The remarkable success of diffusion models in text-to-image generation has sparked growing interest in expanding their capabilities to a variety of multi-modal tasks, including image understanding, manipulation, and perception. These tasks require advanced semantic comprehension across both visual and textual modalities, especially in scenarios involving complex semantic instructions. However, existing approaches often rely heavily on vision-language models (VLMs) or modular designs for semantic guidance, leading to fragmented architectures and computational inefficiency. To address these challenges, we propose UniAlignment, a unified multimodal generation framework within a single diffusion transformer. UniAlignment introduces a dual-stream diffusion training strategy that incorporates both intrinsic-modal semantic alignment and cross-modal semantic alignment, thereby enhancing the model&#39;s cross-modal consistency and instruction-following robustness. Additionally, we present SemGen-Bench, a new benchmark specifically designed to evaluate multimodal semantic consistency under complex textual instructions. Extensive experiments across multiple tasks and benchmarks demonstrate that UniAlignment outperforms existing baselines, underscoring the significant potential of diffusion models in unified multimodal generation.
</details>

<details>
    <summary>Key points</summary>
    * Introduced intrinsic-modal and cross-modal semantic alignment in training
    * Developed SemGen-Bench for evaluating multimodal semantic consistency
    * Outperformed existing baselines across multiple tasks
    * Demonstrated potential of diffusion models in unified multimodal generation
</details>
</details>

---


<details>
<summary><b> Query-Kontext: An Unified Multimodal Model for Image Generation and Editing</b></summary>

* **Authors:** Yuxin Song, Wenkai Dong, Shizun Wang, Qi Zhang, Song Xue, Tao Yuan, Hu Yang, Haocheng Feng, Hang Zhou, Xinyan Xiao, Jingdong Wang
* **arXiv ID:** 2509.26641
* **One-liner:** Introduced Query-Kontext, a novel approach bridging VLM and diffusion models for multimodal generative reasoning.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2509.26641) | [[PDF]](https://arxiv.org/pdf/2509.26641)

> **Core Innovation**
> Delegated generative reasoning to VLM while using diffusion for high-fidelity synthesis, matching or outperforming state-of-the-art methods.

<details>
    <summary>Abstract</summary>
    Unified Multimodal Models (UMMs) have demonstrated remarkable performance in text-to-image generation (T2I) and editing (TI2I), whether instantiated as assembled unified frameworks which couple powerful vision-language model (VLM) with diffusion-based generator, or as naive Unified Multimodal Models with an early fusion of understanding and generation modalities. We contend that in current unified frameworks, the crucial capability of multimodal generative reasoning which encompasses instruction understanding, grounding, and image referring for identity preservation and faithful reconstruction, is intrinsically entangled with high-fidelity synthesis. In this work, we introduce Query-Kontext, a novel approach that bridges the VLM and diffusion model via a multimodal ``kontext&#39;&#39; composed of semantic cues and coarse-grained image conditions encoded from multimodal inputs. This design delegates the complex ability of multimodal generative reasoning to powerful VLM while reserving diffusion model&#39;s role for high-quality visual synthesis. To achieve this, we propose a three-stage progressive training strategy. First, we connect the VLM to a lightweight diffusion head via multimodal kontext tokens to unleash the VLM&#39;s generative reasoning ability. Second, we scale this head to a large, pre-trained diffusion model to enhance visual detail and realism. Finally, we introduce a low-level image encoder to improve image fidelity and perform instruction tuning on downstream tasks. Furthermore, we build a comprehensive data pipeline integrating real, synthetic, and open-source datasets, covering diverse multimodal reference-to-image scenarios, including image generation, instruction-driven editing, customized generation, and multi-subject composition. Experiments show that our approach matches strong unified baselines and even outperforms task-specific state-of-the-art methods in several cases.
</details>

<details>
    <summary>Key points</summary>
    * Used multimodal kontext tokens for semantic cues and image conditions
    * Implemented a three-stage progressive training strategy
    * Built a comprehensive data pipeline for diverse reference-to-image scenarios
    * Achieved competitive performance with unified and task-specific models
</details>
</details>

---


<details>
<summary><b> M6: A Chinese Multimodal Pretrainer</b></summary>

* **Authors:** Junyang Lin, Rui Men, An Yang, Chang Zhou, Ming Ding, Yichang Zhang, Peng Wang, Ang Wang, Le Jiang, Xianyan Jia, Jie Zhang, Jianwei Zhang, Xu Zou, Zhikang Li, Xiaodong Deng, Jie Liu, Jinbao Xue, Huiling Zhou, Jianxin Ma, Jin Yu, Yong Li, Wei Lin, Jingren Zhou, Jie Tang, Hongxia Yang
* **arXiv ID:** 2103.00823
* **One-liner:** Constructed the largest Chinese multimodal pretraining dataset and proposed the M6 model for unified pretraining.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2103.00823) | [[PDF]](https://arxiv.org/pdf/2103.00823)

> **Core Innovation**
> Scaled model size up to 100 billion parameters and demonstrated outstanding performance in downstream applications, including text-guided image generation.

<details>
    <summary>Abstract</summary>
    In this work, we construct the largest dataset for multimodal pretraining in Chinese, which consists of over 1.9TB images and 292GB texts that cover a wide range of domains. We propose a cross-modal pretraining method called M6, referring to Multi-Modality to Multi-Modality Multitask Mega-transformer, for unified pretraining on the data of single modality and multiple modalities. We scale the model size up to 10 billion and 100 billion parameters, and build the largest pretrained model in Chinese. We apply the model to a series of downstream applications, and demonstrate its outstanding performance in comparison with strong baselines. Furthermore, we specifically design a downstream task of text-guided image generation, and show that the finetuned M6 can create high-quality images with high resolution and abundant details.
</details>

<details>
    <summary>Key points</summary>
    * Built a dataset with over 1.9TB images and 292GB texts
    * Scaled M6 model to 10B and 100B parameters
    * Applied to various downstream tasks with high performance
    * Fine-tuned for high-quality image generation with high resolution
</details>
</details>

---


<details>
<summary><b> Flamingo: a Visual Language Model for Few-Shot Learning</b></summary>

* **Authors:** Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, Karen Simonyan
* **arXiv ID:** 2204.14198
* **One-liner:** Introduced Flamingo, a family of VLMs with in-context few-shot learning capabilities for multimodal tasks.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2204.14198) | [[PDF]](https://arxiv.org/pdf/2204.14198)

> **Core Innovation**
> Achieved state-of-the-art performance on various image and video tasks using few-shot learning without extensive fine-tuning.

<details>
    <summary>Abstract</summary>
    Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer; captioning tasks, which evaluate the ability to describe a scene or an event; and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data.
</details>

<details>
    <summary>Key points</summary>
    * Bridged pretrained vision-only and language-only models
    * Handled interleaved visual and textual data sequences
    * Trained on large-scale multimodal web corpora
    * Outperformed models fine-tuned on more data in benchmarks
</details>
</details>

---


<details>
<summary><b> SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs</b></summary>

* **Authors:** Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolfgang Macherey, Yanping Huang, David A. Ross, Irfan Essa, Yonatan Bisk, Ming-Hsuan Yang, Kevin Murphy, Alexander G. Hauptmann, Lu Jiang
* **arXiv ID:** 2306.17842
* **One-liner:** Introduced Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform multimodal understanding and generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2306.17842) | [[PDF]](https://arxiv.org/pdf/2306.17842)

> **Core Innovation**
> Enabled frozen LLMs to generate images and achieve over 25% improvement in understanding tasks, marking a first in such capabilities.

<details>
    <summary>Abstract</summary>
    In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos. SPAE converts between raw pixels and interpretable lexical tokens (or words) extracted from the LLM&#39;s vocabulary. The resulting tokens capture both the semantic meaning and the fine-grained details needed for visual reconstruction, effectively translating the visual content into a language comprehensible to the LLM, and empowering it to perform a wide array of multimodal tasks. Our approach is validated through in-context learning experiments with frozen PaLM 2 and GPT 3.5 on a diverse set of image understanding and generation tasks. Our method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25%.
</details>

<details>
    <summary>Key points</summary>
    * Converted pixels to interpretable lexical tokens from LLM vocabulary
    * Validated through in-context learning with frozen PaLM 2 and GPT 3.5
    * Surpassed state-of-the-art in image understanding under same setting
    * Empowered LLMs for diverse multimodal tasks
</details>
</details>

---


<details>
<summary><b> Emu: Generative Pretraining in Multimodality</b></summary>

* **Authors:** Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, Xinlong Wang
* **arXiv ID:** 2307.05222
* **One-liner:** Presented Emu, a Transformer-based multimodal foundation model for seamless image and text generation in multimodal contexts.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2307.05222) | [[PDF]](https://arxiv.org/pdf/2307.05222)

> **Core Innovation**
> Achieved superb performance in zero-shot/few-shot tasks across image and video modalities through unified autoregressive training.

<details>
    <summary>Abstract</summary>
    We present Emu, a Transformer-based multimodal foundation model, which can seamlessly generate images and texts in multimodal context. This omnivore model can take in any single-modality or multimodal data input indiscriminately (e.g., interleaved image, text and video) through a one-model-for-all autoregressive training process. First, visual signals are encoded into embeddings, and together with text tokens form an interleaved input sequence. Emu is then end-to-end trained with a unified objective of classifying the next text token or regressing the next visual embedding in the multimodal sequence. This versatile multimodality empowers the exploration of diverse pretraining data sources at scale, such as videos with interleaved frames and text, webpages with interleaved images and text, as well as web-scale image-text pairs and video-text pairs. Emu can serve as a generalist multimodal interface for both image-to-text and text-to-image tasks, and supports in-context image and text generation. Across a broad range of zero-shot/few-shot tasks including image captioning, visual question answering, video question answering and text-to-image generation, Emu demonstrates superb performance compared to state-of-the-art large multimodal models. Extended capabilities such as multimodal assistants via instruction tuning are also demonstrated with impressive performance.
</details>

<details>
    <summary>Key points</summary>
    * Encoded visual signals into embeddings for interleaved input sequences
    * Trained with unified objective for next token or embedding prediction
    * Supported in-context image and text generation
    * Demonstrated extended capabilities via instruction tuning
</details>
</details>

---


<details>
<summary><b> NExT-GPT: Any-to-Any Multimodal LLM</b></summary>

* **Authors:** Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, Tat-Seng Chua
* **arXiv ID:** 2309.05519
* **One-liner:** Presented NExT-GPT, an end-to-end any-to-any MM-LLM system for input and output in text, images, videos, and audio.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2309.05519) | [[PDF]](https://arxiv.org/pdf/2309.05519)

> **Core Innovation**
> Enabled perception and generation in arbitrary multimodal combinations with low-cost training and expansion potential.

<details>
    <summary>Abstract</summary>
    While recently Multimodal Large Language Models (MM-LLMs) have made exciting strides, they mostly fall prey to the limitation of only input-side multimodal understanding, without the ability to produce content in multiple modalities. As we humans always perceive the world and communicate with people through various modalities, developing any-to-any MM-LLMs capable of accepting and delivering content in any modality becomes essential to human-level AI. To fill the gap, we present an end-to-end general-purpose any-to-any MM-LLM system, NExT-GPT. We connect an LLM with multimodal adaptors and different diffusion decoders, enabling NExT-GPT to perceive inputs and generate outputs in arbitrary combinations of text, images, videos, and audio. By leveraging the existing well-trained highly-performing encoders and decoders, NExT-GPT is tuned with only a small amount of parameter (1%) of certain projection layers, which not only benefits low-cost training and also facilitates convenient expansion to more potential modalities. Moreover, we introduce a modality-switching instruction tuning (MosIT) and manually curate a high-quality dataset for MosIT, based on which NExT-GPT is empowered with complex cross-modal semantic understanding and content generation. Overall, our research showcases the promising possibility of building an AI agent capable of modeling universal modalities, paving the way for more human-like AI research in the community. Project page: <a href="https://next-gpt.github.io/" rel="external noopener nofollow" class="link-external link-https">this https URL</a>
</details>

<details>
    <summary>Key points</summary>
    * Connected LLM with multimodal adaptors and diffusion decoders
    * Tuned with only 1% parameters for projection layers
    * Introduced modality-switching instruction tuning (MosIT)
    * Curated high-quality dataset for complex cross-modal understanding and generation
</details>
</details>

---


<details>
<summary><b> Making LLaMA SEE and Draw with SEED Tokenizer</b></summary>

* **Authors:** Yuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li, Xintao Wang, Ying Shan
* **arXiv ID:** 2310.01218
* **One-liner:** SEED enables LLMs to perform scalable multimodal autoregression for both comprehension and generation tasks.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2310.01218) | [[PDF]](https://arxiv.org/pdf/2310.01218)

> **Core Innovation**
> SEED introduces an image tokenizer that allows text and images to be represented and processed interchangeably in a unified autoregressive Transformer.

<details>
    <summary>Abstract</summary>
    The great success of Large Language Models (LLMs) has expanded the potential of multimodality, contributing to the gradual evolution of General Artificial Intelligence (AGI). A true AGI agent should not only possess the capability to perform predefined multi-tasks but also exhibit emergent abilities in an open-world context. However, despite the considerable advancements made by recent multimodal LLMs, they still fall short in effectively unifying comprehension and generation tasks, let alone open-world emergent abilities. We contend that the key to overcoming the present impasse lies in enabling text and images to be represented and processed interchangeably within a unified autoregressive Transformer. To this end, we introduce SEED, an elaborate image tokenizer that empowers LLMs with the ability to SEE and Draw at the same time. We identify two crucial design principles: (1) Image tokens should be independent of 2D physical patch positions and instead be produced with a 1D causal dependency, exhibiting intrinsic interdependence that aligns with the left-to-right autoregressive prediction mechanism in LLMs. (2) Image tokens should capture high-level semantics consistent with the degree of semantic abstraction in words, and be optimized for both discriminativeness and reconstruction during the tokenizer training phase. With SEED tokens, LLM is able to perform scalable multimodal autoregression under its original training recipe, i.e., next-word prediction. SEED-LLaMA is therefore produced by large-scale pretraining and instruction tuning on the interleaved textual and visual data, demonstrating impressive performance on a broad range of multimodal comprehension and generation tasks. More importantly, SEED-LLaMA has exhibited compositional emergent abilities such as multi-turn in-context multimodal generation, acting like your AI assistant.
</details>

<details>
    <summary>Key points</summary>
    * Image tokens are designed with 1D causal dependency independent of 2D patch positions.
    * Tokens capture high-level semantics optimized for discriminativeness and reconstruction.
    * SEED-LLaMA is trained on interleaved textual and visual data, enabling emergent abilities like multi-turn in-context generation.
</details>
</details>

---


<details>
<summary><b> Kosmos-G: Generating Images in Context with Multimodal Large Language Models</b></summary>

* **Authors:** Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng, Wenhu Chen, Furu Wei
* **arXiv ID:** 2310.02992
* **One-liner:** Kosmos-G achieves zero-shot subject-driven image generation with interleaved multi-image and text input.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2310.02992) | [[PDF]](https://arxiv.org/pdf/2310.02992)

> **Core Innovation**
> Kosmos-G aligns MLLM output with CLIP using textual modality and performs compositional instruction tuning.

<details>
    <summary>Abstract</summary>
    Recent advancements in subject-driven image generation have made significant strides. However, current methods still fall short in diverse application scenarios, as they require test-time tuning and cannot accept interleaved multi-image and text input. These limitations keep them far from the ultimate goal of &#34;image as a foreign language in image generation.&#34; This paper presents Kosmos-G, a model that leverages the advanced multimodal perception capabilities of Multimodal Large Language Models (MLLMs) to tackle the aforementioned challenge. Our approach aligns the output space of MLLM with CLIP using the textual modality as an anchor and performs compositional instruction tuning on curated data. Kosmos-G demonstrates an impressive capability of zero-shot subject-driven generation with interleaved multi-image and text input. Notably, the score distillation instruction tuning requires no modifications to the image decoder. This allows for a seamless substitution of CLIP and effortless integration with a myriad of U-Net techniques ranging from fine-grained controls to personalized image decoder variants. We posit Kosmos-G as an initial attempt towards the goal of &#34;image as a foreign language in image generation.&#34; The code can be found at <a href="https://aka.ms/Kosmos-G" rel="external noopener nofollow" class="link-external link-https">this https URL</a>
</details>

<details>
    <summary>Key points</summary>
    * Aligns MLLM output space with CLIP using text as an anchor.
    * Uses score distillation instruction tuning without modifying the image decoder.
    * Enables seamless integration with various U-Net techniques for fine-grained control.
</details>
</details>

---


<details>
<summary><b> Gemini: A Family of Highly Capable Multimodal Models</b></summary>

* **Authors:** Gemini Team Google, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul R. Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, Jack Krawczyk, Cosmo Du, Ed Chi, Heng-Tze Cheng, Eric Ni, Purvi Shah, Patrick Kane, Betty Chan, Manaal Faruqui, Aliaksei Severyn, Hanzhao Lin, YaGuang Li, Yong Cheng, Abe Ittycheriah, Mahdis Mahdieh, Mia Chen, Pei Sun, Dustin Tran, Sumit Bagri, Balaji Lakshminarayanan, Jeremiah Liu, Andras Orban, Fabian Güra, Hao Zhou, Xinying Song, Aurelien Boffy, Harish Ganapathy, Steven Zheng, HyunJeong Choe, Ágoston Weisz, Tao Zhu, Yifeng Lu, Siddharth Gopal, Jarrod Kahn, Maciej Kula, Jeff Pitman, Rushin Shah, Emanuel Taropa, Majd Al Merey, Martin Baeuml, Zhifeng Chen, Laurent El Shafey, Yujing Zhang, Olcan Sercinoglu, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, Anaïs White, Anders Andreassen, Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha Khalman, Jakub Sygnowski, Alexandre Frechette, Charlotte Smith, Laura Culp, Lev Proleev, Yi Luan, Xi Chen, James Lottes, Nathan Schucher, Federico Lebron, Alban Rrustemi, Natalie Clay, Phil Crone, Tomas Kocisky, Jeffrey Zhao, Bartek Perz, Dian Yu, Heidi Howard, Adam Bloniarz, Jack W. Rae, Han Lu, Laurent Sifre, Marcello Maggioni, Fred Alcober, Dan Garrette, Megan Barnes, Shantanu Thakoor, Jacob Austin, Gabriel Barth-Maron, William Wong, Rishabh Joshi, Rahma Chaabouni, Deeni Fatiha, Arun Ahuja, Gaurav Singh Tomar, Evan Senter, Martin Chadwick, Ilya Kornakov, Nithya Attaluri, Iñaki Iturrate, Ruibo Liu, Yunxuan Li, Sarah Cogan, Jeremy Chen, Chao Jia, Chenjie Gu, Qiao Zhang, Jordan Grimstad, Ale Jakse Hartman, Xavier Garcia, Thanumalayan Sankaranarayana Pillai, Jacob Devlin, Michael Laskin, Diego de Las Casas, Dasha Valter, Connie Tao, Lorenzo Blanco, Adrià Puigdomènech Badia, David Reitter, Mianna Chen, Jenny Brennan, Clara Rivera, Sergey Brin, Shariq Iqbal, Gabriela Surita, Jane Labanowski, Abhi Rao, Stephanie Winkler, Emilio Parisotto, Yiming Gu, Kate Olszewska, Ravi Addanki, Antoine Miech, Annie Louis, Denis Teplyashin, Geoff Brown, Elliot Catt, Jan Balaguer, Jackie Xiang, Pidong Wang, Zoe Ashwood, Anton Briukhov, Albert Webson, Sanjay Ganapathy, Smit Sanghavi, Ajay Kannan, Ming-Wei Chang, Axel Stjerngren, Josip Djolonga, Yuting Sun, Ankur Bapna, Matthew Aitchison, Pedram Pejman, Henryk Michalewski, Tianhe Yu, Cindy Wang, Juliette Love, Junwhan Ahn, Dawn Bloxwich, Kehang Han, Peter Humphreys, Thibault Sellam, James Bradbury, Varun Godbole, Sina Samangooei, Bogdan Damoc, Alex Kaskasoli, Sébastien M. R. Arnold, Vijay Vasudevan, Shubham Agrawal, Jason Riesa, Dmitry Lepikhin, Richard Tanburn, Srivatsan Srinivasan, Hyeontaek Lim, Sarah Hodkinson, Pranav Shyam, Johan Ferret, Steven Hand, Ankush Garg, Tom Le Paine, Jian Li, Yujia Li, Minh Giang, Alexander Neitz, Zaheer Abbas, Sarah York, Machel Reid, Elizabeth Cole, Aakanksha Chowdhery, Dipanjan Das, Dominika Rogozińska, Vitaliy Nikolaev, Pablo Sprechmann, Zachary Nado, Lukas Zilka, Flavien Prost, Luheng He, Marianne Monteiro, Gaurav Mishra, Chris Welty, Josh Newlan, Dawei Jia, Miltiadis Allamanis, Clara Huiyi Hu, Raoul de Liedekerke, Justin Gilmer, Carl Saroufim, Shruti Rijhwani, Shaobo Hou, Disha Shrivastava, Anirudh Baddepudi, Alex Goldin, Adnan Ozturel, Albin Cassirer, Yunhan Xu, Daniel Sohn, Devendra Sachan, Reinald Kim Amplayo, Craig Swanson, Dessie Petrova, Shashi Narayan, Arthur Guez, Siddhartha Brahma, Jessica Landon, Miteyan Patel, Ruizhe Zhao, Kevin Villela, Luyu Wang, Wenhao Jia, Matthew Rahtz, Mai Giménez, Legg Yeung, James Keeling, Petko Georgiev, Diana Mincu, Boxi Wu, Salem Haykal, Rachel Saputro, Kiran Vodrahalli, James Qin, Zeynep Cankara, Abhanshu Sharma, Nick Fernando, Will Hawkins, Behnam Neyshabur, Solomon Kim, Adrian Hutter, Priyanka Agrawal, Alex Castro-Ros, George van den Driessche, Tao Wang, Fan Yang, Shuo-yiin Chang, Paul Komarek, Ross McIlroy, Mario Lučić, Guodong Zhang, Wael Farhan, Michael Sharman, Paul Natsev, Paul Michel, Yamini Bansal, Siyuan Qiao, Kris Cao, Siamak Shakeri, Christina Butterfield, Justin Chung, Paul Kishan Rubenstein, Shivani Agrawal, Arthur Mensch, Kedar Soparkar, Karel Lenc, Timothy Chung, Aedan Pope, Loren Maggiore, Jackie Kay, Priya Jhakra, Shibo Wang, Joshua Maynez, Mary Phuong, Taylor Tobin, Andrea Tacchetti, Maja Trebacz, Kevin Robinson, Yash Katariya, Sebastian Riedel, Paige Bailey, Kefan Xiao, Nimesh Ghelani, Lora Aroyo, Ambrose Slone, Neil Houlsby, Xuehan Xiong, Zhen Yang, Elena Gribovskaya, Jonas Adler, Mateo Wirth, Lisa Lee, Music Li, Thais Kagohara, Jay Pavagadhi, Sophie Bridgers, Anna Bortsova, Sanjay Ghemawat, Zafarali Ahmed, Tianqi Liu, Richard Powell, Vijay Bolina, Mariko Iinuma, Polina Zablotskaia, James Besley, Da-Woon Chung, Timothy Dozat, Ramona Comanescu, Xiance Si, Jeremy Greer, Guolong Su, Martin Polacek, Raphaël Lopez Kaufman, Simon Tokumine, Hexiang Hu, Elena Buchatskaya, Yingjie Miao, Mohamed Elhawaty, Aditya Siddhant, Nenad Tomasev, Jinwei Xing, Christina Greer, Helen Miller, Shereen Ashraf, Aurko Roy, Zizhao Zhang, Ada Ma, Angelos Filos, Milos Besta, Rory Blevins, Ted Klimenko, Chih-Kuan Yeh, Soravit Changpinyo, Jiaqi Mu, Oscar Chang, Mantas Pajarskas, Carrie Muir, Vered Cohen, Charline Le Lan, Krishna Haridasan, Amit Marathe, Steven Hansen, Sholto Douglas, Rajkumar Samuel, Mingqiu Wang, Sophia Austin, Chang Lan, Jiepu Jiang, Justin Chiu, Jaime Alonso Lorenzo, Lars Lowe Sjösund, Sébastien Cevey, Zach Gleicher, Thi Avrahami, Anudhyan Boral, Hansa Srinivasan, Vittorio Selo, Rhys May, Konstantinos Aisopos, Léonard Hussenot, Livio Baldini Soares, Kate Baumli, Michael B. Chang, Adrià Recasens, Ben Caine, Alexander Pritzel, Filip Pavetic, Fabio Pardo, Anita Gergely, Justin Frye, Vinay Ramasesh, Dan Horgan, Kartikeya Badola, Nora Kassner, Subhrajit Roy, Ethan Dyer, Víctor Campos Campos, Alex Tomala, Yunhao Tang, Dalia El Badawy, Elspeth White, Basil Mustafa, Oran Lang, Abhishek Jindal, Sharad Vikram, Zhitao Gong, Sergi Caelles, Ross Hemsley, Gregory Thornton, Fangxiaoyu Feng, Wojciech Stokowiec, Ce Zheng, Phoebe Thacker, Çağlar Ünlü, Zhishuai Zhang, Mohammad Saleh, James Svensson, Max Bileschi, Piyush Patil, Ankesh Anand, Roman Ring, Katerina Tsihlas, Arpi Vezer, Marco Selvi, Toby Shevlane, Mikel Rodriguez, Tom Kwiatkowski, Samira Daruki, Keran Rong, Allan Dafoe, Nicholas FitzGerald, Keren Gu-Lemberg, Mina Khan, Lisa Anne Hendricks, Marie Pellat, Vladimir Feinberg, James Cobon-Kerr, Tara Sainath, Maribeth Rauh, Sayed Hadi Hashemi, Richard Ives, Yana Hasson, Eric Noland, Yuan Cao, Nathan Byrd, Le Hou, Qingze Wang, Thibault Sottiaux, Michela Paganini, Jean-Baptiste Lespiau, Alexandre Moufarek, Samer Hassan, Kaushik Shivakumar, Joost van Amersfoort, Amol Mandhane, Pratik Joshi, Anirudh Goyal, Matthew Tung, Andrew Brock, Hannah Sheahan, Vedant Misra, Cheng Li, Nemanja Rakićević, Mostafa Dehghani, Fangyu Liu, Sid Mittal, Junhyuk Oh, Seb Noury, Eren Sezener, Fantine Huot, Matthew Lamm, Nicola De Cao, Charlie Chen, Sidharth Mudgal, Romina Stella, Kevin Brooks, Gautam Vasudevan, Chenxi Liu, Mainak Chain, Nivedita Melinkeri, Aaron Cohen, Venus Wang, Kristie Seymore, Sergey Zubkov, Rahul Goel, Summer Yue, Sai Krishnakumaran, Brian Albert, Nate Hurley, Motoki Sano, Anhad Mohananey, Jonah Joughin, Egor Filonov, Tomasz Kępa, Yomna Eldawy, Jiawern Lim, Rahul Rishi, Shirin Badiezadegan, Taylor Bos, Jerry Chang, Sanil Jain, Sri Gayatri Sundara Padmanabhan, Subha Puttagunta, Kalpesh Krishna, Leslie Baker, Norbert Kalb, Vamsi Bedapudi, Adam Kurzrok, Shuntong Lei, Anthony Yu, Oren Litvin, Xiang Zhou, Zhichun Wu, Sam Sobell, Andrea Siciliano, Alan Papir, Robby Neale, Jonas Bragagnolo, Tej Toor, Tina Chen, Valentin Anklin, Feiran Wang, Richie Feng, Milad Gholami, Kevin Ling, Lijuan Liu, Jules Walter, Hamid Moghaddam, Arun Kishore, Jakub Adamek, Tyler Mercado, Jonathan Mallinson, Siddhinita Wandekar, Stephen Cagle, Eran Ofek, Guillermo Garrido, Clemens Lombriser, Maksim Mukha, Botu Sun, Hafeezul Rahman Mohammad, Josip Matak, Yadi Qian, Vikas Peswani, Pawel Janus, Quan Yuan, Leif Schelin, Oana David, Ankur Garg, Yifan He, Oleksii Duzhyi, Anton Älgmyr, Timothée Lottaz, Qi Li, Vikas Yadav, Luyao Xu, Alex Chinien, Rakesh Shivanna, Aleksandr Chuklin, Josie Li, Carrie Spadine, Travis Wolfe, Kareem Mohamed, Subhabrata Das, Zihang Dai, Kyle He, Daniel von Dincklage, Shyam Upadhyay, Akanksha Maurya, Luyan Chi, Sebastian Krause, Khalid Salama, Pam G Rabinovitch, Pavan Kumar Reddy M, Aarush Selvan, Mikhail Dektiarev, Golnaz Ghiasi, Erdem Guven, Himanshu Gupta, Boyi Liu, Deepak Sharma, Idan Heimlich Shtacher, Shachi Paul, Oscar Akerlund, François-Xavier Aubet, Terry Huang, Chen Zhu, Eric Zhu, Elico Teixeira, Matthew Fritze, Francesco Bertolini, Liana-Eleonora Marinescu, Martin Bölle, Dominik Paulus, Khyatti Gupta, Tejasi Latkar, Max Chang, Jason Sanders, Roopa Wilson, Xuewei Wu, Yi-Xuan Tan, Lam Nguyen Thiet, Tulsee Doshi, Sid Lall, Swaroop Mishra, Wanming Chen, Thang Luong, Seth Benjamin, Jasmine Lee, Ewa Andrejczuk, Dominik Rabiej, Vipul Ranjan, Krzysztof Styrc, Pengcheng Yin, Jon Simon, Malcolm Rose Harriott, Mudit Bansal, Alexei Robsky, Geoff Bacon, David Greene, Daniil Mirylenka, Chen Zhou, Obaid Sarvana, Abhimanyu Goyal, Samuel Andermatt, Patrick Siegler, Ben Horn, Assaf Israel, Francesco Pongetti, Chih-Wei &#34;Louis&#34; Chen, Marco Selvatici, Pedro Silva, Kathie Wang, Jackson Tolins, Kelvin Guu, Roey Yogev, Xiaochen Cai, Alessandro Agostini, Maulik Shah, Hung Nguyen, Noah Ó Donnaile, Sébastien Pereira, Linda Friso, Adam Stambler, Adam Kurzrok, Chenkai Kuang, Yan Romanikhin, Mark Geller, ZJ Yan, Kane Jang, Cheng-Chun Lee, Wojciech Fica, Eric Malmi, Qijun Tan, Dan Banica, Daniel Balle, Ryan Pham, Yanping Huang, Diana Avram, Hongzhi Shi, Jasjot Singh, Chris Hidey, Niharika Ahuja, Pranab Saxena, Dan Dooley, Srividya Pranavi Potharaju, Eileen O&#39;Neill, Anand Gokulchandran, Ryan Foley, Kai Zhao, Mike Dusenberry, Yuan Liu, Pulkit Mehta, Ragha Kotikalapudi, Chalence Safranek-Shrader, Andrew Goodman, Joshua Kessinger, Eran Globen, Prateek Kolhar, Chris Gorgolewski, Ali Ibrahim, Yang Song, Ali Eichenbaum, Thomas Brovelli, Sahitya Potluri, Preethi Lahoti, Cip Baetu, Ali Ghorbani, Charles Chen, Andy Crawford, Shalini Pal, Mukund Sridhar, Petru Gurita, Asier Mujika, Igor Petrovski, Pierre-Louis Cedoz, Chenmei Li, Shiyuan Chen, Niccolò Dal Santo, Siddharth Goyal, Jitesh Punjabi, Karthik Kappaganthu, Chester Kwak, Pallavi LV, Sarmishta Velury, Himadri Choudhury, Jamie Hall, Premal Shah, Ricardo Figueira, Matt Thomas, Minjie Lu, Ting Zhou, Chintu Kumar, Thomas Jurdi, Sharat Chikkerur, Yenai Ma, Adams Yu, Soo Kwak, Victor Ähdel, Sujeevan Rajayogam, Travis Choma, Fei Liu, Aditya Barua, Colin Ji, Ji Ho Park, Vincent Hellendoorn, Alex Bailey, Taylan Bilal, Huanjie Zhou, Mehrdad Khatir, Charles Sutton, Wojciech Rzadkowski, Fiona Macintosh, Roopali Vij, Konstantin Shagin, Paul Medina, Chen Liang, Jinjing Zhou, Pararth Shah, Yingying Bi, Attila Dankovics, Shipra Banga, Sabine Lehmann, Marissa Bredesen, Zifan Lin, John Eric Hoffmann, Jonathan Lai, Raynald Chung, Kai Yang, Nihal Balani, Arthur Bražinskas, Andrei Sozanschi, Matthew Hayes, Héctor Fernández Alcalde, Peter Makarov, Will Chen, Antonio Stella, Liselotte Snijders, Michael Mandl, Ante Kärrman, Paweł Nowak, Xinyi Wu, Alex Dyck, Krishnan Vaidyanathan, Raghavender R, Jessica Mallet, Mitch Rudominer, Eric Johnston, Sushil Mittal, Akhil Udathu, Janara Christensen, Vishal Verma, Zach Irving, Andreas Santucci, Gamaleldin Elsayed, Elnaz Davoodi, Marin Georgiev, Ian Tenney, Nan Hua, Geoffrey Cideron, Edouard Leurent, Mahmoud Alnahlawi, Ionut Georgescu, Nan Wei, Ivy Zheng, Dylan Scandinaro, Heinrich Jiang, Jasper Snoek, Mukund Sundararajan, Xuezhi Wang, Zack Ontiveros, Itay Karo, Jeremy Cole, Vinu Rajashekhar, Lara Tumeh, Eyal Ben-David, Rishub Jain, Jonathan Uesato, Romina Datta, Oskar Bunyan, Shimu Wu, John Zhang, Piotr Stanczyk, Ye Zhang, David Steiner, Subhajit Naskar, Michael Azzam, Matthew Johnson, Adam Paszke, Chung-Cheng Chiu, Jaume Sanchez Elias, Afroz Mohiuddin, Faizan Muhammad, Jin Miao, Andrew Lee, Nino Vieillard, Jane Park, Jiageng Zhang, Jeff Stanway, Drew Garmon, Abhijit Karmarkar, Zhe Dong, Jong Lee, Aviral Kumar, Luowei Zhou, Jonathan Evens, William Isaac, Geoffrey Irving, Edward Loper, Michael Fink, Isha Arkatkar, Nanxin Chen, Izhak Shafran, Ivan Petrychenko, Zhe Chen, Johnson Jia, Anselm Levskaya, Zhenkai Zhu, Peter Grabowski, Yu Mao, Alberto Magni, Kaisheng Yao, Javier Snaider, Norman Casagrande, Evan Palmer, Paul Suganthan, Alfonso Castaño, Irene Giannoumis, Wooyeol Kim, Mikołaj Rybiński, Ashwin Sreevatsa, Jennifer Prendki, David Soergel, Adrian Goedeckemeyer, Willi Gierke, Mohsen Jafari, Meenu Gaba, Jeremy Wiesner, Diana Gage Wright, Yawen Wei, Harsha Vashisht, Yana Kulizhskaya, Jay Hoover, Maigo Le, Lu Li, Chimezie Iwuanyanwu, Lu Liu, Kevin Ramirez, Andrey Khorlin, Albert Cui, Tian LIN, Marcus Wu, Ricardo Aguilar, Keith Pallo, Abhishek Chakladar, Ginger Perng, Elena Allica Abellan, Mingyang Zhang, Ishita Dasgupta, Nate Kushman, Ivo Penchev, Alena Repina, Xihui Wu, Tom van der Weide, Priya Ponnapalli, Caroline Kaplan, Jiri Simsa, Shuangfeng Li, Olivier Dousse, Fan Yang, Jeff Piper, Nathan Ie, Rama Pasumarthi, Nathan Lintz, Anitha Vijayakumar, Daniel Andor, Pedro Valenzuela, Minnie Lui, Cosmin Paduraru, Daiyi Peng, Katherine Lee, Shuyuan Zhang, Somer Greene, Duc Dung Nguyen, Paula Kurylowicz, Cassidy Hardin, Lucas Dixon, Lili Janzer, Kiam Choo, Ziqiang Feng, Biao Zhang, Achintya Singhal, Dayou Du, Dan McKinnon, Natasha Antropova, Tolga Bolukbasi, Orgad Keller, David Reid, Daniel Finchelstein, Maria Abi Raad, Remi Crocker, Peter Hawkins, Robert Dadashi, Colin Gaffney, Ken Franko, Anna Bulanova, Rémi Leblond, Shirley Chung, Harry Askham, Luis C. Cobo, Kelvin Xu, Felix Fischer, Jun Xu, Christina Sorokin, Chris Alberti, Chu-Cheng Lin, Colin Evans, Alek Dimitriev, Hannah Forbes, Dylan Banarse, Zora Tung, Mark Omernick, Colton Bishop, Rachel Sterneck, Rohan Jain, Jiawei Xia, Ehsan Amid, Francesco Piccinno, Xingyu Wang, Praseem Banzal, Daniel J. Mankowitz, Alex Polozov, Victoria Krakovna, Sasha Brown, MohammadHossein Bateni, Dennis Duan, Vlad Firoiu, Meghana Thotakuri, Tom Natan, Matthieu Geist, Ser tan Girgin, Hui Li, Jiayu Ye, Ofir Roval, Reiko Tojo, Michael Kwong, James Lee-Thorp, Christopher Yew, Danila Sinopalnikov, Sabela Ramos, John Mellor, Abhishek Sharma, Kathy Wu, David Miller, Nicolas Sonnerat, Denis Vnukov, Rory Greig, Jennifer Beattie, Emily Caveness, Libin Bai, Julian Eisenschlos, Alex Korchemniy, Tomy Tsai, Mimi Jasarevic, Weize Kong, Phuong Dao, Zeyu Zheng, Frederick Liu, Fan Yang, Rui Zhu, Tian Huey Teh, Jason Sanmiya, Evgeny Gladchenko, Nejc Trdin, Daniel Toyama, Evan Rosen, Sasan Tavakkol, Linting Xue, Chen Elkind, Oliver Woodman, John Carpenter, George Papamakarios, Rupert Kemp, Sushant Kafle, Tanya Grunina, Rishika Sinha, Alice Talbert, Diane Wu, Denese Owusu-Afriyie, Cosmo Du, Chloe Thornton, Jordi Pont-Tuset, Pradyumna Narayana, Jing Li, Saaber Fatehi, John Wieting, Omar Ajmeri, Benigno Uria, Yeongil Ko, Laura Knight, Amélie Héliou, Ning Niu, Shane Gu, Chenxi Pang, Yeqing Li, Nir Levine, Ariel Stolovich, Rebeca Santamaria-Fernandez, Sonam Goenka, Wenny Yustalim, Robin Strudel, Ali Elqursh, Charlie Deck, Hyo Lee, Zonglin Li, Kyle Levin, Raphael Hoffmann, Dan Holtmann-Rice, Olivier Bachem, Sho Arora, Christy Koh, Soheil Hassas Yeganeh, Siim Põder, Mukarram Tariq, Yanhua Sun, Lucian Ionita, Mojtaba Seyedhosseini, Pouya Tafti, Zhiyu Liu, Anmol Gulati, Jasmine Liu, Xinyu Ye, Bart Chrzaszcz, Lily Wang, Nikhil Sethi, Tianrun Li, Ben Brown, Shreya Singh, Wei Fan, Aaron Parisi, Joe Stanton, Vinod Koverkathu, Christopher A. Choquette-Choo, Yunjie Li, TJ Lu, Abe Ittycheriah, Prakash Shroff, Mani Varadarajan, Sanaz Bahargam, Rob Willoughby, David Gaddy, Guillaume Desjardins, Marco Cornero, Brona Robenek, Bhavishya Mittal, Ben Albrecht, Ashish Shenoy, Fedor Moiseev, Henrik Jacobsson, Alireza Ghaffarkhah, Morgane Rivière, Alanna Walton, Clément Crepy, Alicia Parrish, Zongwei Zhou, Clement Farabet, Carey Radebaugh, Praveen Srinivasan, Claudia van der Salm, Andreas Fidjeland, Salvatore Scellato, Eri Latorre-Chimoto, Hanna Klimczak-Plucińska, David Bridson, Dario de Cesare, Tom Hudson, Piermaria Mendolicchio, Lexi Walker, Alex Morris, Matthew Mauger, Alexey Guseynov, Alison Reid, Seth Odoom, Lucia Loher, Victor Cotruta, Madhavi Yenugula, Dominik Grewe, Anastasia Petrushkina, Tom Duerig, Antonio Sanchez, Steve Yadlowsky, Amy Shen, Amir Globerson, Lynette Webb, Sahil Dua, Dong Li, Surya Bhupatiraju, Dan Hurt, Haroon Qureshi, Ananth Agarwal, Tomer Shani, Matan Eyal, Anuj Khare, Shreyas Rammohan Belle, Lei Wang, Chetan Tekur, Mihir Sanjay Kale, Jinliang Wei, Ruoxin Sang, Brennan Saeta, Tyler Liechty, Yi Sun, Yao Zhao, Stephan Lee, Pandu Nayak, Doug Fritz, Manish Reddy Vuyyuru, John Aslanides, Nidhi Vyas, Martin Wicke, Xiao Ma, Evgenii Eltyshev, Nina Martin, Hardie Cate, James Manyika, Keyvan Amiri, Yelin Kim, Xi Xiong, Kai Kang, Florian Luisier, Nilesh Tripuraneni, David Madras, Mandy Guo, Austin Waters, Oliver Wang, Joshua Ainslie, Jason Baldridge, Han Zhang, Garima Pruthi, Jakob Bauer, Feng Yang, Riham Mansour, Jason Gelman, Yang Xu, George Polovets, Ji Liu, Honglong Cai, Warren Chen, XiangHai Sheng, Emily Xue, Sherjil Ozair, Christof Angermueller, Xiaowei Li, Anoop Sinha, Weiren Wang, Julia Wiesinger, Emmanouil Koukoumidis, Yuan Tian, Anand Iyer, Madhu Gurumurthy, Mark Goldenson, Parashar Shah, MK Blake, Hongkun Yu, Anthony Urbanowicz, Jennimaria Palomaki, Chrisantha Fernando, Ken Durden, Harsh Mehta, Nikola Momchev, Elahe Rahimtoroghi, Maria Georgaki, Amit Raul, Sebastian Ruder, Morgan Redshaw, Jinhyuk Lee, Denny Zhou, Komal Jalan, Dinghua Li, Blake Hechtman, Parker Schuh, Milad Nasr, Kieran Milan, Vladimir Mikulik, Juliana Franco, Tim Green, Nam Nguyen, Joe Kelley, Aroma Mahendru, Andrea Hu, Joshua Howland, Ben Vargas, Jeffrey Hui, Kshitij Bansal, Vikram Rao, Rakesh Ghiya, Emma Wang, Ke Ye, Jean Michel Sarr, Melanie Moranski Preston, Madeleine Elish, Steve Li, Aakash Kaku, Jigar Gupta, Ice Pasupat, Da-Cheng Juan, Milan Someswar, Tejvi M., Xinyun Chen, Aida Amini, Alex Fabrikant, Eric Chu, Xuanyi Dong, Amruta Muthal, Senaka Buthpitiya, Sarthak Jauhari, Nan Hua, Urvashi Khandelwal, Ayal Hitron, Jie Ren, Larissa Rinaldi, Shahar Drath, Avigail Dabush, Nan-Jiang Jiang, Harshal Godhia, Uli Sachs, Anthony Chen, Yicheng Fan, Hagai Taitelbaum, Hila Noga, Zhuyun Dai, James Wang, Chen Liang, Jenny Hamer, Chun-Sung Ferng, Chenel Elkind, Aviel Atias, Paulina Lee, Vít Listík, Mathias Carlen, Jan van de Kerkhof, Marcin Pikus, Krunoslav Zaher, Paul Müller, Sasha Zykova, Richard Stefanec, Vitaly Gatsko, Christoph Hirnschall, Ashwin Sethi, Xingyu Federico Xu, Chetan Ahuja, Beth Tsai, Anca Stefanoiu, Bo Feng, Keshav Dhandhania, Manish Katyal, Akshay Gupta, Atharva Parulekar, Divya Pitta, Jing Zhao, Vivaan Bhatia, Yashodha Bhavnani, Omar Alhadlaq, Xiaolin Li, Peter Danenberg, Dennis Tu, Alex Pine, Vera Filippova, Abhipso Ghosh, Ben Limonchik, Bhargava Urala, Chaitanya Krishna Lanka, Derik Clive, Yi Sun, Edward Li, Hao Wu, Kevin Hongtongsak, Ianna Li, Kalind Thakkar, Kuanysh Omarov, Kushal Majmundar, Michael Alverson, Michael Kucharski, Mohak Patel, Mudit Jain, Maksim Zabelin, Paolo Pelagatti, Rohan Kohli, Saurabh Kumar, Joseph Kim, Swetha Sankar, Vineet Shah, Lakshmi Ramachandruni, Xiangkai Zeng, Ben Bariach, Laura Weidinger, Tu Vu, Alek Andreev, Antoine He, Kevin Hui, Sheleem Kashem, Amar Subramanya, Sissie Hsiao, Demis Hassabis, Koray Kavukcuoglu, Adam Sadovsky, Quoc Le, Trevor Strohman, Yonghui Wu, Slav Petrov, Jeffrey Dean, Oriol Vinyals
* **arXiv ID:** 2312.11805
* **One-liner:** Gemini advances state-of-the-art in multimodal understanding across image, audio, video, and text.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2312.11805) | [[PDF]](https://arxiv.org/pdf/2312.11805)

> **Core Innovation**
> Gemini models achieve human-expert performance on benchmarks like MMLU and improve results in 30 of 32 benchmarks.

<details>
    <summary>Abstract</summary>
    This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of the Gemini family in cross-modal reasoning and language understanding will enable a wide variety of use cases. We discuss our approach toward post-training and deploying Gemini models responsibly to users through services including Gemini, Gemini Advanced, Google AI Studio, and Cloud Vertex AI.
</details>

<details>
    <summary>Key points</summary>
    * Consists of Ultra, Pro, and Nano sizes for diverse applications.
    * Demonstrates strong cross-modal reasoning and language understanding capabilities.
    * Responsibly deployed through services like Gemini Advanced and Google AI Studio.
</details>
</details>

---


<details>
<summary><b> Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action</b></summary>

* **Authors:** Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, Aniruddha Kembhavi
* **arXiv ID:** 2312.17172
* **One-liner:** Unified-IO 2 is the first autoregressive multimodal model for understanding and generating image, text, audio, and action.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2312.17172) | [[PDF]](https://arxiv.org/pdf/2312.17172)

> **Core Innovation**
> Unified-IO 2 tokenizes diverse modalities into a shared semantic space and uses a single encoder-decoder transformer.

<details>
    <summary>Abstract</summary>
    We present Unified-IO 2, the first autoregressive multimodal model that is capable of understanding and generating image, text, audio, and action. To unify different modalities, we tokenize inputs and outputs -- images, text, audio, action, bounding boxes, etc., into a shared semantic space and then process them with a single encoder-decoder transformer model. Since training with such diverse modalities is challenging, we propose various architectural improvements to stabilize model training. We train our model from scratch on a large multimodal pre-training corpus from diverse sources with a multimodal mixture of denoisers objective. To learn an expansive set of skills, such as following multimodal instructions, we construct and finetune on an ensemble of 120 datasets with prompts and augmentations. With a single unified model, Unified-IO 2 achieves state-of-the-art performance on the GRIT benchmark and strong results in more than 35 benchmarks, including image generation and understanding, natural language understanding, video and audio understanding, and robotic manipulation. We release all our models to the research community.
</details>

<details>
    <summary>Key points</summary>
    * Tokenizes inputs and outputs (e.g., images, text, audio) into a unified space.
    * Employs architectural improvements for stable training with multimodal mixture of denoisers.
    * Finetuned on 120 datasets to achieve state-of-the-art on GRIT and other benchmarks.
</details>
</details>

---


<details>
<summary><b> VARGPT: Unified Understanding and Generation in a Visual Autoregressive Multimodal Large Language Model</b></summary>

* **Authors:** Xianwei Zhuang, Yuxin Xie, Yufan Deng, Liming Liang, Jinghan Ru, Yuguo Yin, Yuexian Zou
* **arXiv ID:** 2501.12327
* **One-liner:** VARGPT unifies visual understanding and generation in a single autoregressive framework.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2501.12327) | [[PDF]](https://arxiv.org/pdf/2501.12327)

> **Core Innovation**
> VARGPT uses next-token prediction for understanding and next-scale prediction for generation, extending LLaVA architecture.

<details>
    <summary>Abstract</summary>
    We present VARGPT, a novel multimodal large language model (MLLM) that unifies visual understanding and generation within a single autoregressive framework. VARGPT employs a next-token prediction paradigm for visual understanding and a next-scale prediction paradigm for visual autoregressive generation. VARGPT innovatively extends the LLaVA architecture, achieving efficient scale-wise autoregressive visual generation within MLLMs while seamlessly accommodating mixed-modal input and output within a single model framework. Our VARGPT undergoes a three-stage unified training process on specially curated datasets, comprising a pre-training phase and two mixed visual instruction-tuning phases. The unified training strategy are designed to achieve alignment between visual and textual features, enhance instruction following for both understanding and generation, and improve visual generation quality, respectively. Despite its LLAVA-based architecture for multimodel understanding, VARGPT significantly outperforms LLaVA-1.5 across various vision-centric benchmarks, such as visual question-answering and reasoning tasks. Notably, VARGPT naturally supports capabilities in autoregressive visual generation and instruction-to-image synthesis, showcasing its versatility in both visual understanding and generation tasks. Project page is at: \url{<a href="https://vargpt-1.github.io/" rel="external noopener nofollow" class="link-external link-https">this https URL</a>}
</details>

<details>
    <summary>Key points</summary>
    * Employs next-token and next-scale prediction paradigms.
    * Undergoes three-stage unified training for alignment and quality enhancement.
    * Supports autoregressive visual generation and instruction-to-image synthesis.
</details>
</details>

---


<details>
<summary><b> Generative Multimodal Models are In-Context Learners</b></summary>

* **Authors:** Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, Xinlong Wang
* **arXiv ID:** 2312.13286
* **One-liner:** Emu2 enhances multimodal in-context learning with 37B parameters, achieving state-of-the-art in few-shot tasks.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2312.13286) | [[PDF]](https://arxiv.org/pdf/2312.13286)

> **Core Innovation**
> Emu2 is trained on large-scale multimodal sequences with a unified autoregressive objective.

<details>
    <summary>Abstract</summary>
    The human ability to easily solve multimodal tasks in context (i.e., with only a few demonstrations or simple instructions), is what current multimodal systems have largely struggled to imitate. In this work, we demonstrate that the task-agnostic in-context learning capabilities of large multimodal models can be significantly enhanced by effective scaling-up. We introduce Emu2, a generative multimodal model with 37 billion parameters, trained on large-scale multimodal sequences with a unified autoregressive objective. Emu2 exhibits strong multimodal in-context learning abilities, even emerging to solve tasks that require on-the-fly reasoning, such as visual prompting and object-grounded generation. The model sets a new record on multiple multimodal understanding tasks in few-shot settings. When instruction-tuned to follow specific instructions, Emu2 further achieves new state-of-the-art on challenging tasks such as question answering benchmarks for large multimodal models and open-ended subject-driven generation. These achievements demonstrate that Emu2 can serve as a base model and general-purpose interface for a wide range of multimodal tasks. Code and models are publicly available to facilitate future research.
</details>

<details>
    <summary>Key points</summary>
    * Scales up to 37 billion parameters for improved in-context learning.
    * Exhibits emergent abilities in visual prompting and object-grounded generation.
    * Instruction-tuned for top performance on question answering and subject-driven generation.
</details>
</details>

---


<details>
<summary><b> MM-Interleaved: Interleaved Image-Text Generative Modeling via Multi-modal Feature Synchronizer</b></summary>

* **Authors:** Changyao Tian, Xizhou Zhu, Yuwen Xiong, Weiyun Wang, Zhe Chen, Wenhai Wang, Yuntao Chen, Lewei Lu, Tong Lu, Jie Zhou, Hongsheng Li, Yu Qiao, Jifeng Dai
* **arXiv ID:** 2401.10208
* **One-liner:** MM-Interleaved is an end-to-end generative model for interleaved image-text data with improved detail capture.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2401.10208) | [[PDF]](https://arxiv.org/pdf/2401.10208)

> **Core Innovation**
> MM-Interleaved uses a multi-scale and multi-image feature synchronizer to access fine-grained image features.

<details>
    <summary>Abstract</summary>
    Developing generative models for interleaved image-text data has both research and practical value. It requires models to understand the interleaved sequences and subsequently generate images and text. However, existing attempts are limited by the issue that the fixed number of visual tokens cannot efficiently capture image details, which is particularly problematic in the multi-image scenarios. To address this, this paper presents MM-Interleaved, an end-to-end generative model for interleaved image-text data. It introduces a multi-scale and multi-image feature synchronizer module, allowing direct access to fine-grained image features in the previous context during the generation process. MM-Interleaved is end-to-end pre-trained on both paired and interleaved image-text corpora. It is further enhanced through a supervised fine-tuning phase, wherein the model improves its ability to follow complex multi-modal instructions. Experiments demonstrate the versatility of MM-Interleaved in recognizing visual details following multi-modal instructions and generating consistent images following both textual and visual conditions. Code and models are available at \url{<a href="https://github.com/OpenGVLab/MM-Interleaved" rel="external noopener nofollow" class="link-external link-https">this https URL</a>}.
</details>

<details>
    <summary>Key points</summary>
    * Introduces multi-scale and multi-image feature synchronizer module.
    * End-to-end pre-trained on paired and interleaved image-text corpora.
    * Enhanced through supervised fine-tuning for complex multimodal instructions.
</details>
</details>

---


<details>
<summary><b> AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling</b></summary>

* **Authors:** Jun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong Zhang, Zhigeng Liu, Xin Zhang, Ruibin Yuan, Ge Zhang, Linyang Li, Hang Yan, Jie Fu, Tao Gui, Tianxiang Sun, Yu-Gang Jiang, Xipeng Qiu
* **arXiv ID:** 2402.12226
* **One-liner:** AnyGPT enables any-to-any multimodal conversation using discrete representations without altering LLM architecture.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2402.12226) | [[PDF]](https://arxiv.org/pdf/2402.12226)

> **Core Innovation**
> AnyGPT utilizes discrete representations for unified processing of speech, text, images, and music.

<details>
    <summary>Abstract</summary>
    We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitating any-to-any multimodal conversation while achieving performance comparable to specialized models across all modalities, proving that discrete representations can effectively and conveniently unify multiple modalities within a language model. Demos are shown in <a href="https://junzhan2000.github.io/AnyGPT.github.io/" rel="external noopener nofollow" class="link-external link-https">this https URL</a>
</details>

<details>
    <summary>Key points</summary>
    * Relies on data-level preprocessing for seamless modality integration.
    * Trained on multimodal text-centric dataset and synthesized instruction dataset.
    * Achieves performance comparable to specialized models across modalities.
</details>
</details>

---


<details>
<summary><b> PVC: Progressive Visual Token Compression for Unified Image and Video Processing in Large Vision-Language Models</b></summary>

* **Authors:** Chenyu Yang, Xuan Dong, Xizhou Zhu, Weijie Su, Jiahao Wang, Hao Tian, Zhe Chen, Wenhai Wang, Lewei Lu, Jifeng Dai
* **arXiv ID:** 2412.09613
* **One-liner:** PVC unifies token compression for images and videos, achieving state-of-the-art in video understanding without image performance loss.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2412.09613) | [[PDF]](https://arxiv.org/pdf/2412.09613)

> **Core Innovation**
> PVC progressively encodes and compresses visual tokens, treating images as static videos to preserve details.

<details>
    <summary>Abstract</summary>
    Large Vision-Language Models (VLMs) have been extended to understand both images and videos. Visual token compression is leveraged to reduce the considerable token length of visual inputs. To meet the needs of different tasks, existing high-performance models usually process images and videos separately with different token compression strategies, limiting the capabilities of combining images and videos. To this end, we extend each image into a &#34;static&#34; video and introduce a unified token compression strategy called Progressive Visual Token Compression (PVC), where the tokens of each frame are progressively encoded and adaptively compressed to supplement the information not extracted from previous frames. Video tokens are efficiently compressed with exploiting the inherent temporal redundancy. Images are repeated as static videos, and the spatial details can be gradually supplemented in multiple frames. PVC unifies the token compressing of images and videos. With a limited number of tokens per frame (64 tokens by default), spatial details and temporal changes can still be preserved. Experiments show that our model achieves state-of-the-art performance across various video understanding benchmarks, including long video tasks and fine-grained short video tasks. Meanwhile, our unified token compression strategy incurs no performance loss on image benchmarks, particularly in detail-sensitive tasks.
</details>

<details>
    <summary>Key points</summary>
    * Extends images to static videos for unified token compression.
    * Progressively encodes tokens with adaptive compression to supplement information.
    * Maintains performance on detail-sensitive image tasks while excelling in video benchmarks.
</details>
</details>

---


<details>
<summary><b> Omni-RGPT: Unifying Image and Video Region-level Understanding via Token Marks</b></summary>

* **Authors:** Miran Heo, Min-Hung Chen, De-An Huang, Sifei Liu, Subhashree Radhakrishnan, Seon Joo Kim, Yu-Chiang Frank Wang, Ryo Hachiuma
* **arXiv ID:** 2501.08326
* **One-liner:** Omni-RGPT facilitates region-level comprehension for images and videos using Token Mark for consistent representation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2501.08326) | [[PDF]](https://arxiv.org/pdf/2501.08326)

> **Core Innovation**
> Omni-RGPT embeds Token Mark tokens into visual features and text prompts for direct connection.

<details>
    <summary>Abstract</summary>
    We present Omni-RGPT, a multimodal large language model designed to facilitate region-level comprehension for both images and videos. To achieve consistent region representation across spatio-temporal dimensions, we introduce Token Mark, a set of tokens highlighting the target regions within the visual feature space. These tokens are directly embedded into spatial regions using region prompts (e.g., boxes or masks) and simultaneously incorporated into the text prompt to specify the target, establishing a direct connection between visual and text tokens. To further support robust video understanding without requiring tracklets, we introduce an auxiliary task that guides Token Mark by leveraging the consistency of the tokens, enabling stable region interpretation across the video. Additionally, we introduce a large-scale region-level video instruction dataset (RegVID-300k). Omni-RGPT achieves state-of-the-art results on image and video-based commonsense reasoning benchmarks while showing strong performance in captioning and referring expression comprehension tasks.
</details>

<details>
    <summary>Key points</summary>
    * Introduces Token Mark for highlighting target regions in visual feature space.
    * Uses auxiliary task for stable region interpretation in videos without tracklets.
    * Trained on large-scale region-level video instruction dataset (RegVID-300k).
</details>
</details>

---


<details>
<summary><b> UGen: Unified Autoregressive Multimodal Model with Progressive Vocabulary Learning</b></summary>

* **Authors:** Hongxuan Tang, Hao Liu, Xinyan Xiao
* **arXiv ID:** 2503.21193
* **One-liner:** Introduced UGen, a unified autoregressive multimodal model achieving strong performance in text processing, image understanding, and generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2503.21193) | [[PDF]](https://arxiv.org/pdf/2503.21193)

> **Core Innovation**
> UGen converts texts and images into discrete token sequences and uses a single transformer for uniform autoregressive generation, enhanced by progressive vocabulary learning.

<details>
    <summary>Abstract</summary>
    We introduce UGen, a unified autoregressive multimodal model that demonstrates strong performance across text processing, image understanding, and image generation tasks simultaneously. UGen converts both texts and images into discrete token sequences and utilizes a single transformer to generate them uniformly in an autoregressive manner. To address the challenges associated with unified multimodal learning, UGen is trained using a novel mechanism, namely progressive vocabulary learning. In this process, visual token IDs are incrementally activated and integrated into the training phase, ultimately enhancing the effectiveness of unified multimodal learning. Experiments on comprehensive text and image tasks show that UGen achieves a significant overall performance improvement of 13.3% compared to the vanilla unified autoregressive method, and it also delivers competitive results across all tasks against several task-specific models.
</details>

<details>
    <summary>Key points</summary>
    * Progressive vocabulary learning for incremental integration of visual tokens
    * Unified autoregressive generation with a single transformer
    * Achieved 13.3% overall performance improvement over vanilla methods
</details>
</details>

---


<details>
<summary><b> OmniMamba: Efficient and Unified Multimodal Understanding and Generation via State Space Models</b></summary>

* **Authors:** Jialv Zou, Bencheng Liao, Qian Zhang, Wenyu Liu, Xinggang Wang
* **arXiv ID:** 2503.08686
* **One-liner:** Presented OmniMamba, the first linear-architecture-based multimodal generation model with high efficiency and data efficiency.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2503.08686) | [[PDF]](https://arxiv.org/pdf/2503.08686)

> **Core Innovation**
> OmniMamba uses Mamba-2 for linear computational complexity and introduces decoupled vocabularies and task-specific LoRA for efficient multimodal generation.

<details>
    <summary>Abstract</summary>
    Recent advancements in unified multimodal understanding and visual generation (or multimodal generation) models have been hindered by their quadratic computational complexity and dependence on large-scale training data. We present OmniMamba, the first linear-architecture-based multimodal generation model that generates both text and images through a unified next-token prediction paradigm. The model fully leverages Mamba-2&#39;s high computational and memory efficiency, extending its capabilities from text generation to multimodal generation. To address the data inefficiency of existing unified models, we propose two key innovations: (1) decoupled vocabularies to guide modality-specific generation, and (2) task-specific LoRA for parameter-efficient adaptation. Furthermore, we introduce a decoupled two-stage training strategy to mitigate data imbalance between two tasks. Equipped with these techniques, OmniMamba achieves competitive performance with JanusFlow while surpassing Show-o across benchmarks, despite being trained on merely 2M image-text pairs, which is 1,000 times fewer than Show-o. Notably, OmniMamba stands out with outstanding inference efficiency, achieving up to a 119.2 times speedup and 63% GPU memory reduction for long-sequence generation compared to Transformer-based counterparts. Code and models are released at <a href="https://github.com/hustvl/OmniMamba" rel="external noopener nofollow" class="link-external link-https">this https URL</a>
</details>

<details>
    <summary>Key points</summary>
    * Linear architecture based on Mamba-2 for efficiency
    * Decoupled vocabularies for modality-specific generation
    * Task-specific LoRA for parameter-efficient adaptation
    * Trained on only 2M image-text pairs with competitive performance
</details>
</details>

---


<details>
<summary><b> VARGPT-v1.1: Improve Visual Autoregressive Large Unified Model via Iterative Instruction Tuning and Reinforcement Learning</b></summary>

* **Authors:** Xianwei Zhuang, Yuxin Xie, Yufan Deng, Dongchao Yang, Liming Liang, Jinghan Ru, Yuguo Yin, Yuexian Zou
* **arXiv ID:** 2504.02949
* **One-liner:** Advanced VARGPT-v1.1 to achieve state-of-the-art performance in multimodal understanding and text-to-image instruction-following.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2504.02949) | [[PDF]](https://arxiv.org/pdf/2504.02949)

> **Core Innovation**
> VARGPT-v1.1 integrates iterative visual instruction tuning with DPO, an expanded corpus, and an upgraded backbone for enhanced generation and emergent editing.

<details>
    <summary>Abstract</summary>
    In this work, we present VARGPT-v1.1, an advanced unified visual autoregressive model that builds upon our previous framework VARGPT. The model preserves the dual paradigm of next-token prediction for visual understanding and next-scale generation for image synthesis. Specifically, VARGPT-v1.1 integrates: (1) a novel training strategy combining iterative visual instruction tuning with reinforcement learning through Direct Preference Optimization (DPO), (2) an expanded training corpus containing 8.3M visual-generative instruction pairs, (3) an upgraded language model backbone using Qwen2, (4) enhanced image generation resolution, and (5) emergent image editing capabilities without architectural modifications. These advancements enable VARGPT-v1.1 to achieve state-of-the-art performance in multimodal understanding and text-to-image instruction-following tasks, demonstrating significant improvements in both comprehension and generation metrics. Notably, through visual instruction tuning, the model acquires image editing functionality while maintaining architectural consistency with its predecessor, revealing the potential for unified visual understanding, generation, and editing. Our findings suggest that well-designed unified visual autoregressive models can effectively adopt flexible training strategies from large language models (LLMs), exhibiting promising scalability. The codebase and model weights are publicly available at <a href="https://github.com/VARGPT-family/VARGPT-v1.1" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Iterative visual instruction tuning with Direct Preference Optimization
    * Expanded training corpus of 8.3M visual-generative instruction pairs
    * Upgraded language model backbone using Qwen2
    * Enhanced image generation resolution and emergent editing capabilities
</details>
</details>

---


<details>
<summary><b> FUDOKI: Discrete Flow-based Unified Understanding and Generation via Kinetic-Optimal Velocities</b></summary>

* **Authors:** Jin Wang, Yao Lai, Aoxue Li, Shifeng Zhang, Jiacheng Sun, Ning Kang, Chengyue Wu, Zhenguo Li, Ping Luo
* **arXiv ID:** 2505.20147
* **One-liner:** Introduced FUDOKI, a unified multimodal model based on discrete flow matching as an alternative to autoregressive paradigms.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2505.20147) | [[PDF]](https://arxiv.org/pdf/2505.20147)

> **Core Innovation**
> FUDOKI leverages metric-induced probability paths for iterative refinement and bidirectional context integration, initialized from pre-trained AR models.

<details>
    <summary>Abstract</summary>
    The rapid progress of large language models (LLMs) has catalyzed the emergence of multimodal large language models (MLLMs) that unify visual understanding and image generation within a single framework. However, most existing MLLMs rely on autoregressive (AR) architectures, which impose inherent limitations on future development, such as the raster-scan order in image generation and restricted reasoning abilities in causal context modeling. In this work, we challenge the dominance of AR-based approaches by introducing FUDOKI, a unified multimodal model purely based on discrete flow matching, as an alternative to conventional AR paradigms. By leveraging metric-induced probability paths with kinetic optimal velocities, our framework goes beyond the previous masking-based corruption process, enabling iterative refinement with self-correction capability and richer bidirectional context integration during generation. To mitigate the high cost of training from scratch, we initialize FUDOKI from pre-trained AR-based MLLMs and adaptively transition to the discrete flow matching paradigm. Experimental results show that FUDOKI achieves performance comparable to state-of-the-art AR-based MLLMs across both visual understanding and image generation tasks, highlighting its potential as a foundation for next-generation unified multimodal models. Furthermore, we show that applying test-time scaling techniques to FUDOKI yields significant performance gains, further underscoring its promise for future enhancement through reinforcement learning.
</details>

<details>
    <summary>Key points</summary>
    * Discrete flow matching framework for iterative refinement
    * Bidirectional context integration during generation
    * Initialization from pre-trained AR-based MLLMs
    * Performance comparable to state-of-the-art AR models
</details>
</details>

---


<details>
<summary><b> MMaDA: Multimodal Large Diffusion Language Models</b></summary>

* **Authors:** Ling Yang, Ye Tian, Bowen Li, Xinchen Zhang, Ke Shen, Yunhai Tong, Mengdi Wang
* **arXiv ID:** 2505.15809
* **One-liner:** Developed MMaDA, a unified multimodal diffusion foundation model with strong generalization across reasoning, understanding, and generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2505.15809) | [[PDF]](https://arxiv.org/pdf/2505.15809)

> **Core Innovation**
> MMaDA uses a unified diffusion architecture, mixed CoT fine-tuning, and UniGRPO RL algorithm for seamless multimodal integration.

<details>
    <summary>Abstract</summary>
    We introduce MMaDA, a novel class of multimodal diffusion foundation models designed to achieve superior performance across diverse domains such as textual reasoning, multimodal understanding, and text-to-image generation. The approach is distinguished by three key innovations: (i) MMaDA adopts a unified diffusion architecture with a shared probabilistic formulation and a modality-agnostic design, eliminating the need for modality-specific components. This architecture ensures seamless integration and processing across different data types. (ii) We implement a mixed long chain-of-thought (CoT) fine-tuning strategy that curates a unified CoT format across modalities. By aligning reasoning processes between textual and visual domains, this strategy facilitates cold-start training for the final reinforcement learning (RL) stage, thereby enhancing the model&#39;s ability to handle complex tasks from the outset. (iii) We propose UniGRPO, a unified policy-gradient-based RL algorithm specifically tailored for diffusion foundation models. Utilizing diversified reward modeling, UniGRPO unifies post-training across both reasoning and generation tasks, ensuring consistent performance improvements. Experimental results demonstrate that MMaDA-8B exhibits strong generalization capabilities as a unified multimodal foundation model. It surpasses powerful models like LLaMA-3-7B and Qwen2-7B in textual reasoning, outperforms Show-o and SEED-X in multimodal understanding, and excels over SDXL and Janus in text-to-image generation. These achievements highlight MMaDA&#39;s effectiveness in bridging the gap between pretraining and post-training within unified diffusion architectures, providing a comprehensive framework for future research and development. We open-source our code and trained models at: <a href="https://github.com/Gen-Verse/MMaDA" rel="external noopener nofollow" class="link-external link-https">this https URL</a>
</details>

<details>
    <summary>Key points</summary>
    * Unified diffusion architecture with modality-agnostic design
    * Mixed long chain-of-thought fine-tuning
    * UniGRPO unified policy-gradient RL algorithm
    * Surpasses models like LLaMA-3-7B and SDXL in various tasks
</details>
</details>

---


<details>
<summary><b> Many-for-Many: Unify the Training of Multiple Video and Image Generation and Manipulation Tasks</b></summary>

* **Authors:** Tao Yang, Ruibin Li, Yangming Shi, Yuqi Zhang, Qide Dong, Haoran Cheng, Weiguo Feng, Shilei Wen, Bingyue Peng, Lei Zhang
* **arXiv ID:** 2506.01758
* **One-liner:** Introduced a many-for-many unified framework for visual generation and manipulation tasks with improved video generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2506.01758) | [[PDF]](https://arxiv.org/pdf/2506.01758)

> **Core Innovation**
> The framework uses a lightweight adapter and joint image-video learning to train a single model for multiple tasks, incorporating depth maps for 3D perception.

<details>
    <summary>Abstract</summary>
    Diffusion models have shown impressive performance in many visual generation and manipulation tasks. Many existing methods focus on training a model for a specific task, especially, text-to-video (T2V) generation, while many other works focus on finetuning the pretrained T2V model for image-to-video (I2V), video-to-video (V2V), image and video manipulation tasks, etc. However, training a strong T2V foundation model requires a large amount of high-quality annotations, which is very costly. In addition, many existing models can perform only one or several tasks. In this work, we introduce a unified framework, namely many-for-many, which leverages the available training data from many different visual generation and manipulation tasks to train a single model for those different tasks. Specifically, we design a lightweight adapter to unify the different conditions in different tasks, then employ a joint image-video learning strategy to progressively train the model from scratch. Our joint learning leads to a unified visual generation and manipulation model with improved video generation performance. In addition, we introduce depth maps as a condition to help our model better perceive the 3D space in visual generation. Two versions of our model are trained with different model sizes (8B and 2B), each of which can perform more than 10 different tasks. In particular, our 8B model demonstrates highly competitive performance in video generation tasks compared to open-source and even commercial engines. Our models and source codes are available at <a href="https://github.com/leeruibin/MfM.git" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Lightweight adapter to unify different conditions
    * Joint image-video learning strategy
    * Use of depth maps for 3D space perception
    * Supports over 10 different tasks with competitive performance
</details>
</details>

---


<details>
<summary><b> Pisces: An Auto-regressive Foundation Model for Image Understanding and Generation</b></summary>

* **Authors:** Zhiyang Xu, Jiuhai Chen, Zhaojiang Lin, Xichen Pan, Lifu Huang, Tianyi Zhou, Madian Khabsa, Qifan Wang, Di Jin, Michihiro Yasunaga, Lili Yu, Xi Victoria Lin, Shaoliang Nie
* **arXiv ID:** 2506.10395
* **One-liner:** Presented Pisces, an autoregressive multimodal foundation model achieving competitive performance in both image understanding and generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2506.10395) | [[PDF]](https://arxiv.org/pdf/2506.10395)

> **Core Innovation**
> Pisces employs a decoupled visual encoding architecture and tailored training techniques to address modality differences.

<details>
    <summary>Abstract</summary>
    Recent advances in large language models (LLMs) have enabled multimodal foundation models to tackle both image understanding and generation within a unified framework. Despite these gains, unified models often underperform compared to specialized models in either task. A key challenge in developing unified models lies in the inherent differences between the visual features needed for image understanding versus generation, as well as the distinct training processes required for each modality. In this work, we introduce Pisces, an auto-regressive multimodal foundation model that addresses this challenge through a novel decoupled visual encoding architecture and tailored training techniques optimized for multimodal generation. Combined with meticulous data curation, pretraining, and finetuning, Pisces achieves competitive performance in both image understanding and image generation. We evaluate Pisces on over 20 public benchmarks for image understanding, where it demonstrates strong performance across a wide range of tasks. Additionally, on GenEval, a widely adopted benchmark for image generation, Pisces exhibits robust generative capabilities. Our extensive analysis reveals the synergistic relationship between image understanding and generation, and the benefits of using separate visual encoders, advancing the field of unified multimodal models.
</details>

<details>
    <summary>Key points</summary>
    * Decoupled visual encoding architecture
    * Tailored training techniques for multimodal generation
    * Meticulous data curation, pretraining, and finetuning
    * Strong performance on over 20 benchmarks for understanding and generation
</details>
</details>

---


<details>
<summary><b> Ming-Omni: A Unified Multimodal Model for Perception and Generation</b></summary>

* **Authors:** Inclusion AI, Biao Gong, Cheng Zou, Chuanyang Zheng, Chunluan Zhou, Canxiang Yan, Chunxiang Jin, Chunjie Shen, Dandan Zheng, Fudong Wang, Furong Xu, GuangMing Yao, Jun Zhou, Jingdong Chen, Jianxin Sun, Jiajia Liu, Jianjiang Zhu, Jun Peng, Kaixiang Ji, Kaiyou Song, Kaimeng Ren, Libin Wang, Lixiang Ru, Lele Xie, Longhua Tan, Lyuxin Xue, Lan Wang, Mochen Bai, Ning Gao, Pei Chen, Qingpei Guo, Qinglong Zhang, Qiang Xu, Rui Liu, Ruijie Xiong, Sirui Gao, Tinghao Liu, Taisong Li, Weilong Chai, Xinyu Xiao, Xiaomei Wang, Xiaoxue Chen, Xiao Lu, Xiaoyu Li, Xingning Dong, Xuzheng Yu, Yi Yuan, Yuting Gao, Yunxiao Sun, Yipeng Chen, Yifei Wu, Yongjie Lyu, Ziping Ma, Zipeng Feng, Zhijiang Fang, Zhihao Qiu, Ziyuan Huang, Zhengyu He
* **arXiv ID:** 2506.09344
* **One-liner:** Proposed Ming-Omni, a unified multimodal model supporting image, text, audio, and video processing with generation capabilities.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2506.09344) | [[PDF]](https://arxiv.org/pdf/2506.09344)

> **Core Innovation**
> Ming-Omni uses dedicated encoders and an MoE architecture with modality-specific routers for efficient multimodal fusion and generation.

<details>
    <summary>Abstract</summary>
    We propose Ming-Omni, a unified multimodal model capable of processing images, text, audio, and video, while demonstrating strong proficiency in both speech and image generation. Ming-Omni employs dedicated encoders to extract tokens from different modalities, which are then processed by Ling, an MoE architecture equipped with newly proposed modality-specific routers. This design enables a single model to efficiently process and fuse multimodal inputs within a unified framework, thereby facilitating diverse tasks without requiring separate models, task-specific fine-tuning, or structural redesign. Importantly, Ming-Omni extends beyond conventional multimodal models by supporting audio and image generation. This is achieved through the integration of an advanced audio decoder for natural-sounding speech and Ming-Lite-Uni for high-quality image generation, which also allow the model to engage in context-aware chatting, perform text-to-speech conversion, and conduct versatile image editing. Our experimental results showcase Ming-Omni offers a powerful solution for unified perception and generation across all modalities. Notably, our proposed Ming-Omni is the first open-source model we are aware of to match GPT-4o in modality support, and we release all code and model weights to encourage further research and development in the community.
</details>

<details>
    <summary>Key points</summary>
    * Dedicated encoders for different modalities
    * MoE architecture with modality-specific routers
    * Integration of advanced audio and image decoders
    * Matches GPT-4o in modality support and open-sourced
</details>
</details>

---


<details>
<summary><b> UniCode$^2$: Cascaded Large-scale Codebooks for Unified Multimodal Understanding and Generation</b></summary>

* **Authors:** Yanzhe Chen, Huasong Zhong, Yan Li, Zhenheng Yang
* **arXiv ID:** 2506.20214
* **One-liner:** Introduced UniCode^2, a cascaded codebook framework for stable and semantically aligned visual tokenization.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2506.20214) | [[PDF]](https://arxiv.org/pdf/2506.20214)

> **Core Innovation**
> UniCode^2 builds a 500K-entry codebook by clustering SigLIP embeddings, enabling high utilization and integration with diffusion decoders.

<details>
    <summary>Abstract</summary>
    Unified multimodal large language models (MLLMs) have shown promise in jointly advancing multimodal understanding and generation, with visual codebooks discretizing images into tokens for autoregressive modeling. Existing codebook-based methods either rely on small vocabularies (~16K entries) that lack fine-grained semantics or naively scale up, resulting in low token utilization and unstable training. We propose UniCode$^2$, a cascaded codebook framework enabling large-scale, semantically aligned, and stable visual tokenization. By clustering millions of SigLIP sequence embeddings, we build a 500K-entry codebook that preserves vision-language alignment while expanding capacity. Stability is ensured via a cascaded design: a frozen codebook anchors the embedding space, and a trainable codebook refines task-specific semantics. This decoupling promotes high utilization and robust learning. Moreover, the alignment of our visual tokens with textual semantics enables seamless integration with pretrained diffusion decoders, supporting high-quality visual synthesis with minimal adaptation. UniCode^2 delivers strong performance across diverse benchmarks, demonstrating the viability of scaling visual token spaces without sacrificing stability, semantics, or modularity.
</details>

<details>
    <summary>Key points</summary>
    * Cascaded codebook design with frozen and trainable components
    * Clustering of SigLIP sequence embeddings for alignment
    * Large-scale 500K-entry codebook
    * Seamless integration with pretrained diffusion decoders
</details>
</details>

---


<details>
<summary><b> Ovis-U1 Technical Report</b></summary>

* **Authors:** Guo-Hua Wang, Shanshan Zhao, Xinjie Zhang, Liangfu Cao, Pengxin Zhan, Lunhao Duan, Shiyin Lu, Minghao Fu, Xiaohao Chen, Jianshan Zhao, Yang Li, Qing-Guo Chen
* **arXiv ID:** 2506.23044
* **One-liner:** Developed Ovis-U1, a unified model integrating multimodal understanding, text-to-image generation, and image editing.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2506.23044) | [[PDF]](https://arxiv.org/pdf/2506.23044)

> **Core Innovation**
> Ovis-U1 uses a diffusion-based visual decoder and unified training from a language model, achieving high benchmark scores.

<details>
    <summary>Abstract</summary>
    In this report, we introduce Ovis-U1, a 3-billion-parameter unified model that integrates multimodal understanding, text-to-image generation, and image editing capabilities. Building on the foundation of the Ovis series, Ovis-U1 incorporates a diffusion-based visual decoder paired with a bidirectional token refiner, enabling image generation tasks comparable to leading models like GPT-4o. Unlike some previous models that use a frozen MLLM for generation tasks, Ovis-U1 utilizes a new unified training approach starting from a language model. Compared to training solely on understanding or generation tasks, unified training yields better performance, demonstrating the enhancement achieved by integrating these two tasks. Ovis-U1 achieves a score of 69.6 on the OpenCompass Multi-modal Academic Benchmark, surpassing recent state-of-the-art models such as Ristretto-3B and SAIL-VL-1.5-2B. In text-to-image generation, it excels with scores of 83.72 and 0.89 on the DPG-Bench and GenEval benchmarks, respectively. For image editing, it achieves 4.00 and 6.42 on the ImgEdit-Bench and GEdit-Bench-EN, respectively. As the initial version of the Ovis unified model series, Ovis-U1 pushes the boundaries of multimodal understanding, generation, and editing.
</details>

<details>
    <summary>Key points</summary>
    * Diffusion-based visual decoder with bidirectional token refiner
    * Unified training approach from a language model
    * Achieved high scores on OpenCompass, DPG-Bench, and GenEval
    * Enhanced performance in understanding, generation, and editing
</details>
</details>

---


<details>
<summary><b> UniLiP: Adapting CLIP for Unified Multimodal Understanding, Generation and Editing</b></summary>

* **Authors:** Hao Tang, Chenwei Xie, Xiaoyi Bao, Tingyu Weng, Pandeng Li, Yun Zheng, Liwei Wang
* **arXiv ID:** 2507.23278
* **One-liner:** Proposed UniLIP, a unified framework adapting CLIP for multimodal understanding, generation, and editing with high performance.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2507.23278) | [[PDF]](https://arxiv.org/pdf/2507.23278)

> **Core Innovation**
> Introduced a two-stage training scheme with self-distillation to enhance CLIP's reconstruction abilities while maintaining comprehension, achieving state-of-the-art results on benchmarks like GenEval and WISE.

<details>
    <summary>Abstract</summary>
    In this paper, we propose UniLIP, a unified framework that adapts CLIP for multimodal understanding, generation and editing. Although CLIP excels at understanding, it lacks reconstruction abilities required to be a unified visual encoder. However, previous CLIP-based unified methods fail to balance understanding and reconstruction, leading to semantic degradation or inconsistent reconstructions. In contrast, we introduce a novel two-stage training scheme with a self-distillation strategy that progressively endows CLIP with high-fidelity reconstruction abilities while preserving its original comprehension performance. For enhanced reasoning and consistency in generation and editing, we further develop a dual-condition architecture built upon the MetaQuery framework. Our architecture jointly utilizes multimodal hidden states for rich contextual details and learnable query embeddings to harness the powerful reasoning abilities of Multimodal Large Language Models (MLLMs). Leveraging advanced image representation and architectural design, UniLIP demonstrates superior instruction following and edit fidelity. With only 1B and 3B parameters, UniLIP can outperform larger unified models such as BAGEL (7B) and Uniworld-V1 (12B), achieving state-of-the-art performance of 0.90 on GenEval, 0.63 on WISE, and 3.94 on ImgEdit. These results demonstrate that UniLIP successfully expands the application of CLIP, establishing its continuous features to not only serve as the optimal choice for understanding tasks but also achieve highly competitive performance in generation and editing tasks. Code and models are available at <a href="https://github.com/nnnth/UniLIP" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
</details>

<details>
    <summary>Key points</summary>
    * Two-stage training with self-distillation
    * Dual-condition architecture based on MetaQuery
    * Utilization of multimodal hidden states and learnable query embeddings
    * Integration with Multimodal Large Language Models (MLLMs)
</details>
</details>

---


<details>
<summary><b> Hyper-Bagel: A Unified Acceleration Framework for Multimodal Understanding and Generation</b></summary>

* **Authors:** Yanzuo Lu, Xin Xia, Manlin Zhang, Huafeng Kuang, Jianbin Zheng, Yuxi Ren, Xuefeng Xiao
* **arXiv ID:** 2509.18824
* **One-liner:** Developed Hyper-Bagel, a unified acceleration framework for multimodal tasks, significantly speeding up understanding and generation.
* **Published in:** 
* **Links:** [[Paper]](https://arxiv.org/abs/2509.18824) | [[PDF]](https://arxiv.org/pdf/2509.18824)

> **Core Innovation**
> Employed speculative decoding and multi-stage distillation to achieve over 2x speedup in understanding and up to 22x in generation, while preserving output quality.

<details>
    <summary>Abstract</summary>
    Unified multimodal models have recently attracted considerable attention for their remarkable abilities in jointly understanding and generating diverse content. However, as contexts integrate increasingly numerous interleaved multimodal tokens, the iterative processes of diffusion denoising and autoregressive decoding impose significant computational overhead. To address this, we propose Hyper-Bagel, a unified acceleration framework designed to simultaneously speed up both multimodal understanding and generation tasks. Our approach uses a divide-and-conquer strategy, employing speculative decoding for next-token prediction and a multi-stage distillation process for diffusion denoising. The framework delivers substantial performance gains, achieving over a 2x speedup in multimodal understanding. For generative tasks, our resulting lossless 6-NFE model yields a 16.67x speedup in text-to-image generation and a 22x speedup in image editing, all while preserving the high-quality output of the original model. We further develop a highly efficient 1-NFE model that enables near real-time interactive editing and generation. By combining advanced adversarial distillation with human feedback learning, this model achieves ultimate cost-effectiveness and responsiveness, making complex multimodal interactions seamless and instantaneous.
</details>

<details>
    <summary>Key points</summary>
    * Divide-and-conquer strategy with speculative decoding
    * Multi-stage distillation for diffusion denoising
    * Development of lossless 6-NFE and efficient 1-NFE models
    * Combination of adversarial distillation and human feedback learning
</details>
</details>

---
